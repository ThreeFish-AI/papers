# Agent0：通过工具集成推理从零数据释放自进化智能体

## 作者

Peng Xia¹, Kaide Zeng¹, Jiaqi Liu¹, Can Qin², Fang Wu³, Yiyang Zhou¹, Caiming Xiong², Huaxiu Yao¹

¹UNC-Chapel Hill ²Salesforce Research ³Stanford University

_通讯作者：Peng Xia <pxia@cs.unc.edu>, Huaxiu Yao <huaxiu@cs.unc.edu>_

_初步工作。_

arXiv:2511.16043v1 [cs.LG] 2025 年 11 月 20 日

## 摘要

大语言模型（LLM）智能体通常通过强化学习（RL）进行训练，但受到对人工策划数据的依赖限制，这种依赖限制了可扩展性并将人工智能束缚在人类知识范围内。

现有的自我进化框架提供了一种替代方案，但通常受到模型固有能力限制和单轮交互的约束，阻碍了涉及工具使用或动态推理的复杂课程发展。我们引入了 Agent0，这是一个完全自主的框架，通过多步共同进化和无缝工具集成来进化高性能智能体，而无需外部数据。

Agent0 在从同一基础 LLM 初始化的两个智能体之间建立共生竞争：一个课程智能体提出越来越具挑战性的前沿任务，一个执行者智能体学习解决这些任务。我们集成外部工具来增强执行者的问题解决能力；这种改进反过来又推动课程智能体构建更复杂的、工具感知的任务。

通过这个迭代过程，Agent0 建立了一个自我强化的循环，持续产生高质量的课程。经验上，Agent0 显著提升了推理能力，在数学推理基准测试上将 Qwen3-8B-Base 模型提高了 18%，在一般推理基准测试上提高了 24%。

代码可在 https://github.com/aiming-lab/Agent0 获取。

## 1. 引言

大语言模型（LLM）智能体在解决需要与环境广泛交互的复杂、长期问题方面表现出了卓越能力，例如深度研究和智能体编程。为了优化这些复杂的多步交互并超越硬编码的工作流程，强化学习已成为主要的训练范式，在复杂推理任务上取得了重大进展。

然而，无论是基于人类反馈的强化学习（RLHF）还是基于可验证奖励的强化学习（RLVR），这些方法的有效性都严重依赖于大规模、高质量的人工策划数据集。这种依赖不仅造成了严重的可扩展性瓶颈（耗时、劳动密集且成本高昂），而且从根本上将人工智能的潜力限制在人类知识和标注速度的范围内。

为了摆脱对人工数据的依赖，自我进化框架已成为一个有前景的替代方案，通过使模型能够自主生成自己的训练数据来提供可扩展的路径。然而，尽管它们具有潜力，现有的自我博弈或自我挑战方法面临着严重约束。

首先，它们的能力受到模型固有知识和推理能力的限制，导致生成的任务很少能超越模型当前的复杂性，导致学习停滞。其次，这些框架通常只在单轮交互中运行，无法捕获现实世界问题的动态、上下文依赖性质。

这种双重限制不仅限制了自我生成课程的复杂性，更重要的是阻碍了模型掌握需要复杂工具使用或多步推理的基本技能。

为了解决这些挑战，如图 1 所示，我们引入了 Agent0，这是一个完全自主的框架，旨在完全从头开始指导智能体的进化。Agent0 完全消除了对任何外部数据或人工标注的依赖，开创性地将工具集成与多轮共同进化相结合。

该框架的实施从一个基础 LLM 开始，我们从中初始化两个功能不同的智能体：一个执行者智能体和一个课程智能体。这些智能体通过共生竞争共同进化：课程智能体使用 RL 训练来提出前沿任务，这些任务精确挑战执行者的当前能力，使用执行者的不确定性（即多个答案间的自一致性）及其工具使用频率作为奖励信号。

同时，执行者智能体通过 RL 训练来成功解决这些任务，在由冻结的课程智能体生成的挑战性问题过滤集上优化，并使用从其自身多数投票得出的伪标签。为执行者配备工具增强了其问题解决能力，这反过来又迫使配备工具的课程智能体生成更复杂的、基于工具的课程。

这建立了一个良性循环，推动智能体能力和课程复杂性的同步螺旋式提升。此外，我们将这个范式扩展到支持多轮交互，能够生成上下文丰富的、对话式的任务，更好地反映现实世界的问题解决。

本文的主要贡献是 Agent0，这是一个新颖的框架，通过工具增强推理自主地从头开始进化 LLM 智能体，而无需依赖任何外部数据。在跨越数学和一般推理的十个基准测试中，经验结果表明 Agent0 取得了实质性的模型无关能力增益，将数学推理性能提高了 18%，一般推理性能提高了 24%。

此外，我们的分析证实这种增益是由我们的共同进化循环驱动的，其中课程智能体学会生成逐渐复杂的任务，创建执行者能力提升的良性循环。

## 2. 预备知识

### LLM 作为策略智能体

我们将 LLM 表述为一个智能体，用具有参数 θ 的策略 πθ 表示。给定提示 x，智能体自回归生成响应 y ~ πθ(·|x)。

强化学习的一般目标是优化 θ 以最大化期望奖励：
$$J(θ) = \mathbb{E}_{x∼D,y∼πθ(·|x)}[R(y|x)]$$

### 群体相对策略优化（GRPO）

GRPO 是一种强化学习方法，通过使用群体内相对奖励来避免训练批评者。对于每个提示 x，模型采样 G 个响应{y1, ..., yG}，这些响应被评分以获得奖励{r1, ..., rG}。GRPO 使用 z 分数计算标准化优势 Âi：

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G) + \epsilon_{\text{norm}}}$$

其中 εnorm 是用于数值稳定性的小常数。然后通过最小化以下 PPO 风格的裁剪损失函数来更新策略：

$$L_{\text{GRPO}}(θ) = -\frac{1}{G} \sum_{i=1}^G \min\left(\frac{π_θ(x_i)}{π_{θ_{\text{old}}}(x_i)}\hat{A}_i, \text{clip}\left[\frac{π_θ(x_i)}{π_{θ_{\text{old}}}(x_i)}, 1-\epsilon, 1+\epsilon\right]\hat{A}_i\right) + β_{KL}(π_θ ∥ π_{θ_{\text{old}}})$$

其中$\frac{π_θ(x_i)}{π_{θ_{\text{old}}}(x_i)}$是当前策略 πθ 与上一次迭代的参考策略 πθold 之间的重要性采样比率。Âi 是标准化优势，ε 和 β 是超参数。KL 散度项作为正则化惩罚以稳定训练。

## 3. Agent0 框架

### 3.1. 框架概述

Agent0 是一个完全自主的迭代共同进化框架，旨在增强 LLM 智能体的能力，而无需依赖任何人工标注的数据。这个框架的核心是两个从同一基础 LLM πbase 初始化的功能不同的智能体：

1. **课程智能体（πθ）**：旨在生成适合当前执行者智能体挑战的前沿任务
2. **执行者智能体（πφ）**：旨在解决课程智能体提出的越来越复杂的任务

这两个智能体通过共生竞争过程迭代共同进化，如图 2 所示。此过程的每次迭代 t 分为两个阶段：

**课程进化**：我们使用 RL 训练课程智能体 πθ，专门生成挑战当前执行者智能体 πφ^{(t-1)}的任务。

**执行者进化**：我们使用冻结的课程智能体 πθ^{(t)}生成任务池，从中筛选出一个具有挑战性的数据集 D^{(t)}。然后我们在这个数据集上使用 RL 训练执行者智能体 πφ，将其进化为 πφ^{(t)}。

代码解释器工具的集成建立了良性循环：执行者智能体的问题解决能力通过工具得到增强，这反过来又迫使配备工具的课程智能体生成更复杂的、基于工具的课程。此外，该框架支持多轮交互，使课程智能体能够生成上下文丰富的、对话式的任务，更好地反映现实世界的问题解决。

![Agent0自主共同进化框架](images/LLM%20Agents/Unleashing%20Self-Evolving%20Agents%20from%20Zero%20Data%20via%20Tool-Integrated%20Reasoning/img_1_6_20251127_145854.png)

**图 1. Agent0 自主共同进化框架。** 课程智能体（左）使用 RL 生成前沿任务，通过执行者智能体的不确定性和工具使用频率获得奖励。执行者智能体（右）通过 RL 学习解决这些问题。这种共享的工具集成从零开始驱动了任务复杂性和智能体能力的良性循环。

### 3.2. 课程智能体训练

课程智能体 πθ 的目标是生成一个提示 x，以最大化复合奖励信号 RC。这个奖励信号旨在量化任务 x 对当前执行者智能体 πφ 的挑战性。我们使用第 2 节中描述的 GRPO 算法优化 πθ。

对于 πθ 生成的每个任务 xi，我们通过从当前执行者 πφ 采样 k 个响应{yj}\_{j=1}^k 来计算其奖励。复合奖励 RC 包含两个关键组成部分：

**不确定性奖励**：此奖励激励课程智能体生成执行者感到困惑或不确定的任务。我们使用执行者的自一致性$\hat{p}(x; πφ)$作为不确定性的代理。$\hat{p}$被定义为投票给多数答案（$\tilde{y}$）的 k 个响应的比例。奖励函数设计为在$\hat{p} = 0.5$时最大化，此时执行者的不确定性最高：

$$R_{\text{unc}}(x; πφ) = 1 - 2|\hat{p}(x; πφ) - 0.5|$$

这个函数惩罚那些过于简单（$\hat{p} → 1$）或过于困难（$\hat{p} → 0$）的任务。

**工具使用奖励**：为了驱动良性循环，我们必须明确奖励那些促使执行者使用其工具的任务。我们基于工具调用次数定义 Rtool，通过工具响应标记（即'''output）在完整预测 y = πφ(x)中识别。令 Ntool(y)为 y 中这些标记的总数。奖励计算为加权、上限值：

$$R_{\text{tool}}(x; πφ) = γ \cdot \min(N_{\text{tool}}(y), C)$$

其中 γ 是奖励分数的缩放超参数，C 是奖励调用次数的上限，以防止奖励过度或虚假的工具使用。

**重复惩罚**：为了鼓励训练批次 X 内的多样性，我们引入重复惩罚 Rrep。我们首先使用相似性度量（如 BLEU 分数）计算生成任务之间的成对距离：$d_{ij} = 1 - \text{BLEU}(x_i, x_j)$。然后任务被分组成簇 C = {C1, ..., CK}，其中$d_{ij} < τ_{\text{BLEU}}$。属于簇 Ck 的任务 xi 的惩罚与其相对簇大小成正比：

$$R_{\text{rep}}(x_i) = \lambda_{\text{rep}} \frac{|C_k|}{B}$$

其中 B 是批次大小，λrep 是缩放因子。

**复合奖励**：最终奖励结合这些信号，减去重复惩罚，并通过格式检查 Rformat 进行门控：

$$R_C(x_i) = R_{\text{format}}(x_i) \cdot \max(0, (\lambda_{\text{unc}}R_{\text{unc}} + \lambda_{\text{tool}}R_{\text{tool}}) - R_{\text{rep}}(x_i))$$

其中 λunc、λtool 和 λrep 是超参数。我们将此 RC 用作 GRPO 损失中的奖励 ri。

### 3.3. 执行者智能体训练

执行者智能体 πφ 的目标是最大化其解决课程智能体 πθ 生成任务的成功率。这个训练阶段也基于 GRPO。

#### 3.3.1. 数据集策划和轨迹生成

**挑战性数据集构建**：在课程智能体 πθ^{(t)}训练后，我们冻结它。我们用它生成大量候选任务池 Xpool。对于此池中的每个任务 x，我们让当前执行者 πφ^{(t-1)}采样 k 个响应并计算其自一致性$\hat{p}(x)$。它被计算为投票给此多数答案$\tilde{y}$的响应比例：

$$\hat{p}(x) = \frac{1}{k} \sum_{i=1}^k I(o_i = \tilde{y}), \quad \tilde{y} = \arg\max_y \sum_{i=1}^k I(o_i = y)$$

其中 I 是指示函数。为了构建高效的训练课程，我们筛选位于能力前沿的任务。因此我们只保留自一致性分数落在信息带内的任务：

$$D^{(t)} = \{x ∈ X_{\text{pool}} | |\hat{p}(x; πφ^{(t-1)}) - 0.5| ≤ δ\}$$

其中 δ 是控制课程难度的阈值。这个过滤步骤确保 πφ 只在对其来说既不太容易也不太困难的任务上训练。

**多轮推出**：我们用多步、工具集成的推出过程替换标准的单轮生成。在此过程中，k 个轨迹中的每一个都是通过让策略 πφ^{(t-1)}首先产生文本推理 t1 生成的。当策略发出工具调用触发器（即'''python...'''标签）时，生成暂停。然后在沙盒中执行代码 c1，返回执行结果或错误 f1。这个反馈 f1，前缀为像'''output...'''这样的简单前缀，被反馈给策略。然后策略继续生成，基于历史和新反馈[t1 ⊕ c1 ⊕ f1 ⊕ ...]进行条件化。这个迭代过程重复，直到策略生成最终答案 o（即在{boxed...}标签中），产生完整的、混合推理轨迹。这种动态、交错的反馈机制允许智能体迭代完善其推理并纠正错误，模仿自我纠正的"顿悟时刻"。

**伪标签优势**：在生成 k 个完整轨迹并识别其 k 个最终答案{oi}\_{i=1}^k 后，我们使用先前确定的多数答案$\tilde{y}$作为伪标签。然后我们根据每个轨迹的答案 oi 是否与此伪标签匹配，为其分配终端奖励$R_i = I(o_i = \tilde{y})$。这个结果奖励 Ri 用于计算整个多步轨迹 i 的优势 Ai。

#### 3.3.2. 歧义动态策略优化（ADPO）

标准 GRPO 平等对待所有训练样本。然而，在我们的自我进化设置中，我们依赖多数投票来推导伪标签，这引入了两个关键问题：标签噪声和在模糊任务上的受限探索。为了解决这些问题，我们提出了歧义动态策略优化（ADPO），它结合了受数据歧义信号$\hat{p}(x)$驱动的两个关键修改。

**歧义感知优势缩放**：第一个问题是，对于高歧义任务（低$\hat{p}(x)$），多数答案容易出错。使用标准 GRPO 直接在这些噪声标签上优化有加强错误推理的风险。为了防止对可能不准确的伪标签过拟合，我们缩放标准化优势 Âi。我们定义缩放因子 s(x) = f($\hat{p}(x)$)，其中 f 是自一致性的递增函数。优势被修改为$\tilde{A}_i(x) = \hat{A}_i \cdot s(x)$。这按比例下调了来自不可靠、低一致性样本的训练信号。

**歧义调制信任区域**：第二个问题涉及标准近似算法施加的刚性约束。虽然静态裁剪（如 ε）旨在确保稳定性，但它创建了学习的非对称障碍。如图 3 所示，经验分析显示上裁剪边界主要由低概率标记触发。这表明标准机制不成比例地"钳制"不太可能标记的增长，有效抑制了新推理路径的出现。

这种限制对于高歧义任务（低$\hat{p}(x)$）特别有害，其中正确的推理通常位于当前策略分布的尾部，需要显著更新才能浮现。为了解决这个瓶颈，ADPO 动态调制信任区域。我们将上裁剪边界 εhigh(x)定义为$\hat{p}(x)$的递减函数。这实际上放松了对模糊输入的约束，允许更大的梯度步骤来提升潜在的低概率解决方案，同时对置信样本保持紧密边界以保持稳定性。

执行者智能体通过最小化 ADPO 目标进行更新：

$$L_{\text{ADPO}}(θ) = \mathbb{E}_{x∼D^{(t)}} \left[-\frac{1}{G} \sum_{i=1}^G \min\left(r_i(θ)\tilde{A}_i(x), \text{clip}\left[r_i(θ), 1-\epsilon_{\text{low}}, 1+\epsilon_{\text{high}}(x)\right]\tilde{A}_i(x)\right)\right]$$

其中 ri(θ)是重要性采样比率，$\tilde{A}_i(x)$是歧义缩放优势，εhigh(x)是与$\hat{p}(x)$成反比的动态上界。

### 算法 1：自我进化框架 Agent0

**输入**：基础 LLM πbase；迭代次数 T；样本数 k。

```
1: 初始化 πθ^{(0)} ← πbase 和 πφ^{(0)} ← πbase。
2: 对于每次迭代 t = 1, ..., T 执行：
3:   ▷课程进化（训练πθ）
4:   初始化 πθ ← πθ^{(t-1)}
5:   生成一批任务 X = {xi} ∼ πθ
6:   对于任务 xi ∈ X 执行：
7:     从 πφ^{(t-1)} 采样k个响应 {yj}_{j=1}^k ∼ πφ^{(t-1)}(xi)
8:     使用公式5计算RC(xi)
9:   结束对于
10:  使用LGRPO通过(X, RC)更新πθ → πθ^{(t)}
11:  ▷执行者进化（训练πφ）
12:  生成Xpool ∼ πθ^{(t)}并筛选为D^{(t)} = {(x, $\hat{p}$, $\tilde{y}$)}，其中|$\hat{p}$(x) - 0.5| ≤ δ
13:  初始化 πφ ← πφ^{(t-1)}
14:  对于批次BD = {(x, $\hat{p}$(x), $\tilde{y}$)} ∼ D^{(t)} 执行：
15:    初始化Tbatch, $\tilde{A}$batch, Pbatch
16:    对于(x, $\hat{p}$(x), $\tilde{y}$) ∈ BD执行：
17:      从πφ采样k个轨迹{τi}_{i=1}^k ∼ πφ(x)
18:      计算奖励Ri = I(oi = $\tilde{y}$)
19:      计算缩放优势$\tilde{A}_i$ ← Ai · f($\hat{p}$(x))
20:      将{τi}添加到Tbatch，{$\tilde{A}_i$}添加到$\tilde{A}$batch，$\hat{p}$(x)添加到Pbatch
21:    结束对于
22:  在收集的批次上使用LADPO（公式8）更新πφ
23:  结束对于
24:  πφ^{(t)} ← πφ
25: 结束对于
```

这个算法描述了 Agent0 的完整自我进化过程，展示了课程智能体和执行者智能体的共同进化循环。

![Agent0共同进化循环](images/LLM%20Agents/Unleashing%20Self-Evolving%20Agents%20from%20Zero%20Data%20via%20Tool-Integrated%20Reasoning/img_2_6_20251127_145855.png)

**图 2. Agent0 共同进化循环。** (1) 课程进化：课程智能体 πθ 通过 RL 训练生成任务，最大化基于执行者不确定性 Runc、工具使用 Rtool 和重复惩罚 Rrep 的奖励 RC。(2) 执行者进化：任务通过自一致性分数$\hat{p}$筛选以创建具有挑战性的数据集 D(t)。然后执行者智能体 πφ 在 D(t)上通过 ADPO 训练，这是一种使用多数投票伪标签$\tilde{y}$的歧义感知 RL 方法。

![上裁剪标记概率](images/LLM%20Agents/Unleashing%20Self-Evolving%20Agents%20from%20Zero%20Data%20via%20Tool-Integrated%20Reasoning/img_4_0_20251127_145855.png)

**图 3. 上裁剪标记概率。** 大多数上裁剪标记具有低概率，意味着标准裁剪限制了探索。

## 4. 实验

在本节中，我们评估 Agent0 的性能，旨在回答以下问题：(1) Agent0 的性能与最先进的自我进化基线相比如何？(2) 提出的共同进化循环是否在多次迭代中有效逐步改善智能体的性能？(3) 我们框架的每个关键组件的有效性如何？(4) Agent0 培养的数学推理能力是否能够泛化到改善一般领域推理任务上的性能？

### 4.1. 实验设置

**实现细节**：我们的框架 Agent0 基于 VeRL 实现。我们在两个基础模型上评估 Agent0：Qwen3-4B-Base 和 Qwen3-8B-Base。两个智能体都从这些基础模型初始化。在共同进化循环期间，对于每个任务 xi，我们从执行者采样 k = 10 个响应来计算不确定性并生成伪标签。任务过滤阈值设置为 δ = 0.25，保留自一致性$\hat{p}(x)$在 0.3 到 0.8 之间的任务。对于课程智能体，我们设置工具奖励缩放 λtool = 0.6 和上限 C = 4。对于执行者智能体，我们集成基于 VeRL-Tool 的沙盒代码解释器，允许它执行包含在'''python...'''标签中的代码片段并接收'''output...'''。

**基线方法**：我们将 Agent0 与几种最先进的自我改进方法进行比较：

1. **基础模型**：未经任何微调的预训练基础模型
2. **带工具的基础模型**：在零样本设置中评估的基础模型，但给予代码解释器访问权限
3. **自我进化方法**：R-Zero、Absolute Zero、SPIRAL 和 Socratic-Zero

**评估数据集和指标**：Agent0 不需要人工策划的数据集进行训练，但在多个基准测试上进行评估以展示其能力：

- **数学推理基准**：MATH、GSM8K、AMC、Minerva、Olympiad-Bench、AIME24、AIME25
- **一般推理基准**：SuperGPQA、MMLU-Pro、BBEH

### 4.2. 主要结果

![数学和一般推理基准上的性能](images/LLM%20Agents/Unleashing%20Self-Evolving%20Agents%20from%20Zero%20Data%20via%20Tool-Integrated%20Reasoning/img_6_0_20251127_145909.png)

**图 4. 数学和一般推理基准上的性能，显示 Qwen3-4B 和 Qwen3-8B 在三个共同进化迭代中的一致改进。**

如图 4 所示，我们的方法在迭代过程中表现出稳定和渐进的改进。在 Qwen3-8B-Base 上，平均数学分数从 55.1（第 1 次迭代）提高到 56.5（第 2 次迭代），在 58.2（第 3 次迭代）达到峰值。除了数学之外，Agent0 在其他一般领域推理任务上也显示出类似趋势，比前一次迭代平均每次改进 2%。这种迭代增益验证了我们共同进化循环的有效性。

#### 消融研究

如表 3 所示，我们进行了一系列消融实验来评估我们方法中每个组件的影响。具体来说，我们评估以下影响：(1) 课程智能体的训练，(2) 工具奖励，(3) 重复惩罚，(4) 我们的歧义缩放机制，以及(5) 多轮推理能力。

**表 3. Agent0 的消融研究**

| 方法              | 一般平均分 | 数学平均分 |
| ----------------- | ---------- | ---------- |
| Agent0            | 36.7       | 58.2       |
| 课程智能体不训练  | 29.5       | 46.8       |
| 不使用工具奖励    | 31.8       | 48.7       |
| 不使用重复惩罚    | 31.3       | 47.9       |
| 执行者不使用 ADPO | 34.9       | 56.2       |
| 不使用多轮        | 35.3       | 55.9       |

对于课程智能体，不训练时，性能显著下降 9.3%。这反映了学习课程的价值。接下来，当不包括工具奖励时，模型的性能下降 7.2%。这测试了我们的核心假设，即明确奖励工具使用任务是必要的。当我们移除多样性组件时，性能显示严重下降，表明 Rrep 对于课程多样性非常有效，特别是对于一般任务。

对于执行者智能体，使用具有标准优势和裁剪的原始 GRPO 训练它导致性能下降 1.9%。这是因为原始算法没有考虑伪标签的可靠性，证明了我们提出的歧义缩放机制的有效性。多轮推理的引入在提升 Agent0 性能方面发挥了重要作用，特别是对于需要多轮推理的复杂数学推理。

#### 战略性工具集成的重要性

我们的优势不仅在于拥有工具，还在于学会如何使用它。如表 4 所示，仅仅提供工具（即带工具的基础模型）产生轻微的性能提升。Agent0 显著优于其他使用工具的基线，如 Absolute Zero。Agent0 也显著超过像 R-Zero 和 SPIRAL 这样的非工具方法。

**表 4. 非工具和其他工具集成基线的比较**

| 模型                     | MATH | 一般 |
| ------------------------ | ---- | ---- |
| Qwen3-4B                 | 42.6 | 22.0 |
| 不使用工具 - SPIRAL      | 47.0 | 30.0 |
| 不使用工具 - R-Zero      | 49.1 | 29.8 |
| 使用工具 - TIR           | 44.2 | 25.7 |
| 使用工具 - Absolute Zero | 46.4 | 29.3 |
| 使用工具 - Agent0        | 52.5 | 32.6 |

这表明我们的课程智能体，通过使用 Rtool 奖励来明确激励生成需要工具使用的复杂任务，比那些仅使用工具进行验证（如 Absolute Zero）或根本不使用工具（如 R-Zero）的方法要有效得多。此外，执行者智能体将工具与多步推理结合使用，这也导致性能提升，从而产生共同进化。

#### 任务难度和工具使用的进化

我们分析了课程智能体在训练迭代期间生成的任务。我们为每次分析采样 200 个问题。如表 5 所示，当在由第 1、2、3 次迭代的课程智能体生成的任务集上评估时，执行者智能体（来自第 1 次迭代）的通过率逐渐下降。这表明任务难度在逐渐增加，证实了课程适应执行者能力的改进。

**表 5. 任务难度和工具使用的进化。我们报告固定执行者智能体（来自第 1 次迭代）在不同阶段课程智能体生成的数据集上的通过率。**

| 数据集        | 通过率（执行者第 1 次迭代） | 平均工具调用次数 |
| ------------- | --------------------------- | ---------------- |
| D 第 1 次迭代 | 64.0                        | 1.65             |
| D 第 2 次迭代 | 58.5                        | 2.10             |
| D 第 3 次迭代 | 51.0                        | 2.60             |

更重要的是，每个生成任务的平均工具调用次数在迭代中稳定增加。这直接证明了我们的 Rtool 奖励成功引导课程智能体生成更复杂和依赖工具的问题，从而驱动良性循环。

#### 定性分析

图 5 说明了任务复杂性和解决熟练程度的共同进化。课程智能体有效生成越来越困难的问题，从基本几何（第 1 次迭代）发展到复杂约束满足任务（第 3 次迭代）。同时，执行者 Agent0 展示了可靠的问题解决能力。在提供的示例中，智能体有效地将自然语言推理与 Python 代码解释器结合，验证了模型处理混合推理任务的能力。

![定性案例分析](images/LLM%20Agents/Unleashing%20Self-Evolving%20Agents%20from%20Zero%20Data%20via%20Tool-Integrated%20Reasoning/img_7_5_20251127_145909.png)

**图 5. 定性案例分析。** 左侧：生成的示例问题显示从第 1 次迭代到第 3 次迭代复杂性和多样性的明显增加。右侧：Agent0 解决过程的演示，利用数学推理和 Python 代码执行的混合方法来解决标准 MATH 问题。

## 5. 相关工作

### 从零数据自我进化

自我进化范式，其中 LLM 生成自己的训练数据，已经获得了显著关注。这种方法范围从可验证域中的双智能体"编程者-测试者"设置到完全自主的框架，这些框架学会从头开始生成新问题。

为了指导这种学习，许多方法使用无标签强化学习，依赖于启发式奖励信号，如输出置信度或一致性。然而，这些系统关键受到模型固有知识的限制，导致课程停滞，因为任务很少能超越模型的当前复杂性。

Agent0 通过集成外部工具来打破这个上限，提供外部问题解决能力。然而，没有外部工具，这种闭环系统有模式崩溃和课程停滞的风险，因为它们仍然受限于模型的固有知识。Agent0 通过集成外部工具来引入客观问题解决能力打破了这个上限。

### 工具集成推理（TIR）

应用强化学习来增强 LLM 工具使用是一个不断发展的领域。许多方法依赖于特定领域的数据或监督微调。更一般的零 RL 设置在多轮场景中是出了名的不稳定。

TIR 的最新进展通过三个关键维度解决了这些挑战：稳定性、泛化和复杂性。为了稳定学习动态，像 ASPO 和 SimpleTIR 这样的方法为无效轮次引入理论保证和梯度过滤。除了稳定性之外，某些方法证明了工具使用技能的跨域可转移性。最后，为了处理复杂的多轮场景，先进技术优化长期规划、内存管理和交互效率。

## 6. 结论

我们介绍了 Agent0，这是一个完全自主的框架，其中课程智能体和执行者智能体在没有任何人工策划数据的情况下共同进化。我们将代码解释器集成到循环中，这创建了良性循环：配备工具的执行者的改进能力驱动课程智能体生成更困难的任务。

我们的实验表明，Agent0 显著增强了基础 LLM 的推理能力。它展示了进化高能力智能体的可扩展和有效路径，打破了依赖人工标注数据集的限制。

## 致谢

我们感谢 Chengsong Huang 的有益讨论。这项工作部分得到了 Renaissance Philanthropy 的人工智能数学基金支持。作者也感谢国家人工智能研究资源（NAIRR）试点、Purdue Anvil AI 对本研究结果的贡献。

## 参考文献

[此处省略完整的参考文献列表，实际论文中包含了详细的参考文献引用]
