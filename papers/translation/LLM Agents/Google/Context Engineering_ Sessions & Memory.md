# 上下文工程：会话与内存

**作者：Kimberly Milam 和 Antonio Gulli**

2025年11月

## 致谢

**内容贡献者**
- Kaitlin Ardiff
- Shangjie Chen
- Yanfei Chen
- Derek Egan
- Hangfei Lin
- Ivan Nardini
- Anant Nawalgaria
- Kanchana Patlolla
- Huang Xia
- Jun Yan
- Bo Yang
- Michael Zimmermann

**策展人和编辑**
- Anant Nawalgaria
- Kanchana Patlolla

**设计师**
- Michael Lanning

---

## 目录

### 引言
### 上下文工程
### 会话
- 跨框架和模型的差异
- 多智能体系统的会话
- 多个智能体框架之间的互操作性
- 会话的生产环境考虑因素
- 管理长上下文对话：权衡和优化

### 内存
- 内存类型
- 信息类型
- 组织模式
- 存储架构
- 创建机制
- 内存范围
- 多模态内存
- 内存生成：提取与整合
- 深度探讨：内存提取
- 深度探讨：内存整合
- 内存溯源
- 内存管理过程中的内存谱系计算
- 推理过程中的内存谱系计算
- 触发内存生成
- 内存即工具
- 后台 vs 阻塞操作
- 内存检索
- 检索时机
- 带内存的推理
- 系统指令中的内存
- 对话历史中的内存
- 程序性内存

### 测试与评估
### 内存的生产环境考虑因素
### 结论
### 结束语

---

## 引言

本白皮书探讨了会话和内存在构建有状态、智能LLM智能体中的关键作用，旨在赋能开发者创建更强大、个性化和持久化的AI体验。为了使大语言模型（LLMs）能够记忆、学习并个性化交互，开发者必须在其上下文窗口内动态组装和管理信息——这个过程被称为上下文工程。

以下是本白皮书总结的核心概念：

- **上下文工程**：在LLM的上下文窗口内动态组装和管理信息的过程，以实现有状态的智能智能体。

- **会话**：与智能体进行整个对话的容器，保存对话的时间历史记录和智能体的工作内存。

有状态和个人AI始于上下文工程。

- **内存**：长期持久化的机制，捕获和整合跨多个会话的关键信息，为LLM智能体提供连续和个性化的体验。

### 上下文工程

LLMs本质上是无状态的。在其训练数据之外，它们的推理和意识仅限于单次API调用的"上下文窗口"内提供的信息。这提出了一个根本性问题，因为AI智能体必须配备识别可执行操作的操作指令、用于推理的证据和事实数据，以及定义当前任务的即时对话信息。要构建能够记忆、学习并个性化交互的有状态智能智能体，开发者必须为对话的每个轮次构建这种上下文。这种为LLM动态组装和管理信息的过程被称为上下文工程。

上下文工程代表了从传统提示工程的演变。提示工程专注于制作最佳的、通常是静态的系统指令。相反，上下文工程解决整个有效载荷，基于用户、对话历史和外部数据动态构建有状态感知的提示。它涉及战略性地选择、总结和注入不同类型的信息，以最大化相关性同时最小化噪声。外部系统——如RAG数据库、会话存储和内存管理器——管理大部分这种上下文。智能体框架必须协调这些系统来检索和组装上下文到最终的提示中。

将上下文工程想象为智能体的mise en place（备料工作）——厨师在烹饪前收集和准备所有食材的关键步骤。如果你只给厨师食谱（提示），他们可能会用随机的食材做出还可以的餐点。然而，如果你首先确保他们有所有合适的、高质量的食材、专门的工具和清晰的演示风格理解，他们可以可靠地制作出优秀、定制化的结果。上下文工程的目标是确保模型拥有完成任务所需的最相关、不太多也不太少的信息。

上下文工程管理着复杂有效载荷的组装，可能包括各种组件：

- **指导推理的上下文**定义了智能体的基本推理模式和可用操作，指导其行为：
  - **系统指令**：定义智能体角色、能力和约束的高级指令。
  - **工具定义**：智能体用来与外部世界交互的API或函数的模式。
  - **少样本示例**：通过上下文学习指导模型推理过程的精选示例。

- **证据与事实数据**是智能体推理的实质数据，包括预存在知识和为特定任务动态检索的信息；它作为智能体响应的"证据"：
  - **长期内存**：跨多个会话收集的关于用户或主题的持久化知识。
  - **外部知识**：从数据库或文档检索的信息，通常使用检索增强生成（RAG）¹。
  - **工具输出**：工具返回的数据或结果。
  - **子智能体输出**：被委托执行特定子任务的专业智能体返回的结论或结果。
  - **工件**：与用户或会话关联的非文本数据（如文件、图像）。

- **即时对话信息**将智能体锚定在当前交互中，定义即时任务：
  - **对话历史**：当前交互的轮次记录。
  - **状态/草稿板**：智能体用于其即时推理过程的临时、进行中信息或计算。
  - **用户提示**：需要解决的即时查询。

上下文的动态构建至关重要。例如，内存不是静态的；它们必须随着用户与智能体交互或新数据被摄入而选择性地检索和更新。此外，有效的推理通常依赖于上下文学习²（LLM从提示中的演示学习如何执行任务的过程）。当智能体使用与当前任务相关的少样本示例，而不是依赖硬编码的示例时，上下文学习可能更有效。类似地，外部知识由RAG工具基于用户的即时查询检索。

构建上下文感知智能体最关键的挑战之一是管理不断增长的对话历史。理论上，具有大上下文窗口的模型可以处理广泛的对话记录；实际上，随着上下文的增长，成本和延迟增加。此外，模型可能遭受"上下文腐化"，即随着上下文增长，它们对关键信息的注意力减弱的现象。上下文工程直接通过采用策略来动态改变历史记录——如总结、选择性修剪或其他压缩技术——来解决这一问题，以保留重要信息同时管理总体token数量，最终实现更强大和个性化的AI体验。

这种实践表现为智能体操作循环中每个对话轮次的连续循环：

**图1. 智能体上下文管理流程**

1. **获取上下文**：智能体首先检索上下文——如用户内存、RAG文档和最近的对话事件。对于动态上下文检索，智能体将使用用户查询和其他元数据来识别要检索的信息。

2. **准备上下文**：智能体框架动态构建LLM调用的完整提示。虽然单个API调用可能是异步的，但准备上下文是阻塞的、"热路径"过程。智能体必须等到上下文准备就绪才能继续。

3. **调用LLM和工具**：智能体迭代调用LLM和任何必要的工具，直到为用户生成最终响应。工具和模型输出被附加到上下文中。

4. **上传上下文**：在轮次期间收集的新信息被上传到持久化存储。这通常是"后台"过程，允许智能体在内存整合或其他后处理异步进行的同时完成执行。

在这个生命周期的心脏是两个基本组件：会话和内存。会话管理单个对话的轮次状态。相反，内存提供了长期持久化的机制，捕获和整合跨多个会话的关键信息。

你可以将会话想象为用于特定项目的工作台或办公桌。当你工作时，上面覆盖着所有必要的工具、笔记和参考资料。一切都立即可访问，但也是临时的，专门针对当前任务。一旦项目完成，你不会把整个凌乱的办公桌塞进存储。相反，你开始创建内存的过程，这就像一个有组织的文件柜。你回顾办公桌上的材料，丢弃草稿和冗余笔记，只将最关键的、最终文档归档到标记的文件夹中。这确保文件柜保持干净、可靠和高效的真相来源，不会被工作台的暂时混乱所杂化。这个类比直接反映了有效智能体的操作方式：会话作为单个对话的临时工作台，而智能体的内存是精心组织的文件柜，允许它在未来的交互中回忆关键信息。

基于上下文工程的高级概述，我们现在可以探索两个核心组件：会话和内存，从会话开始。

## 会话

上下文工程的基础元素是会话，它封装了单个连续对话的即时对话历史和工作内存。每个会话是与特定用户绑定的自包含记录。会话允许智能体在单个对话范围内保持上下文并提供连贯的响应。一个用户可以有多个会话，但每个都作为特定交互的不同、断开连接的日志。每个会话包含两个关键组件：时间历史（事件）和智能体的工作内存（状态）。

事件是对话的构建块。常见的事件类型包括：用户输入（来自用户的消息（文本、音频、图像等））、智能体响应（智能体对用户的回复）、工具调用（智能体决定使用外部工具或API）、或工具输出（从工具调用返回的数据，智能体用它来继续推理）。

除了聊天历史，会话通常包括状态——结构化的"工作内存"或草稿板。这保存与当前对话相关的临时结构化数据，比如购物车中的物品。

随着对话的进行，智能体将会向会话添加额外事件。此外，它可能根据智能体中的逻辑改变状态。

事件的结构类似于传递给Gemini API的Content对象列表，其中每个具有角色和部分的项目代表对话中的一个轮次——或一个事件。

**代码片段1：向Gemini进行多轮调用的示例**

```python
contents = [
    {
        "role": "user",
        "parts": [ {"text": "法国的首都是什么？"} ]
    }, {
        "role": "model",
        "parts": [ {"text": "法国的首都是巴黎。"} ]
    }
]
response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=contents
)
```

生产智能体的执行环境通常是无状态的，意味着它在请求完成后不保留任何信息。因此，必须将其对话历史保存到持久化存储以维持连续的用户体验。虽然内存存储适合开发，但生产应用程序应该利用健壮的数据库来可靠地存储和管理会话。例如，你可以将对话历史存储在Agent Engine Sessions³等托管解决方案中。

### 跨框架和模型的差异

虽然核心思想相似，但不同的智能体框架以不同方式实现会话、事件和状态。智能体框架负责维护LLMs的对话历史和状态，使用此上下文构建LLM请求，以及解析和存储LLM响应。

智能体框架充当你的代码和LLM之间的通用翻译器。作为开发者，你使用框架一致的、内部数据结构处理每个对话轮次，而框架处理将这些结构转换为LLM所需精确格式的关键任务。这种抽象很强大，因为它将你的智能体逻辑与你正在使用的特定LLM解耦，防止供应商锁定。

**图2：智能体的上下文管理流程**

最终目标是生成LLM能够理解的"请求"。对于Google的Gemini模型，这是List[Content]。每个Content对象是一个简单的字典状结构，包含两个键：role（定义谁在说话（"user"或"model"））和parts（定义消息的实际内容（文本、图像、工具调用等））。

框架在发出API调用之前自动处理从其内部对象（例如ADK事件）到Content对象中相应角色和部分的数据映射。本质上，框架为开发者提供了稳定的内部API，同时在幕后管理不同LLMs的复杂和多变的外部API。

ADK使用显式的Session对象，包含Event对象列表和单独的状态对象。会话就像文件柜，一个文件夹用于对话历史（事件），另一个用于工作内存（状态）。

LangGraph没有正式的"会话"对象。相反，状态就是会话。这个全能的状态对象保存对话历史（作为Message对象列表）和所有其他工作数据。与传统会话的仅追加日志不同，LangGraph的状态是可变的。它可以被转换，历史压缩等策略可以改变记录。这对于管理长对话和token限制很有用。

### 多智能体系统的会话

在多智能体系统中，多个智能体协作。每个智能体专注于更小、专门化的任务。为了使这些智能体有效地协同工作，它们必须共享信息。如下图所示，系统的架构定义了它们用来共享信息的通信模式。这个架构的一个中心部分是系统如何处理会话历史——所有交互的持久化日志。

**图3：不同的多智能体架构模式³⁰**

在探索管理此历史的架构模式之前，将其与发送到LLM的上下文区分开来至关重要。将会话历史想象为整个对话的永久、完整记录。另一方面，上下文是精心制作的信息有效载荷，为单个轮次发送到LLM。智能体可能通过从历史中选择相关摘录或添加特殊格式（如指导性前言）来构建此上下文，以引导模型的响应。本节专注于跨智能体传递什么信息，而不一定是发送到LLM的上下文。

智能体框架使用两种主要方法之一处理多智能体系统的会话历史：所有智能体都为单一日志做出贡献的共享、统一历史，或每个智能体维护自己视角的独立、个别历史⁴。在这两种模式之间的选择取决于任务的性质和智能体之间期望的协作风格。

对于共享、统一历史模型，系统中的所有智能体都从同一单个对话历史读取并将所有事件写入其中。每个智能体的消息、工具调用和观察都按时间顺序附加到一个中央日志中。这种方法最适合需要单一真相源的紧密耦合、协作任务，比如一个智能体的输出是下一个的直接输入的多步问题解决过程。即使有共享历史，子智能体可能在将日志传递给LLM之前处理它。例如，它可以过滤相关事件的子集或添加标签来识别每个事件是由哪个智能体生成的。

如果你使用ADK的LLM驱动的委托移交给子智能体，子智能体的所有中间事件将写入与根智能体相同的会话⁵：

**代码片段2：跨多个智能体框架的A2A通信**

```python
from google.adk.agents import LlmAgent
# 子智能体可以访问会话并向其写入事件。
sub_agent_1 = LlmAgent(...)
# 可选地，子智能体可以将最终响应文本（或结构化输出）保存到指定的状态键。
sub_agent_2 = LlmAgent(
  ...,
  output_key="..."
)
# 父智能体。
root_agent = LlmAgent(
  ...,
  sub_agents=[sub_agent_1, sub_agent_2]
)
```

在独立、个别历史模型中，每个智能体维护自己的私人对话历史，对其他智能体作为黑盒运行。所有内部过程——如中间思维、工具使用和推理步骤——保留在智能体的私人日志中，对其他智能体不可见。通信仅通过显式消息发生，其中智能体共享其最终输出，而不是其过程。

这种交互通常通过实现智能体即工具或使用智能体到智能体（A2A）协议来实现。使用智能体即工具，一个智能体调用另一个，就好像它是一个标准工具，传递输入并接收最终的、自包含的输出⁶。使用智能体到智能体（A2A）协议，智能体使用结构化协议进行直接消息传递⁷。

### 多个智能体框架之间的互操作性

**图4：使用不同框架的多个智能体之间的A2A通信**

框架使用内部数据表示引入了多智能体系统的关键架构权衡：将智能体与LLM解耦的抽象也将其与使用其他智能体框架的智能体隔离开来。这种隔离在持久化层得到巩固。会话的存储模型通常将数据库模式直接耦合到框架的内部对象，创建刚性、相对不可移植的对话记录。因此，使用LangGraph构建的智能体无法原生解释基于ADK的智能体持久化的不同会话和事件对象，使得无缝任务移交变得不可能。

协调这些隔离智能体之间协作的一个新兴架构模式是智能体到智能体（A2A）通信⁸。虽然此模式允许智能体交换消息，但它未能解决共享丰富、上下文状态的核心问题。每个智能体的对话历史都以其框架的内部模式编码。因此，任何包含会话事件的A2A消息都需要翻译层才能有用。

互操作性的更健壮架构模式涉及将共享知识抽象为框架无关的数据层，如内存。与会话存储（保留原始的、框架特定的对象如事件和消息）不同，内存层设计为保存处理的、规范的信息。关键信息——如总结、提取的实体和事实——从对话中提取，通常作为字符串或字典存储。内存层的数据结构不耦合到任何单一框架的内部数据表示，这使它能够作为通用、公共数据层服务。这种模式允许异构智能体通过共享公共认知资源实现真正的协作智能，而无需自定义翻译器。

### 会话的生产环境考虑因素

当将智能体移动到生产环境时，其会话管理系统必须从简单日志演变为健壮、企业级服务。关键考虑因素分为三个关键领域：安全和隐私、数据完整性和性能。托管的会话存储，如Agent Engine Sessions，专门设计用于解决这些生产要求。

#### 安全和隐私

保护会话中包含的敏感信息是不可协商的要求。**严格隔离**是最关键的安全原则。会话由单个用户拥有，系统必须强制执行严格隔离，确保一个用户永远无法访问另一个用户的会话数据（即通过ACL）。对会话存储的每个请求都必须根据会话的所有者进行身份验证和授权。

处理个人可识别信息（PII）的最佳实践是在会话数据写入存储之前编辑它。这是基本的安全措施，极大地减少了潜在数据泄露的风险和"爆炸半径"。通过确保敏感数据从不使用Model Armor⁹等工具持久化，你简化了与GDPR和CCPA等隐私法规的合规性并建立用户信任。

#### 数据完整性和生命周期管理

生产系统需要明确的规则来规定会话数据如何随时间存储和维护。会话不应该永远存在。你可以实施生存时间（TTL）策略来自动删除非活动会话以管理存储成本和减少数据管理开销。这需要明确的数据保留策略，定义会话在归档或永久删除之前应保留多长时间。

此外，系统必须保证操作以确定性顺序附加到会话历史。维护事件的正确时间顺序对对话日志的完整性至关重要。

#### 性能和可扩展性

会话数据在每次用户交互的"热路径"上，使其性能成为主要关注点。读写会话历史必须极快以确保响应式用户体验。智能体运行时通常是无状态的，因此整个会话历史在每个轮次开始时从中央数据库检索，产生网络传输延迟。

为了缓解延迟，关键是减少传输数据的大小。一个关键的优化是在发送到智能体之前过滤或压缩会话历史。例如，你可以移除对于当前对话状态不再需要的旧的、不相关的函数调用输出。下一节详细介绍了几种压缩历史记录的策略，以有效管理长上下文对话。

### 管理长上下文对话：权衡和优化

在简单的架构中，会话是用户和智能体之间对话的不可变日志。然而，随着对话规模扩大，对话的token使用量增加。现代LLMs可以处理长上下文，但限制存在，特别是对于延迟敏感的应用程序¹⁰：

1. **上下文窗口限制**：每个LLM一次可以处理的最大文本量（上下文窗口）。如果对话历史超过此限制，API调用将失败。

2. **API成本（$）**：大多数LLM提供商标记你发送和接收的token数量。较短的历史记录意味着每个轮次更少的token和更低的成本。

3. **延迟（速度）**：向模型发送更多文本需要更长时间处理，导致用户响应时间更慢。压缩使智能体保持快速和响应。

4. **质量**：随着token数量增加，性能可能因上下文中的额外噪声和自回归错误而变差。

管理与智能体的长对话可以比作 savvy 旅行者为长途旅行打包手提箱。手提箱代表智能体有限的上下文窗口，衣物和物品是来自对话的信息片段。如果你只是试图把所有东西都塞进去，手提箱变得太重和杂乱，很难快速找到你需要的东西——就像过载的上下文窗口增加处理成本和减慢响应时间。另一方面，如果你打包太少，你可能会把护照或保暖外套等必需品抛在后面，损害整个旅行——就像智能体可能丢失关键上下文，导致不相关或不正确的答案。旅行者和智能体都在类似的约束下运行：成功不在于你能携带多少，而在于只携带你需要的东西。

压缩策略缩小长对话历史，将对话压缩以适应模型的上下文窗口，减少API成本和延迟。随着对话变长，每个轮次发送给模型的历史可能变得太大。压缩策略通过智能修剪历史同时尝试保留最重要上下文来解决这个问题。

那么，你如何知道会话中丢弃什么内容而不丢失有价值的信息？策略范围从简单截断到复杂压缩：

- **保留最后N个轮次**：这是最简单的策略。智能体只保留对话中最新的N个轮次（"滑动窗口"），丢弃所有较旧的。

- **基于Token的截断**：在将历史发送给模型之前，智能体计算消息中的token，从最新开始向后工作。它包含尽可能多的消息而不超过预定义的token限制（例如4000个token）。所有较旧的都被切断。

- **递归总结**：对话的较旧部分被AI生成的摘要替换。随着对话增长，智能体定期使用另一个LLM调用来总结最旧的消息。然后这个摘要用作历史记录的压缩形式，通常前缀到更最近的、逐字的消息。

例如，你可以使用ADK为应用程序使用内置插件来保留最后N个轮次，这将限制发送给模型的上下文。这不会修改会话存储中存储的历史事件：

**代码片段3：使用ADK仅使用最后N个轮次的会话截断**

```python
from google.adk.apps import App
from google.adk.plugins.context_filter_plugin import ContextFilterPlugin

app = App(
    name='hello_world_app',
    root_agent=agent,
    plugins=[
        # 保留最后10个轮次和最近的用户查询。
        ContextFilterPlugin(num_invocations_to_keep=10),
    ],
)
```

考虑到复杂压缩策略旨在减少成本和延迟，关键是在后台异步执行昂贵的操作（如递归总结）并持久化结果。"在后台"确保客户端不被等待，"持久化"确保昂贵的计算不会过度重复。通常，智能体的内存管理器负责生成和持久化这些递归摘要。智能体还必须记录哪些事件包含在压缩摘要中；这防止原始、更详细的事件被不必要地发送到LLM。

此外，智能体必须决定何时压缩是必要的。触发机制通常分为几个不同的类别：

- **基于计数的触发器**（即token大小或轮次计数阈值）：一旦对话超过特定预定义阈值就压缩对话。这种方法对于管理上下文长度通常"足够好"。

- **基于时间的触发器**：压缩不是由对话大小触发，而是由缺乏活动触发。如果用户停止交互一段时间（例如15或30分钟），系统可以在后台运行压缩作业。

- **基于事件的触发器**（即语义/任务完成）：当智能体检测到特定任务、子目标或对话主题已结束时，决定触发压缩。

例如，你可以使用ADK的EventsCompactionConfig在配置数量的轮次后触发基于LLM的总结：

**代码片段4：使用ADK和总结的会话压缩**

```python
from google.adk.apps import App
from google.adk.apps.app import EventsCompactionConfig

app = App(
    name='hello_world_app',
    root_agent=agent,
    events_compaction_config=EventsCompactionConfig(
        compaction_interval=5,
        overlap_size=1,
    ),
)
```

内存生成是从冗长和嘈杂数据源提取持久知识的广泛能力。在本节中，我们介绍了从对话历史提取信息的一个主要例子：会话压缩。压缩提炼整个对话的逐字记录，提取关键事实和摘要，同时丢弃对话填充物。

在压缩的基础上，下一节将更广泛地探讨内存生成和管理。我们将讨论创建、存储和检索内存的各种方法，以构建智能体的长期知识。

## 内存

内存和会话共享深度共生关系：会话是生成内存的主要数据源，而内存是管理会话大小的关键策略。内存是从对话或数据源提取的有意义信息的快照。它是保留重要上下文的压缩表示，使其对未来交互有用。通常，内存在会话之间持久化，以提供连续和个性化的体验。

作为专门的、解耦的服务，"内存管理器"为多智能体互操作性提供基础。内存管理器经常使用框架无关的数据结构，如简单字符串和字典。这允许基于不同框架构建的智能体连接到单个内存存储，使任何连接的智能体都能利用的共享知识库的创建成为可能。

注意：一些框架可能也会将会话或逐字对话称为"短期内存"。对于本白皮书，内存定义为提取的信息，而不是逐次对话的原始对话。

存储和检索内存对于构建复杂和智能智能体至关重要。健壮的内存系统通过解锁几个关键能力将基本聊天机器人转变为真正的智能智能体：

- **个性化**：最常见的用例是记住用户偏好、事实和过去交互，以定制未来响应。例如，记住用户最喜欢的运动队或他们在飞机上的首选座位创造更有帮助和个性化的体验。

- **上下文窗口管理**：随着对话变长，完整历史可能超过LLM的上下文窗口。内存系统可以通过创建摘要或提取关键事实来压缩此历史，保留上下文而不在每个轮次发送数千个token。这既减少了成本也减少了延迟。

- **数据挖掘和洞察**：通过分析许多用户的存储内存（以聚合、隐私保护的方式），你可以从噪声中提取洞察。例如，零售聊天机器人可能识别许多用户询问特定产品的退货政策，标记潜在问题。

- **智能体自我改进和适应**：智能体通过创建关于其自身性能的程序性内存——记录哪些策略、工具或推理路径导致成功结果——从先前运行中学习。这使智能体能够建立有效解决方案的战术手册，允许它随时间适应和改进其问题解决。

在AI系统中创建、存储和利用内存是协作过程。堆栈中的每个组件——从最终用户到开发者的代码——都有不同的角色要扮演。

1. **用户**：提供内存的原始源数据。在一些系统中，用户可能直接提供内存（即通过表单）。

2. **智能体（开发者逻辑）**：配置如何决定记住什么和何时记住，编排对内存管理器的调用。在简单架构中，开发者可以实现逻辑，使内存*总是*被检索和*总是*触发生成。在更高级的架构中，开发者可以实现内存即工具，其中智能体（通过LLM）决定何时应该检索或生成内存。

3. **智能体框架**（例如ADK、LangGraph）：为内存交互提供结构和工具。框架充当管道工。它定义开发者的逻辑如何访问对话历史并与内存管理器交互，但它本身不管理长期存储。它还定义了如何将检索到的内存填充到上下文窗口中。

4. **会话存储**（即Agent Engine Sessions、Spanner、Redis）：存储会话的轮次对话。原始对话将被摄入内存管理器以生成内存。

5. **内存管理器**（例如Agent Engine Memory Bank、Mem0、Zep）：处理内存的存储、检索和压缩。存储和检索内存的机制取决于使用什么提供商。这是专业的服务或组件，接收智能体识别的潜在内存并处理其整个生命周期。
   - **提取**从源数据中提炼关键信息。
   - **整合**管理内存以合并重复实体。
   - **存储**将内存持久化到持久数据库。
   - **检索**获取相关内存为新交互提供上下文

**图5：会话、内存和外部知识之间的信息流**

责任的分工确保开发者可以专注于智能体的独特逻辑，而无需构建内存持久化和管理的复杂底层基础设施。重要的是认识到内存管理器是一个活动系统，而不仅仅是被动向量数据库。虽然它使用相似性搜索进行检索，但其核心价值在于随时间智能提取、整合和管理内存的能力。托管内存服务，如Agent Engine Memory Bank，处理内存生成和存储的整个生命周期，使你能够专注于智能体的核心逻辑。

这种检索能力也是为什么内存经常与另一个关键架构模式比较：检索增强生成（RAG）。然而，它们建立在不同的架构原则上，因为RAG处理静态、外部数据，而内存管理动态、用户特定的上下文。它们满足两个不同和互补的角色：RAG使智能体成为事实专家，而内存使其成为用户专家。下表分解了它们的高级差异：

**表1：RAG引擎和内存管理器的比较**

| 方面 | RAG引擎 | 内存管理器 |
|------|---------|------------|
| **主要目标** | 向上下文注入外部、事实知识 | 创建个性化和有状态的体验。智能体记住事实，随时间适应用户，并维护长期上下文。 |
| **数据源** | 静态、预索引的外部知识库（例如PDFs、wikis、文档、APIs） | 用户和智能体之间的对话 |
| **隔离级别** | 通常共享。知识库通常是全局的、只读资源，所有用户可访问以确保一致、事实答案。 | 高度隔离：内存几乎总是按用户范围，以防止数据泄漏。 |
| **信息类型** | 静态、事实和权威。通常包含特定领域数据、产品细节或技术文档 | 动态和（通常）用户特定。内存从对话中派生，因此存在固有的不确定性水平。 |
| **写入模式** | 批处理<br>通过离线、管理操作触发 | 事件驱动处理<br>在某些节奏（即每个轮次或会话结束时）或内存即工具（智能体决定生成内存）时触发 |
| **读取模式** | RAG数据几乎总是"作为工具"检索。当智能体决定用户的查询需要外部信息时检索。 | 有两种常见读取模式：<br>• 内存即工具：当用户的查询需要关于用户（或某些其他身份）的额外信息时检索<br>• 静态检索：内存总是在每个轮次开始时检索 |
| **数据格式** | 自然语言"块" | 自然语言片段或结构化配置文件 |
| **数据准备** | 分块和索引：源文档被分解为更小的块，然后转换为嵌入并存储以供快速查找 | 提取和整合：从对话中提取关键细节，确保内容不重复或不矛盾 |

理解差异的一个有用方法是将RAG想象为智能体的研究图书管理员，将内存管理器想象为其个人助理。

研究图书管理员（RAG）在充满百科全书、教科书和官方文档的广阔公共图书馆工作。当智能体需要既定事实——如产品的技术规格或历史日期——它咨询图书管理员。图书管理员从这个静态、共享和权威的知识库检索信息，提供一致、事实答案。图书管理员是世界事实的专家，但他们对询问问题的用户一无所知。

相反，个人助理（内存）跟随智能体并携带私人笔记本，记录与特定用户的每次交互细节。这个笔记本是动态和高度隔离的，包含个人偏好、过去对话和演变目标。当智能体需要回忆用户最喜欢的运动队或上周项目讨论的上下文时，它转向助理。助理的专业不是全球事实，而是用户本身。

最终，真正的智能智能体两者都需要。RAG为其提供世界专家知识，而内存为其提供对服务用户的专家理解。

下一节通过检查其核心组件来解构内存概念：它存储的信息类型、其组织模式、其存储和创建机制、其范围的战略定义，以及它如何处理多模态与文本数据。

### 内存类型

智能体的内存可以按信息如何存储和捕获来分类。这些不同类型的内存协同工作，创建对用户及其需求的丰富、上下文理解。在所有类型的内存中，规则是内存是描述性的，而不是预测性的。

"内存"是由内存管理器返回并被智能体用作上下文的原子上下文片段。虽然确切的模式可能不同，但单个内存通常包含两个主要组件：内容和元数据。

内容是从源数据（即会话的原始对话）提取的内存实质。关键是，内容设计为框架无关，使用任何智能体可以轻松摄取的简单数据结构。内容可以是结构化或非结构化数据。结构化内存包括通常存储在通用格式如字典或JSON中的信息。其模式通常由开发者定义，而不是特定框架。例如，{"seat_preference": "Window"}。非结构化内存是捕获较长交互、事件或主题本质的自然语言描述。例如，"用户更喜欢靠窗座位。"

元数据提供关于内存的上下文，通常存储为简单字符串。这可以包括内存的唯一标识符、内存"所有者"的标识符，以及描述内存内容或数据源的标签。

### 信息类型

除了基本结构外，内存可以按它们代表的基本知识类型进行分类。这种区别对于理解智能体如何使用内存至关重要，将内存分为两个主要功能类别，源于认知科学¹¹：陈述性内存（"知道什么"）和程序性内存（"知道如何"）。

**陈述性内存**是智能体对事实、数字和事件的知识。它是智能体可以明确陈述或"声明"的所有信息。如果内存是对"什么"问题的回答，那就是陈述性。这个类别包括一般世界知识（语义）和特定用户事实（实体/情景）。

**程序性内存**是智能体对技能和工作流的知识。它通过隐式演示如何正确执行任务来指导智能体的行为。如果内存有助于回答"如何"问题——如预订旅行的正确工具调用序列——那就是程序性的。

### 组织模式

一旦内存被创建，下一个问题是如何组织它。内存管理器通常采用以下一种或多种模式来组织内存：集合¹²、结构化用户配置文件或"滚动摘要"。这些模式定义了单个内存如何彼此关联以及与用户关联。

**集合¹³**模式将内容组织为单个用户的多个自包含、自然语言内存。每个内存是不同事件、摘要或观察，尽管集合中可能有单个高级主题的多个内存。集合允许存储和搜索与特定目标或主题相关的更大、结构化程度较低的信息池。

**结构化用户配置文件**模式将内存组织为一组关于用户的核心事实，如不断用新、稳定信息更新的联系卡。它设计用于快速查找基本、事实信息，如姓名、偏好和账户详情。

与结构化用户配置文件不同，**"滚动"摘要**模式将所有信息整合到单个、演变的内存中，代表整个用户-智能体关系的自然语言摘要。管理器不是创建新的、单独的内存，而是持续更新这个主文档。这种模式经常用于压缩长会话，保留重要信息同时管理总体token数量。

### 存储架构

此外，存储架构是决定智能体如何快速和智能地检索内存的关键决策。架构的选择定义了智能体是否擅长找到概念相似的想法、理解结构化关系或两者。

内存通常存储在向量数据库和/或知识图谱中。向量数据库帮助找到与查询概念相似的内存。知识图谱将内存存储为实体及其关系的网络。

**向量数据库**是最常见的方法，支持基于语义相似性而不是精确关键词的检索。内存被转换为嵌入向量，数据库找到与用户查询最接近的概念匹配。这擅长检索非结构化、自然语言内存，其中上下文和含义是关键（即"原子事实"¹⁴）。

**知识图谱**用于将内存存储为实体（节点）及其关系（边）的网络。检索涉及遍历此图以找到直接和间接连接，允许智能体推理不同事实如何链接。它适用于结构化、关系查询和理解数据内的复杂连接（即"知识三元组"¹⁵）。

你也可以通过用向量嵌入丰富知识图谱的结构化实体来结合两种方法形成混合方法。这使得系统能够同时执行关系和语义搜索。这提供了图的结构化推理和向量数据库的细致、概念搜索，提供两全其美。

### 创建机制

我们还可以按内存如何创建以及信息如何派生来分类内存。**显式内存**在用户给智能体直接命令记住某事时创建（例如，"记住我的结婚纪念日是10月26日"）。另一方面，**隐式内存**在智能体从对话中推断和提取信息而没有直接命令时创建（例如，"我的结婚纪念日是下周。你能帮我为我的伴侣找礼物吗？"）

内存也可以通过内存提取逻辑位于智能体框架内部还是外部来区分。**内部内存**指直接构建到智能体框架中的内存管理。它便于开始但通常缺乏高级功能。内部内存可以使用外部存储，但生成内存的机制在智能体内部。

**外部内存**涉及使用独立的、专门用于内存管理的服务（例如Agent Engine Memory Bank、Mem0、Zep）。智能体框架对此外部服务进行API调用来存储、检索和处理内存。这种方法提供更复杂的功能，如语义搜索、实体提取和自动总结，将内存管理的复杂任务卸载到专门构建的工具。

### 内存范围

你还需要考虑内存描述谁或什么。这对用于聚合和检索内存的实体（即用户、会话或应用程序）有影响。

**用户级范围**是最常见的实现，旨在为每个个体创建连续、个性化的体验；例如，"用户更喜欢中间座位。"内存绑定到特定用户ID并在其所有会话中持久化，允许智能体建立对其偏好和历史的长期理解。

**会话级范围**旨在压缩长对话；例如，"用户正在购物2025年11月7日至2025年11月14日之间纽约和巴黎之间的机票。他们更喜欢直飞和中间座位"。它创建了从单个会话提取的见解的持久记录，允许智能体用简洁的关键事实集替换冗长、token重的记录。关键是，这个内存与原始会话日志不同；它只包含对话的处理见解，而不是对话本身，其上下文隔离到特定会话。

**应用程序级范围**（或全局上下文），是应用程序所有用户可访问的内存；例如，"代号XYZ指的是项目..."。此范围用于提供共享上下文、广播系统范围信息或建立公共知识基线。应用程序级内存的常见用例是程序性内存，为智能体提供"如何做"指令；内存通常旨在帮助智能体为所有用户进行推理。这些内存必须清理所有敏感内容，以防止用户之间的数据泄漏。

### 多模态内存

"多模态内存"是描述智能体如何处理非文本信息（如图像、视频和音频）的关键概念。关键是区分内存派生的数据（其源）和内存存储为的数据（其内容）。

**来自多模态源的内存**是最常见的实现。智能体可以处理各种数据类型——文本、图像、音频——但它创建的内存是从该源派生的文本见解。例如，智能体可以处理用户的语音备忘录来创建内存。它不存储音频文件本身；相反，它转录音频并创建文本内存，如"用户对最近的运输延迟表示沮丧。"

**具有多模态内容的内存**是更高级的方法，其中内存本身包含非文本媒体。智能体不仅描述内容；它直接存储内容。例如，用户可以上传图像并说"记住这个设计用于我们的徽标。"智能体创建直接包含图像文件的内存，链接到用户的请求。

大多数当代内存管理器专注于处理多模态源同时产生文本内容。这是因为为特定内存生成和检索非结构化二进制数据如图像或音频需要专门的模型、算法和基础设施。将所有输入转换为通用、可搜索格式：文本要简单得多。例如，你可以使用Agent Engine Memory Bank从多模态输入¹⁶生成内存。输出内存将是提取的内容中的文本见解：

**代码片段5：Agent Engine Memory Bank的示例内存生成API调用**

```python
from google.genai import types

client = vertexai.Client(project=..., location=...)
response = client.agent_engines.memories.generate(
    name=agent_engine_name,
    direct_contents_source={
        "events": [{
            "content": types.Content(
                role="user",
                parts=[
                    types.Part.from_text(
                        "这是关于多模态输入的上下文。"
                    ),
                    types.Part.from_bytes(
                        data=CONTENT_AS_BYTES,
                        mime_type=MIME_TYPE
                    ),
                    types.Part.from_uri(
                        file_uri="file/path/to/content",
                        mime_type=MIME_TYPE
                    )
                ]
            )}
        ]
    },
    scope={"user_id": user_id}
)
```

下一节检查内存生成的机制，详细说明两个核心阶段：从源数据提取新信息，以及随后将该信息与现有内存语料库整合。

### 内存生成：提取与整合

内存生成自主地将原始对话数据转换为结构化、有意义的见解，发挥作用。将其想象为LLM驱动的ETL（提取、转换、加载）管道，设计用于提取和压缩内存。内存生成的ETL管道区分内存管理器与RAG引擎和传统数据库。

内存管理器使用LLM智能地决定何时添加、更新或合并内存，而不是要求开发者手动指定数据库操作。这种自动化是内存管理器的核心优势；它抽象了管理数据库内容、链接LLM调用和部署后台数据处理服务的复杂性。

**图6：内存生成的高级算法，从新数据源提取内存并与现有内存整合**

虽然具体算法因平台而异（例如Agent Engine Memory Bank、Mem0、Zep），但内存生成的高级过程通常遵循这四个阶段：

1. **摄入**：当客户端向内存管理器提供原始数据源（通常是对话历史）时，过程开始。

2. **提取和过滤**：内存管理器使用LLM从源数据中提取有意义的内容。关键是LLM不提取所有内容；它只捕获符合预定义主题定义的信息。如果摄入的数据包含与这些主题匹配的信息，则不创建内存。

3. **整合**：这是最复杂的阶段，内存管理器处理冲突解决和去重。它执行"自我编辑"过程，使用LLM将新提取的信息与现有内存比较。为确保用户的知识库保持连贯、准确并基于新信息随时间演变，管理器可以决定：
   - 将新见解合并到现有内存中。
   - 如果现有内存现在无效，则删除它。
   - 如果主题新颖，则创建全新内存。

4. **存储**：最后，新的或更新的内存被持久化到持久存储层（如向量数据库或知识图谱），以便在未来的交互中检索。

托管内存管理器，如Agent Engine Memory Bank，完全自动化此管道。它们提供将对话噪声转换为结构化知识的单一、连贯系统，使开发者能够专注于智能体逻辑，而不是自己构建和维护底层数据基础设施。例如，使用Memory Bank触发内存生成只需要简单的API调用¹⁷：

**代码片段6：使用Agent Engine Memory Bank生成内存**

```python
from google.cloud import vertexai

client = vertexai.Client(project=..., location=...)
client.agent_engines.memories.generate(
    name="projects/.../locations/...reasoningEngines/...",
    scope={"user_id": "123"},
    direct_contents_source={
        "events": [...]
    },
    config={
        # 在后台运行内存生成。
        "wait_for_completion": False
    }
}
```

内存生成过程可以比作勤奋园丁照料花园的工作。提取就像接收新种子和幼苗（来自对话的新信息）。园丁不只是随机地将它们扔到地块上。相反，他们通过拔除杂草（删除冗余或冲突数据）、修剪过度生长的树枝以改善现有植物健康（优化和总结现有内存），然后仔细地将新幼苗种植在最佳位置来进行整合。这种持续、深思熟虑的管理确保花园保持健康、有组织，并随时间继续繁荣，而不是变成过度生长、无法使用的混乱。这个异步过程在后台进行，确保花园总是为下次访问做好准备。

现在，让我们深入了解内存生成的两个关键步骤：提取和整合。

### 深度探讨：内存提取

内存提取的目标是回答基本问题："这次对话中的什么信息足够有意义成为内存？"这不是简单总结；它是有针对性的、智能过滤过程，设计用于分离信号（重要事实、偏好、目标）和噪声（寒暄、填充文本）。

"有意义"不是普遍概念；它完全由智能体的目的和用例定义。客户支持智能体需要记住的内容（例如订单号、技术问题）与个人健康教练需要记住的内容（例如长期目标、情绪状态）根本不同。因此，定制保存什么信息是创建真正有效智能体的关键。

内存管理器的LLM通过遵循精心构建的编程护栏和指令集来决定提取什么，通常嵌入在复杂的系统提示中。此提示通过为LLM提供一组主题定义来定义"有意义"的含义。使用基于模式和模板的提取，LLM被给予预定义的JSON模式或使用LLM功能如结构化输出¹⁸的模板；LLM被指导使用对话中的相应信息构造JSON。或者，使用自然语言主题定义，LLM通过主题的简单自然语言描述来引导。

使用少样本提示，LLM被"展示"使用示例提取什么信息。提示包括几个输入文本示例和应该提取的理想、高保真内存。LLM从示例中学习所需的提取模式，使其对难以用模式或简单描述描述的自定义或细微主题非常有效。

大多数内存管理器开箱即用地查找常见主题，如用户偏好、关键事实或目标。许多平台还允许开发者定义自己的自定义主题，定制提取过程以适应其特定领域。例如，你可以通过提供自己的主题定义和少样本示例来自定义Agent Engine Memory Bank认为有意义要持久化的信息¹⁹：

**代码片段7：自定义Agent Engine Memory Bank认为有意义要持久化的信息**

```python
from google.genai.types import Content, Part

# 有关更多信息，请参阅 https://cloud.google.com/agent-builder/agent-engine/memory-bank/set-up
memory_bank_config = {
    "customization_configs": [{
        "memory_topics": [
            {"managed_memory_topic": {"managed_topic_enum": "USER_PERSONAL_INFO"}},
            {
                "custom_memory_topic": {
                    "label": "business_feedback",
                    "description": """
                    用户对咖啡店体验的具体反馈。这包括对饮料、食品、糕点、氛围、员工友好性、服务速度、清洁度以及任何改进建议的意见。
                    """
                }
            }
        ],
        "generate_memories_examples": {
            "conversationSource": {
                "events": [
                    {
                        "content": Content(
                            role="model",
                            parts=[Part(text="欢迎回到The Daily Grind！我们很乐意听取您对访问的反馈。")]
                        )
                    },
                    {
                        "content": Content(
                            role="user",
                            parts=[Part(text="嘿。今天的滴滤咖啡有点温，很令人失望。另外，音乐太大声，我几乎听不到我朋友说话。")]
                        )
                    }
                ]
            },
            "generatedMemories": [
                {"fact": "用户报告滴滤咖啡温了。"},
                {"fact": "用户觉得店里的音乐太大声。"}
            ]
        }
    }]
}

agent_engine = client.agent_engines.create(
    config={
        "context_spec": {"memory_bank_config": memory_bank_config}
    }
)
```

虽然内存提取本身不是"总结"，但算法可能结合总结来提炼信息。为了增强效率，许多内存管理器将对话的滚动摘要直接整合到内存提取提示²⁰中。这个压缩历史提供了从最近交互中提取关键信息的必要上下文。它消除了每个轮次重复处理完整、冗长对话以维持上下文的需要。

一旦信息已从数据源提取，现有内存语料库必须通过整合更新以反映新信息。

### 深度探讨：内存整合

在从冗长对话提取内存后，整合应将新信息整合到连贯、准确和演变的知识库中。可以说这是内存生命周期中最复杂的阶段，将简单的事实集合转换为对用户的策划理解。没有整合，智能体的内存将很快成为每个捕获信息片段的嘈杂、矛盾和不可靠的日志。这种"自我管理"通常由LLM管理，是将内存管理器提升到简单数据库之上的原因。

整合解决了从对话数据产生的基本问题，包括：

- **信息重复**：用户可能在不同的对话中以多种方式提及相同事实（例如，"我需要去NYC的航班"和后来"我计划去纽约旅行"）。简单的提取过程会创建两个冗余内存。

- **冲突信息**：用户状态随时间变化。没有整合，智能体的内存将包含矛盾事实。

- **信息演变**：简单事实可能变得更细致。初始内存"用户对营销感兴趣"可能演变为"用户正在领导专注于Q4客户获取的营销项目。"

- **内存相关性衰减**：并非所有内存都永远有用。智能体必须参与遗忘——主动修剪旧的、陈旧的或低置信度内存，以保持知识库相关和高效。遗忘可以通过在整合期间指导LLM依赖更新信息或通过生存时间（TTL）自动删除来实现。

整合过程是LLM驱动的工作流，将新提取的见解与用户现有内存进行比较。首先，工作流尝试检索与新提取内存相似的现有内存。这些现有内存是整合的候选者。如果现有内存与新信息矛盾，可能会被删除。如果被增强，可能会被更新。

其次，LLM被呈现现有内存和新信息。其核心任务是一起分析它们并识别应执行什么操作。主要操作包括：

- **更新**：用新或更正信息修改现有内存。

- **创建**：如果新见解完全新颖且与现有内存无关，创建新内存。

- **删除/作废**：如果新信息使旧内存完全无关或不正确，删除或作废它。

最后，内存管理器将LLM的决定转换为更新内存存储的事务。

### 内存溯源

经典的机器学习公理"垃圾进，垃圾出"对LLMs甚至更关键，其中结果通常是"垃圾进，自信垃圾出"。为了使智能体能够做出可靠决策和内存管理器有效整合内存，它们必须能够批判性地评估自己内存的质量。这种可信度直接从内存的溯源——其起源和历史的详细记录——派生。

**图7：数据源和内存之间的信息流。单个内存可以从多个数据源派生，单个数据源可能贡献多个内存。**

内存整合过程——将多个源的信息合并到单个、演变内存中——产生了跟踪其谱系的需要。如上图所示，单个内存可能是多个数据源的混合，单个源可能被分割为多个内存。

为了评估可信度，智能体必须为每个源跟踪关键细节，如其起源（源类型）和年龄（"新鲜度"）。这些细节至关重要，原因有二：它们决定了每个源在内存整合期间的权重，它们告知智能体在推理期间应依赖该内存的程度。

**源类型**是确定可信度最重要的因素之一。数据源分为三个主要类别：

- **引导数据**：从内部系统预加载的信息，如CRM。这种高信任数据可用于初始化用户内存以解决冷启动问题，这是为智能体从未交互过的用户提供个性化体验的挑战。

- **用户输入**：这包括明确提供的数据（例如通过表单，这是高信任）或从对话中隐式提取的信息（通常不太可靠）。

- **工具输出**：从外部工具调用返回的数据。从工具输出生成内存通常不鼓励，因为这些内存往往脆弱和陈旧，使这种源类型更适合短期缓存。

#### 内存管理过程中的内存谱系计算

这种动态、多源内存方法在管理内存时产生了两个主要的操作挑战：冲突解决和删除派生数据。

内存整合不可避免地导致冲突，其中一个数据源与另一个冲突。内存的溯源允许内存管理器为其信息源建立信任层次。当来自不同源的内存彼此矛盾时，智能体必须在冲突解决策略中使用此层次结构。常见策略包括优先考虑最可信的源、偏向最新信息或寻找多个数据点之间的确认。

管理内存的另一个挑战发生在删除内存时。内存可能从多个数据源派生。当用户撤销对一个数据源的访问时，从该源派生的数据也应被删除。删除该源"触及"的每个内存可能过于激进。一个更精确、尽管计算昂贵的方法是仅使用剩余的、有效源从头重新生成受影响的内存。

除了静态溯源细节外，对内存的置信度必须演变。通过确认增加置信度，如多个可信源提供一致信息时。然而，高效的内存系统也必须通过内存修剪——识别和"遗忘"不再有用的内存的过程——主动管理其现有知识。这种修剪可能由几个因素触发。

- **基于时间的衰减**：内存的重要性可能随时间降低。两年前会议的内存可能比上周的记忆相关性更低。

- **低置信度**：从弱推理创建且从未被其他源确认的内存可能被修剪。

- **无关性**：随着智能体获得对用户更复杂的理解，它可能确定一些旧的、琐碎的内存不再与用户当前目标相关。

通过将反应式整合管道与主动修剪相结合，内存管理器确保智能体的知识库不是所说的所有东西的增长日志。相反，它是用户的策划、准确和相关的理解。

#### 推理过程中的内存谱系计算

除了在管理语料库内容时考虑内存的谱系外，在推理时间也应考虑内存的可信度。智能体对内存的置信度不应是静态的；它必须基于新信息和时间流逝而演变。通过确认增加置信度，如多个可信源提供一致信息时。相反，置信度随着旧内存变得陈旧而降低（或衰减），当引入矛盾信息时也会下降。最终，系统可以通过归档或删除低置信度内存来"遗忘"。这种动态置信度分数在推理时间至关重要。内存和（如果可用）其置信度分数被注入提示中，使LLM能够评估信息可靠性并做出更细致的决策，而不是显示给用户。

这整个信任框架服务于智能体的内部推理过程。内存及其置信度分数通常不直接显示给用户。相反，它们被注入系统提示中，允许LLM权衡证据，考虑其信息的可靠性，并最终做出更细致和可信的决策。

### 触发内存生成

虽然内存管理器在生成被触发后自动化内存提取和整合，但智能体仍必须决定何时应该尝试内存生成。这是一个关键的架构选择，平衡数据新鲜度与计算成本和延迟。此决策通常由智能体的逻辑管理，可以采用几种触发策略。内存生成可以基于各种事件启动：

- **会话完成**：在多轮会话结束时触发生成。

- **轮次节奏**：在特定数量的轮次后运行过程（例如每5个轮次）。

- **实时**：在每个单个轮次后生成内存。

- **显式命令**：在直接用户命令时激活过程（例如"记住这个"）。

触发的选择涉及成本和保真度之间的直接权衡。频繁生成（例如实时）确保内存高度详细和新鲜，捕获对话的每个细微差别。然而，这产生最高的LLM和数据库成本，如果处理不当可能引入延迟。不频繁生成（例如在会话完成时）成本效益高得多，但有创建较低保真度内存的风险，因为LLM必须一次性总结更大的对话块。你还想小心内存管理器不多次处理相同事件，因为这引入不必要的成本。

### 内存即工具

更复杂的方法是允许智能体自己决定何时创建内存。在此模式中，内存生成作为工具暴露（即`create_memory`）；工具定义应定义什么类型的信息应被视为有意义。然后智能体可以分析对话并在识别有意义要持久化的信息时自主决定调用此工具。这将识别"有意义信息"的责任从外部内存管理器转移到智能体（因此作为开发者的你）本身。

例如，你可以使用ADK通过将你的内存生成代码打包到智能体在认为对话有意义要持久化时决定调用的工具²¹中来做到这一点。你可以将会话发送到Memory Bank，Memory Bank将从对话历史提取和整合内存：

**代码片段8：使用自定义工具触发内存生成的ADK智能体。Memory Bank将提取和整合内存。**

```python
from google.adk.agents import LlmAgent
from google.adk.memory import VertexAiMemoryBankService
from google.adk.runners import Runner
from google.adk.tools import ToolContext

def generate_memories(tool_context: ToolContext):
    """触发内存生成以记住会话。"""
    # 选项1：使用ADK内存服务从完整对话历史提取内存。
    tool_context._invocation_context.memory_service.add_session_to_memory(session)

    # 选项2：从最后对话轮次提取内存。
    client.agent_engines.memories.generate(
        name="projects/.../locations/...reasoningEngines/...",
        direct_contents_source={
            "events": [
                {"content": tool_context._invocation_context.user_content}
            ]
        },
        scope={
            "user_id": tool_context._invocation_context.user_id,
            "app_name": tool_context._invocation_context.app_name
        },
        # 在后台生成内存
        config={"wait_for_completion": False}
    )
    return {"status": "success"}

agent = LlmAgent(
    ...,
    tools=[generate_memories]
)

runner = Runner(
    agent=agent,
    app_name=APP_NAME,
    session_service=session_service,
    memory_service=VertexAiMemoryBankService(
        agent_engine_id=AGENT_ENGINE_ID,
        project=PROJECT,
        location=LOCATION
    )
)
```

另一种方法是利用内部内存，其中智能体主动决定从对话中记住什么。在此工作流中，智能体负责提取关键信息。可选地，这些提取的内存随后被发送到Agent Engine Memory Bank以与用户现有内存整合²²：

**代码片段9：使用自定义工具从对话中提取内存并使用Agent Engine Memory Bank触发整合的ADK智能体。与代码片段8不同，智能体负责提取内存，而不是Memory Bank。**

```python
def extract_memories(query: str, tool_context: ToolContext):
    """触发内存生成以记住信息。
    参数:
      query: 关于用户应持久化的有意义信息。
    """
    client.agent_engines.memories.generate(
        name="projects/.../locations/...reasoningEngines/...",
        # 有意义的信息已从对话中提取，所以我们只想将其与同一用户的现有内存整合。
        direct_memories_source={
            "direct_memories": [{"fact": query}]
        },
        scope={
            "user_id": tool_context._invocation_context.user_id,
            "app_name": tool_context._invocation_context.app_name
        },
        config={"wait_for_completion": False}
    )
    return {"status": "success"}

agent = LlmAgent(
    ...,
    tools=[extract_memories]
)
```

### 后台 vs 阻塞操作

内存生成是昂贵的操作，需要LLM调用和数据库写入。对于生产中的智能体，内存生成几乎应始终作为后台进程²³异步处理。

在智能体向用户发送响应后，内存生成管道可以并行运行而不阻塞用户体验。这种解耦对于保持智能体感觉快速和响应至关重要。阻塞（或同步）方法，用户必须等待内存写入才能接收响应，会创造不可接受的缓慢和令人沮丧的用户体验。这要求内存生成发生在与智能体核心运行时架构上分离的服务中。

### 内存检索

有了内存生成机制，你的重点可以转移到关键任务：检索。智能检索策略对智能体性能至关重要，涵盖关于检索哪些内存以及何时检索它们的决策。

检索内存的策略在很大程度上依赖于内存的组织方式。对于结构化用户配置文件，检索通常是完整配置文件或特定属性的简单查找。然而，对于内存集合，检索是更复杂的搜索问题。目标是从大量非结构化或半结构化数据池中发现最相关、概念相关的信息。本节讨论的策略旨在解决内存集合的复杂检索挑战。

内存检索搜索当前对话中最相关的内存。有效的检索策略至关重要；提供无关内存会混淆模型并降低其响应，而找到完美的上下文片段可能导致卓越的智能交互。核心挑战是在严格的延迟预算内平衡内存'有用性'。

高级内存系统超越简单搜索，并从多个维度对潜在内存评分以找到最佳匹配。

- **相关性（语义相似性）**：此内存与当前对话在概念上有多相关？

- **最近性（基于时间）**：此内存是多久前创建的？

- **重要性（显著性）**：此内存总体上有多关键？与相关性不同，内存的"重要性"可能在生成时定义。

仅依赖基于向量的相关性是一个常见陷阱。相似性分数可能揭示概念相似但陈旧或琐碎的内存。最有效的策略是结合所有三个维度分数的混合方法。

对于准确性至关重要的应用程序，检索可以使用查询重写、重新排序或专门检索器等方法来完善。然而，这些技术计算成本高昂并增加显著延迟，使其不适合大多数实时应用程序。对于需要这些复杂算法且内存不会很快变得陈旧的场景，缓存层可以是有效的缓解措施。缓存允许检索查询的昂贵结果被临时存储，为后续相同请求绕过高延迟成本。

使用查询重写，LLM可用于改进搜索查询本身。这可能涉及将用户模糊输入重写为更精确的查询，或将单个查询扩展为多个相关查询以捕获主题的不同方面。虽然这显著提高了初始搜索结果的质量，但它在过程开始时增加了额外LLM调用的延迟。

使用重新排序，初始检索使用相似性搜索获取一组广泛的候选内存（例如前50个结果）。然后，LLM可以重新评估和重新排序这个较小集合以产生更准确的最终列表²⁴。

最后，你可以使用微调训练专门的检索器。然而，这需要访问标记数据并可能显著增加成本。

最终，检索的最佳方法从更好的内存生成开始。确保内存语料库高质量且没有无关信息是保证任何检索内存集合都有帮助的最有效方法。

### 检索时机

检索的最终架构决策是何时检索内存。一种方法是主动检索，在每个轮次开始时自动加载内存。这确保上下文总是可用，但为不需要内存访问的轮次引入不必要的延迟。由于内存在单个轮次期间保持静态，它们可以有效地缓存以减轻此性能成本。

例如，你可以使用内置的PreloadMemoryTool或自定义回调²⁵在ADK中实现主动检索：

**代码片段10：使用ADK在每轮开始时检索内存**

```python
# 选项1：使用内置的PreloadMemoryTool，它每轮都使用相似性搜索检索内存。
agent = LlmAgent(
    ...,
    tools=[adk.tools.preload_memory_tool.PreloadMemoryTool()]
)

# 选项2：使用自定义回调以更好地控制如何检索内存。
def retrieve_memories_callback(callback_context, llm_request):
    user_id = callback_context._invocation_context.user_id
    app_name = callback_context._invocation_context.app_name
    response = client.agent_engines.memories.retrieve(
        name="projects/.../locations/...reasoningEngines/...",
        scope={
            "user_id": user_id,
            "app_name": app_name
        }
    )
    memories = [f"* {memory.memory.fact}" for memory in list(response)]
    if not memories:
        # 没有要添加到系统指令的内存。
        return
    # 将格式化的内存附加到系统指令
    llm_request.config.system_instruction += "\n以下是您关于用户的信息：\n"
    llm_request.config.system_instruction += "\n".join(memories)

agent = LlmAgent(
    ...,
    before_model_callback=retrieve_memories_callback,
)
```

或者，你可以使用反应式检索（"内存即工具"），其中智能体被赋予查询其内存的工具，自己决定何时检索上下文。这更高效和健壮但需要额外的LLM调用，增加延迟和成本；然而，只在必要时检索内存，因此延迟成本发生频率较低。此外，智能体可能不知道是否存在相关信息要检索。然而，这可以通过使智能体意识到可用内存类型（例如，如果你使用自定义工具，则在工具描述中）来缓解，允许对何时查询做出更明智的决定。

**代码片段11：配置你的ADK智能体使用内置或自定义工具决定何时检索内存**

```python
# 选项1：使用内置的LoadMemory。
agent = LlmAgent(
    ...,
    tools=[adk.tools.load_memory_tool.LoadMemoryTool()],
)

# 选项2：使用自定义工具，你可以描述什么类型的信息可能可用。
def load_memory(query: str, tool_context: ToolContext):
    """为用户检索内存。
    以下类型的信息可能为用户存储：
    * 用户偏好，如用户最喜欢的食物。
    ...
    """
    # 使用相似性搜索检索内存。
    response = tool_context.search_memory(query)
    return response.memories

agent = LlmAgent(
    ...,
    tools=[load_memory],
)
```

### 带内存的推理

一旦检索到相关内存，最后一步是将它们战略性地放置到模型的上下文窗口中。这是一个关键过程；内存的位置可以显著影响LLM的推理，影响操作成本，并最终确定最终答案的质量。

内存主要通过将它们附加到系统指令或注入对话历史中来呈现。在实践中，混合策略通常最有效。对应始终存在的稳定、全局内存（如用户配置文件）使用系统提示。否则，对仅与对话即时上下文相关的瞬态、情景内存使用对话注入或内存即工具。这平衡了持久上下文需求与即时信息检索的灵活性。

#### 系统指令中的内存

使用内存进行推理的简单选项是将内存附加到系统指令。此方法通过将检索到的内存直接附加到系统提示与前缀旁边，保持对话历史清洁，将它们构建为整个交互的基础上下文。例如，你可以使用Jinja动态添加内存到你的系统指令：

**代码片段12：使用检索到的内存构建你的系统指令**

```python
from jinja2 import Template

template = Template("""
{{ system_instructions }}
<MEMORIES>
以下是关于用户的一些信息：
{% for retrieved_memory in data %}* {{ retrieved_memory.memory.fact }}
{% endfor %}
</MEMORIES>
""")

prompt = template.render(
    system_instructions=system_instructions,
    data=retrieved_memories
)
```

在系统指令中包含内存给予内存高权威性，干净地将上下文与对话分离，并适合稳定、"全局"信息如用户配置文件。然而，存在过度影响的风险，智能体可能试图将每个主题都与其核心指令中的内存联系起来，即使不合适。

这种架构模式引入了几个约束。首先，它要求智能体框架支持在每个LLM调用之前的动态系统提示构建；此功能并非总是随时支持。此外，该模式与"内存即工具"不兼容，因为系统提示必须在LLM决定调用内存检索工具之前最终确定。最后，它处理非文本内存效果差。大多数LLMs只为系统指令接受文本，使嵌入如图像或音频等多模态内容到提示中具有挑战性。

#### 对话历史中的内存

在此方法中，检索到的内存直接注入轮次对话中。内存可以放在完整对话历史之前或就在最新用户查询之前。

然而，这种方法可能嘈杂，增加token成本，如果检索到的内存不相关，可能混淆模型。其主要风险是对话注入，模型可能错误地将内存视为在对话中实际说过的内容。你还必须更小心注入对话中的内存的视角；例如，如果你使用"用户"角色和用户级内存，内存应以第一人称视角编写。

将内存注入对话历史的特殊情况是通过工具调用检索内存。内存将直接包含在对话中作为工具输出的一部分。

**代码片段13：将内存作为工具检索，直接将内存插入对话**

```python
def load_memory(query: str, tool_context: ToolContext):
    """将内存加载到对话历史中..."""
    response = tool_context.search_memory(query)
    return response.memories

agent = LlmAgent(
    ...,
    tools=[load_memory],
)
```

#### 程序性内存

本白皮书主要专注于陈述性内存，这种集中反映了当前商业内存格局。大多数内存管理平台也为这种陈述性方法架构，擅长提取、存储和检索"什么"——事实、历史和用户数据。

然而，这些系统并非设计为管理程序性内存，这是改善智能体工作流和推理的机制。存储"如何做"不是信息检索问题；它是推理增强问题。管理这种"知道如何"需要完全分离和专门的算法生命周期，尽管具有类似的高级结构²⁶：

1. **提取**：程序性提取需要专门的提示，设计用于从成功交互提炼可重用策略或"战术手册"，而不仅仅是捕获事实或有意义信息。

2. **整合**：虽然陈述性整合合并相关事实（"什么"），但程序性整合管理工作流本身（"如何"）。这是一个积极的逻辑管理过程，专注于将新的成功方法与现有"最佳实践"集成，修补已知计划中有缺陷的步骤，和修剪过时或无效的程序。

3. **检索**：目标不是检索数据来回答问题，而是检索指导智能体如何执行复杂任务的计划。因此，程序性内存可能与陈述性内存具有不同的数据模式。

这种智能体"自我进化"其逻辑的能力自然引出了与常见适应方法的比较：微调——通常通过人类反馈强化学习（RLHF）²⁷。虽然两个过程都旨在改善智能体行为，但它们的机制

## 测试与评估

现在你有了启用内存的智能体，你应该通过全面的质量和评估测试来验证启用内存的智能体的行为。评估智能体的内存是一个多层过程。评估需要验证智能体记住正确的事物（质量），它能在需要时找到那些内存（检索），以及使用那些内存实际上帮助它实现其目标（任务成功）。虽然学术界专注于可重现的基准测试，但行业评估以内存如何直接影响生产智能体的性能和可用性为中心。

### 内存生成质量指标

评估生成的内存本身的内容，回答问题："智能体是否记住正确的事物？"这通常通过将智能体生成的内存与手动创建的"黄金集"理想内存进行比较来衡量。

- **精确度**：在智能体创建的所有内存中，多少百分比是准确和相关的？高精确度防止"过于急切"的内存系统用无关噪声污染知识库。

- **召回率**：在它应该记住源中所有相关事实中，它捕获了什么百分比？高召回率确保智能体不遗漏关键信息。

- **F1分数**：精确度和召回率的调和平均值，提供单一、平衡的质量度量。

### 内存检索性能指标

评估智能体在正确时间找到正确内存的能力。

- **Recall@K**：当需要内存时，是否在前'K'个检索结果中找到正确的？这是检索系统准确性的主要衡量标准。

- **延迟**：检索在智能体响应的"热路径"上。整个检索过程必须在严格的延迟预算内（例如200ms以下）执行以避免降低用户体验。

### 端到端任务成功指标

这是最终测试，回答问题："内存是否确实帮助智能体更好地执行其工作？"这通过评估智能体使用其内存在下游任务上的性能来衡量，通常使用LLM"裁判"将智能体的最终输出与黄金答案比较。裁判确定智能体的答案是否准确，有效测量内存系统对最终结果的贡献程度。

评估不是一次性事件；它是持续改进的引擎。上述指标提供识别弱点并随时间系统性地增强内存系统所需的数据。这个迭代过程涉及建立基线、分析失败、调整系统（例如优化提示、调整检索算法）和重新评估以测量变化的影响。

虽然上述指标专注于质量，生产就绪性还取决于性能。对于每个评估领域，测量底层算法的延迟及其在负载下的扩展能力至关重要。检索内存"在热路径上"可能有严格的、亚秒延迟预算。生成和整合，虽然经常异步，但必须有足够的吞吐量以跟上用户需求。最终，成功的内存系统必须对现实世界使用智能、高效和健壮。

## 内存的生产环境考虑因素

除了性能，将启用内存的智能体从原型转换到生产需要关注企业级架构关注点。这种转变引入了可扩展性、弹性和安全性的关键要求。生产级系统必须不仅为智能性设计，还要为企业级健壮性设计。

为确保用户体验永远不会被内存生成的计算昂贵过程阻塞，健壮架构必须将内存处理与主应用程序逻辑解耦。虽然这是事件驱动模式，但通常通过对专用内存服务的直接、非阻塞API调用而不是自管理的消息队列来实现。流程看起来像这样：

1. **智能体推送数据**：在相关事件后（例如会话结束），智能体应用程序向内存管理器进行非阻塞API调用，"推送"原始源数据（如对话记录）以供处理。

2. **内存管理器在后台处理**：内存管理器服务立即确认请求并将生成任务放入其自己内部、管理的队列中。然后它独自负责异步繁重工作：进行必要的LLM调用来提取、整合和格式化内存。管理器可能会延迟处理事件，直到一定时间的不活动过去。

3. **内存被持久化**：服务将最终内存——可能是新条目或对现有条目的更新——写入专用、持久数据库。对于托管内存管理器，存储是内置的。

4. **智能体检索内存**：主智能体应用程序然后可以在需要为新用户交互检索上下文时直接查询此内存存储。

这种基于服务、非阻塞的方法确保内存管道中的故障或延迟不直接影响面向用户的应用程序，使系统更具弹性。它还通知了在线（实时）生成（对对话新鲜度理想）和离线（批处理）处理（对从历史数据填充系统有用）之间的选择。

随着应用程序增长，内存系统的可扩展性成为一个关键考虑因素。这种基于服务的架构允许根据需求扩展内存管理服务，而不影响主要智能体应用程序。你可以独立扩展处理和存储组件，优化成本和性能。

然而，这种解耦引入了最终一致性的挑战。主应用程序可能继续使用稍旧的内存版本运行几个轮次，直到后台处理完成并持久化新内存。在大多数个性化用例中，这种轻微的延迟是可以接受的，因为用户的偏好不会在单个对话中发生剧烈变化。

#### 隐私和安全风险

内存从用户数据派生并包含用户数据，因此需要严格的隐私和安全控制。一个有用的类比是将系统内存想象为由专业档案管理员管理的安全企业档案，其工作是在保护公司的同时保存有价值的知识。

此档案的基本规则是数据隔离。正如档案管理员永远不会混合来自不同部门的机密文件一样，内存必须在用户或租户级别严格隔离。为一个用户服务的智能体永远不能访问另一个用户的内存，使用限制性访问控制列表（ACL）强制执行。此外，用户必须对其数据具有程序化控制，有明确的选项选择退出内存生成或请求从档案中删除其所有文件。

在归档任何文档之前，档案管理员执行关键的安全步骤。首先，他们仔细检查每一页以编辑敏感个人信息（PII），确保保存知识而不产生责任。其次，档案管理员接受训练以发现和丢弃伪造或故意误导的文档——这是防止内存中毒²⁸的保障措施。同样，系统必须在将信息提交到长期内存之前验证和清理信息，以防止恶意用户通过提示注入破坏智能体的持久知识。系统必须包含像Model Armor这样的保障措施，在将信息提交到长期内存之前验证和清理信息²⁹。

此外，如果多个用户共享同一组内存，如程序性内存（教导智能体如何做某事），则存在渗漏风险。例如，如果一个用户的程序性内存用作另一个用户的示例——如公司范围内共享备忘录——档案管理员必须首先执行严格的匿名化，以防止敏感信息跨用户边界泄漏。

---

## 结论

本白皮书探讨了上下文工程学科，专注于其两个核心组件：会话和内存。从简单对话轮次到持久、可操作智能信息的旅程由这种实践指导，涉及将所有必要信息——包括对话历史、内存和外部知识——动态组装到LLM的上下文窗口中。整个过程依赖于两个不同但相互关联的系统之间的相互作用：即时会话和长期内存。

会话管理"现在"，充当单个对话的低延迟、时间容器。其主要挑战是性能和安全，需要低延迟访问和严格隔离。为防止上下文窗口溢出和延迟，你必须使用如基于token的截断或递归总结等提取技术来压缩会话历史或单个请求有效载荷内的内容。此外，安全至关重要，要求在会话数据持久化之前进行PII编辑。

内存是长期个性化的引擎和跨多个会话持久化的核心机制。它超越RAG（使智能体成为事实专家）使智能体成为用户专家。内存是一个活动的、LLM驱动的ETL管道——负责提取、整合和检索——从对话历史中提炼最重要信息。通过提取，系统将最关键信息提炼为关键内存点。随后，整合管理并整合此新信息与现有语料库，解决冲突和删除冗余数据以确保连贯的知识库。为保持快速的用户体验，内存生成必须在智能体响应后作为异步后台进程运行。通过跟踪溯源和采用针对内存中毒等风险的保障措施，开发者可以构建真正与用户一起学习和成长的受信任、自适应助手。

---

## 结束语

1. https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=en
2. https://arxiv.org/abs/2301.00234
3. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/sessions/overview
4. https://langchain-ai.github.io/langgraph/concepts/multi_agent/#message-passing-between-agents
5. https://google.github.io/adk-docs/agents/multi-agents/
6. https://google.github.io/adk-docs/agents/multi-agents/#c-explicit-invocation-agenttool
7. https://agent2agent.info/docs/concepts/message/
8. https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/
9. https://cloud.google.com/security-command-center/docs/model-armor-overview
10. https://ai.google.dev/gemini-api/docs/long-context#long-context-limitations
11. https://huggingface.co/blog/Kseniase/memory
12. https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory
13. https://langchain-ai.github.io/langgraph/concepts/memory/#semantic-memory
14. https://arxiv.org/pdf/2412.15266
15. https://arxiv.org/pdf/2412.15266
16. https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#sample-requests-text-gen-multimodal-prompt
17. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories
18. https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output
19. https://cloud.google.com/agent-builder/agent-engine/memory-bank/set-up#memory-bank-config
20. https://arxiv.org/html/2504.19413v1
21. https://google.github.io/adk-docs/tools/#how-agents-use-tools
22. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories#consolidate-pre-extracted-memories
23. https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/memory-bank/generate-memories#background-memory-generation
24. https://arxiv.org/pdf/2503.08026
25. https://google.github.io/adk-docs/callbacks/
26. https://arxiv.org/html/2508.06433v2
27. https://cloud.google.com/blog/products/ai-machine-learning/rlhf-on-google-cloud
28. https://arxiv.org/pdf/2503.03704
29. https://cloud.google.com/security-command-center/docs/model-armor-overview
30. https://cloud.google.com/architecture/choose-design-pattern-agentic-ai-system