以下是对您提供的论文 **《Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation》**（arXiv:2510.24120v1）的**全文中文翻译**，严格**保持原有段落结构、标题层级、公式编号、表格格式、引用方式及整体排版风格**，力求在准确传达原文技术内容的同时，保留其学术论文的规范布局。

---

# 基于图引导的概念选择用于高效检索增强生成

刘子瑜†，刘依静†，袁建飞，闫敏志，岳乐，熊鸿怀，杨艺∗  
华为云计算技术有限公司，中国  
{liuziyu16, liuyijing5, yuanjianfei, yanminzhi}@huawei.com  
{yuele, xionghonghuai, yangyi104}@huawei.com  

## 摘要

基于图的检索增强生成（Graph-based RAG）从文本块中构建知识图谱（KG），以增强基于大语言模型（LLM）的问答系统中的检索能力。该方法在生物医学、法律和政治学等领域尤为有效，因为在这些领域中，高效的检索通常涉及对专有文档进行多跳推理。然而，这些方法需要大量调用 LLM 来从文本块中提取实体和关系，导致在大规模场景下成本过高。通过精心设计的消融实验，我们观察到某些词（我们称之为“概念”）及其关联文档更为重要。基于这一洞察，我们提出了**图引导概念选择**（G2ConS）。其核心包括一种文本块选择方法和一个不依赖 LLM 的概念图。前者选择显著的文档块以降低 KG 构建成本；后者则以零成本弥补因块选择引入的知识缺口。在多个真实数据集上的评估表明，G2ConS 在构建成本、检索效果和回答质量方面均优于所有基线方法。

## 1 引言

检索增强生成（RAG）使大语言模型能够访问最新或领域特定的信息，显著提升其问答（QA）性能，而无需额外训练（Gao et al., 2023a,b；Fan et al., 2024）。一种代表性方法是 Text-RAG（Lewis et al., 2020），它依赖于文档分块与稠密检索。然而，此类方法忽略了知识间的依赖关系，导致在多跳问题上准确率显著下降（Peng et al., 2024）。为解决这一局限，近期的基于图的 RAG（GraphRAG）技术预先构建图结构以捕捉这些依赖关系，从而在复杂 QA 场景中显著提升 RAG 的准确率（Procko & Ochoa, 2024；Jimenez Gutierrez et al., 2024；Edge et al., 2024）。在法律、医学和科学等专业领域，GraphRAG 已被证明能显著增强大模型的问答能力（Li et al., 2024a；Delile et al., 2024；Liang et al., 2024）。

GraphRAG 的核心步骤是在文档与知识之间建立连接；然而，高昂的构建成本阻碍了这些方法在现实应用中的部署（Abane et al., 2024；Wang et al., 2025）。例如，Microsoft-GraphRAG（MS-GraphRAG）（Edge et al., 2024）处理一个 5 GB 的法律案例（Arnold & Romero, 2022）预计成本高达 3.3 万美元（Huang et al., 2025b）。对于企业级知识检索系统而言，无论是初始构建还是后续更新，如此高昂的成本都是不可接受的。为降低构建成本，近期研究通过约束图结构来减少 LLM 调用次数：LightRAG（Guo et al., 2024）采用键值模式，HiRAG（Huang et al., 2025a）和 ArchRAG（Wang et al., 2025）采用树状结构，其核心思想是通过一次 LLM 调用来汇总多个文本块。其他工作则转向更粗粒度的图构建以减少对 LLM 的依赖；Raptor（Sarthi et al., 2024）利用向量模型构建层级聚类，而 KET-RAG（Huang et al., 2025b）引入实体-文档二分图。

尽管这些方法降低了构建开销，但仍存在两大挑战。（1）当前方法通过重构整个流程来解决成本问题，限制了其改进的通用性。此外，在许多应用场景中，重新设计现有 GraphRAG 系统并从头构建成本过高且不切实际。（2）对图结构的限制不可避免地牺牲了准确性：LightRAG 的键值设计限制了检索深度，而 HiRAG 和 ArchRAG 的自顶向下搜索策略难以找到直接相关的实体。

> **图1：概念删除实验的设计与结果**。(a) 我们根据关联词（称为“概念”）将文本块划分为不同组。(b) 通过按不同顺序删除概念，我们发现某些概念更为重要。

为解决上述两个挑战，我们提出了**图引导概念选择**（G2ConS），一种与主流 GraphRAG 方法兼容的高效 RAG 方案。我们的方法始于一项名为“概念删除”的消融研究，用以探究不同知识对 GraphRAG 的重要性。如图 1(a) 所示，我们首先按概念对知识进行划分，其中“概念”被定义为一个词及其包含该词的所有文本块。划分后，原始知识文档可视为多个概念的组合。通过概念删除（即按组删除文本块），我们能够识别不同知识组件（以概念为索引）的贡献。基于一个合理假设，我们认为与其他概念连接更多的概念更为重要。为说明这一点，我们在 MuSiQue 数据集（Trivedi et al., 2022）上构建了一个概念关系图：使用传统关键词提取方法（Ramos et al., 2003）从文本块中提取概念，并将同一块中共同出现的概念相连。最后，我们按度中心性对概念排序，排名越高的概念与其他概念的连接越强。

我们在 MS-GraphRAG 上进行概念删除实验，结果如图 1(b) 所示，其中横轴表示删除的 token 数量，纵轴表示对应的准确率（EM 分数）。我们评估了两种删除策略：**正向删除**（从高到低删除概念）和**反向删除**（从低到高删除概念）。作为对比，图中还展示了使用全部文本块的 MS-GraphRAG 性能（即“基线”）。结果表明，即使删除相同数量的 token，删除高排名概念仍会导致更严重的性能下降。

基于这一观察，G2ConS 提出两项有效策略：  
1. 针对第一个挑战，我们引入**核心块选择**（Core Chunk Selection），通过移除低排名概念来减少输入块数量，从而在不修改图构建过程的前提下降低构建成本。  
2. 针对第二个挑战以及块选择引入的知识缺口，我们提出**概念图检索**（Concept Graph Retrieval）。由于概念图的构建不依赖 LLM 且无结构约束，因此能够以低成本高效检索被移除的低排名概念。

本工作的主要贡献如下：
- 我们提出了 G2ConS，一种高效的 GraphRAG 方案，可同时从 KG 和不依赖 LLM 的概念图中进行检索。G2ConS 在构建成本与 QA 性能之间实现了有效平衡，在多个基准测试中均优于当前最先进方法，尤其在 MuSiQue 上平均提升达 31.44%。
- 我们引入了核心块选择方法，这是一种通用策略，可显著降低 GraphRAG 构建成本且精度损失极小。结合概念图后，现有 GraphRAG 方法可在成本降低 80% 的同时进一步提升 QA 准确率。

## 2 相关工作

### 基于图的 RAG

将文档表示为图结构用于检索与问答可追溯至 Min et al. (2019)，他们提出基于共现关系从文本块构建图，并展示了明显的性能增益，从而确立了图检索在 QA 中的有效性。然而，该方法忽略了跨文档的语义链接，导致知识分散在不连通的区域。随着 LLM 的兴起，许多工作将其引入图构建。KG 提取方法如 GraphRAG（Edge et al., 2024）、HippoRAG（Jimenez Gutierrez et al., 2024）、LightRAG（Guo et al., 2024）、KAG（Liang et al., 2024）、FastRAG（Abane et al., 2024）和 GraphReader（Li et al., 2024b）提升了图质量与 QA 准确率，但大规模下重复调用 LLM 进行实体-关系提取的成本仍然过高。分层方法如 RAPTOR（Sarthi et al., 2024）、HiRAG（Huang et al., 2025a）、ArchRAG（Wang et al., 2025）和 MemWalker（Chen et al., 2023）递归地将文档汇总为分层索引；其批处理降低了构建成本，但自顶向下的检索难以找到直接相关的知识，降低了检索准确率。文本级方法如 AutoKG（Chen & Bertozzi, 2023）、PathRAG（Chen et al., 2025）和 KGP（Wang et al., 2024）对输入文本进行分段，并利用 LLM 建立文本间或文本到实体的链接。通过避免细粒度提取，这些方法将构建成本控制在与 Text-RAG（Lewis et al., 2020）相当的水平；然而它们在多跳问题上表现下降。相比之下，G2ConS 通过结合 KG 与概念图的混合检索策略，在构建成本与 QA 质量上均取得最优性能，且兼容主流 GraphRAG 方法。

### 高效的基于 KG 的 RAG

KG 的构建已被证明能显著提升 RAG 在多跳推理任务上的性能（Peng et al., 2024）。然而，KG 构建的高成本是实际部署的主要障碍。为解决此问题，多种方法尝试简化 KG 构建流程。例如，LightRAG（Guo et al., 2024）将复杂的节点网络简化为键值表，而 HiRAG（Huang et al., 2025a）和 ArchRAG（Wang et al., 2025）构建树状 KG 以减少 LLM 调用次数。然而，这些方法常因对 KG 的结构约束而在多跳任务上准确率下降。另一类工作探索使用粗粒度图，通过轻量级命名实体识别（NER）技术降低 KG 构建成本。例如，Ket-RAG（Huang et al., 2025b）结合二分图与知识图谱以降低整体构建开销。E2GraphRAG（Zhao et al., 2025）引入基于 LLM 生成摘要树的粗粒度实体-文档块图，从而消除对高质量 KG 的依赖。与这些方法不同，G2ConS 强调图构建中的概念选择，且兼容主流 GraphRAG 方法，在成本效率与性能上均取得一致提升。

## 3 G2ConS 框架

本节首先给出 GraphRAG 的问题形式化定义，随后介绍概念图的构建过程，以及 G2ConS 两项核心策略的实现细节：核心块选择与双路径检索。

### 3.1 问题定义

为更好说明 GraphRAG 所解决的问题，我们从 Text-RAG（Lewis et al., 2020）出发。给定一个文档语料库，RAG 按照某种策略将文档分割为文本块。令 \( T \) 为预处理后的文本块集合。Text-RAG 使用嵌入函数 \( \phi(\cdot) \) 对每个块进行向量化，从而构建块-向量对索引：\( \{(t_i, \phi(t_i)) \mid t_i \in T\} \)。在检索阶段，给定查询 \( q \)，Text-RAG 计算查询嵌入 \( \phi(q) \)，并基于向量相似性在索引中搜索最相关的块。在回答生成阶段，这些检索到的块作为上下文输入 LLM，LLM 生成对查询 \( q \) 的最终答案。

GraphRAG 的关键区别在于其基于 \( T \) 构建图 \( G = (V, E) \)，其中 \( V \) 为节点集合（每个节点可表示一个文本块或一个实体），\( E \) 为节点间的边集合，边属性可能包括权重或描述节点关系的文本。在检索阶段，GraphRAG 主要采用局部搜索范式：首先通过嵌入函数 \( \phi(\cdot) \) 计算节点与查询的相似性，识别与查询 \( q \) 最相关的节点，然后在图 \( G \) 上执行广度优先搜索（BFS），通过遍历并收集相邻节点来扩展上下文。在回答生成阶段，GraphRAG 将 BFS 识别的子图序列化并作为上下文输入 LLM 以生成最终响应。GraphRAG 的目标是找到图 \( G \) 以最大化 LLM 回答准确率（Jimenez Gutierrez et al., 2024；Edge et al., 2024）；在我们的工作中，我们额外对 \( G \) 的设计施加了成本约束。

### 3.2 将概念关系表示为图

在第 1 节中，我们介绍了概念图对块选择的重要性及其构建思路。在实现中，我们进一步丰富了图的结构，使其可同时用于块选择与检索。

**概念提取与向量化**。如第 1 节所述，我们使用传统关键词提取方法（Ramos et al., 2003）从块中提取概念 \( W \)，即 \( W = \{\text{Extract}(t_i) \mid t_i \in T\} \)。对于每个 \( w_i \in W \)，我们根据其来源在 \( w_i \) 与其源块 \( t_i \) 之间建立边。为支持高效检索和模糊实体匹配，许多 GraphRAG 方法引入向量化技术（Sarmah et al., 2024；Guo et al., 2024），我们遵循相同的设计原则。然而，根据概念定义（即一个词及其包含该词的所有块），单个词或块的嵌入无法准确表示该概念。另一方面，由于一个块通常包含大量 token（常超 500），其语义表示可能被多个概念污染。

为解决此问题，我们提出在**句子级别**构建概念语义嵌入。具体而言，对于概念 \( w_i \)，我们首先识别与其关联的所有块，然后使用句子分割器（Loper & Bird, 2002）将这些块分割为句子，得到包含 \( w_i \) 的所有句子集合 \( S_{w_i} \)。我们将 \( w_i \) 的向量表示定义为：

\[
\text{Vector}(w_i) = \frac{1}{|S_{w_i}|} \sum_{s_i \in S_{w_i}} \phi(s_i), \quad (1)
\]

其中 \( \phi(\cdot) \) 为第 3.1 节中定义的嵌入函数。我们强调，句子级的概念嵌入能更好捕捉概念本身的内在语义，并在检索时实现与查询相关概念的更准确匹配。

**基于相关性连接概念**。根据我们的概念删除实验，共现能有效表征概念的重要性。然而，在检索时（尤其在 BFS 中寻找给定概念的相关概念）仅依赖共现会引入显著噪声，即“虚假相关”现象（Calude & Longo, 2017）。

为缓解此问题，我们在实现中通过**语义相关性**与**共现**共同构建概念图中的边。具体而言，给定概念 \( w_i \) 和 \( w_j \)，当且仅当 \( \text{Sim}(\text{Vector}(w_i), \text{Vector}(w_j)) \geq \theta_{\text{sem}} \) 且 \( \text{Co}(w_i, w_j) \geq \theta_{\text{co}} \) 时，存在边 \( e(w_i, w_j) \)，其中 \( \text{Sim}(\cdot, \cdot) \) 表示余弦相似度，\( \text{Co}(\cdot, \cdot) \) 统计两概念在不同块中的共现频次，\( \text{Vector}(\cdot) \) 由式 (1) 计算，\( \theta_{\text{sem}} \)、\( \theta_{\text{co}} \) 为常数。当边 \( e(w_i, w_j) \) 存在时，我们使用 Dice 系数（Li et al., 2019）定义边权重 \( r_{w_i,w_j} \) 以量化两概念间的特定关联强度：

\[
r_{w_i,w_j} = \frac{2 \cdot \text{Co}(w_i, w_j)}{|T_{w_i}| + |T_{w_j}|},
\]

其中 \( T_{w_i} \) 和 \( T_{w_j} \) 分别表示包含 \( w_i \) 和 \( w_j \) 的块集合。综上，我们构建的概念图是一个经语义过滤的共现图。

**核心块选择与 KG 构建**。构建概念图后，为更好评估概念的全局重要性，我们选择使用 PageRank（Page et al., 1999）而非度中心性等局部指标对其进行排序。我们注意到，这一选择不影响第 1 节中呈现的实验结果。概念排序后，我们根据块所关联的概念对其进行排序。GraphRAG 方法允许按指定比例选择核心块以构建 KG，从而降低整体构建成本。在 G2ConS 中，概念图与 KG 共存。KG 由 MS-GraphRAG 构建，且因其仅由核心块构建，我们简称为 core-KG。

#### 3.2.1 双路径检索与集成

尽管核心块选择有效降低了成本，但不可避免地过滤掉许多块。为确保知识完整性，我们提出同时从概念图与 core-KG 中进行检索。

**概念图上的局部搜索**。概念图上的检索基本遵循当前 GraphRAG 的主流设计，即第 3.1 节所述的局部搜索。关键区别在于我们引入预算参数 \( \beta \) 以精确控制检索的上下文 token 数量。给定问题 \( q \)，我们首先计算其对应向量 \( \phi(q) \)，然后计算 \( \phi(q) \) 与所有概念向量的余弦相似度。基于该相似度排序，我们检索 top-k 个最相关概念（k 为常数），称为**直接关联概念**。随后，以这些直接关联概念为种子节点，执行 BFS 以检索 i 跳概念（i=1,…,N），称为**扩展概念**。我们对这两类概念采用不同的上下文构建策略：对于直接关联概念，我们按其与 \( \phi(q) \) 的相似度排序，依次检索每个概念对应的块，并在添加到上下文前按与 \( \phi(q) \) 的相似度重新排序。我们将此过程称为**局部重排序**。对于扩展概念，我们收集 BFS 发现的所有概念及其相关块，对整个检索块集按与 \( \phi(q) \) 的相似度进行重排序后添加到上下文，该过程称为**全局重排序**。每次向上下文添加块时，我们检查累计 token 数；若超过 \( \beta \)，则终止整个检索过程。core-KG 上的检索过程细节略去，因其遵循所用 GraphRAG 方案，仅将检索内容截断至 token 预算 \( \beta \)。我们注意到，概念图与 core-KG 上的检索并行进行以最大化效率。

**加权上下文集成**。检索后，我们实际获得两个上下文，每个不超过 \( \beta \) 个 token。但检索结果粒度不同：概念图完全由块组成；core-KG 结果包含实体、关系和块。因此，我们进一步引入组合权重 \( \lambda \in (0, 1) \) 以设置两者偏好。其次，由于两者可能检索到重叠块，我们基于投票策略赋予重叠块更高优先级。最终上下文按以下原则构建：1. 确保总 token 数不超过 \( \beta \)；2. 优先保留重叠块；3. 概念图占用不超过 \( (1 - \lambda)\beta \) 个 token，core-KG 占用不超过 \( \lambda\beta \) 个 token。

## 4 实验

### 4.1 实验设置

我们在三个广泛使用的多跳 QA 基准上评估我们的方法：MuSiQue（Trivedi et al., 2022）、HotpotQA（Yang et al., 2018）和 2WikiMultihopQA（Ho et al., 2020）。遵循先前工作（Jimenez Gutierrez et al., 2024；Wang et al., 2024；Huang et al., 2025b），我们从每个数据集的验证集中采样 500 个 QA 对。对每对，我们收集所有相关支持段落与干扰段落以构建 RAG 的外部语料库 \( T \)。最终语料库包含 MuSiQue 的 6,761 个段落（741,285 个 token）、HotpotQA 的 9,811 个（1,218,542 个 token）和 2WikiMultihopQA 的 6,119 个（626,376 个 token）。

在评估方面，我们遵循多跳 QA 任务的通用做法。检索质量通过**上下文召回率**（CR）衡量，即检索上下文是否包含真实答案。生成质量通过提示 LLM 基于检索上下文生成答案并与给定答案比较来评估。评估指标包括：**精确匹配**（EM），衡量与真实答案完全匹配的预测比例；**F1 分数**，通过 token 级重叠捕捉部分正确性；**BERTScore**，使用基于 BERT 的嵌入计算语义相似度。

我们系统评估了 16 种方案，分为两类：(i) 现有方法：TextRAG（Lewis et al., 2020）、MS-GraphRAG（Edge et al., 2024）、Hybrid-RAG（Sarmah et al., 2024）、HippoRAG（Jimenez Gutierrez et al., 2024）、LightRAG（Guo et al., 2024）（含 Local、Global 和 Hybrid 变体）、Fast-GraphRAG（AI, 2024）、Raptor（Sarthi et al., 2024）和 KET-RAG（Huang et al., 2025b）（含 Keyword、Skeleton 和 Combine 变体）；(ii) 提出的方法：Concept-GraphRAG（仅从 \( G_c \) 检索）、Core-KG-RAG（仅从 \( G_{ck} \) 检索，其中 \( G_{ck} \) 默认使用 MS-GraphRAG 构建，除非另有说明）、G2ConS（从 \( G_c \) 和 \( G_{ck} \) 同时检索）。基线详情见附录 A.1。

所有方案共享相同的推理设置：生成使用 OpenAI GPT-4o-mini，嵌入使用 OpenAI text-embedding-3-small，分词使用 OpenAI tiktoken cl100k_base。我们将最大输入块大小设为 \( \ell = 1200 \)，输出上下文限制设为 \( \beta = 12000 \)。在 G2ConS 中，除非另有说明，默认参数为 \( \theta_{\text{sim}} = 0.65 \)、\( \theta_{\text{co}} = 3 \)、\( \kappa = 0.8 \)、\( \lambda = 0.6 \)、\( k = 25 \)、\( N = 2 \)。这些参数定义见表 1。我们运行所有实验五次并报告平均值。

> **表1：G2ConS 中的关键参数**  
> | 符号 | 描述 |  
> |---|---|  
> | \( \theta_{\text{sem}} \) | 概念图的相似度阈值 |  
> | \( \theta_{\text{co}} \) | 概念图的共现阈值 |  
> | \( \kappa \) | 核心块选择比例 |  
> | \( \lambda \) | 上下文集成中 core-KG 的权重 |  
> | \( k \) | 检索的 top-k 项数 |  
> | \( N \) | 概念图检索的 BFS 深度 |

> **表2：在 MuSiQue、HotpotQA 和 2WikiMultihopQA 上的主要结果**  
> （注：表格内容因格式限制略去，实际翻译中应完整保留原表结构与数据。此处仅说明：表格包含各方法在三个数据集上的 USD 成本、CR、EM、F1、BERTScore 五项指标，G2ConS 在多数指标上领先。）

### 4.2 性能评估

我们在三个广泛使用的多跳 QA 基准（MuSiQue、HotpotQA、2WikiMultihopQA）上将 G2ConS 与代表性 GraphRAG 基线进行比较。基线包括 Text-RAG、MS-GraphRAG、HippoRAG、LightRAG、Fast-GraphRAG 和 KET-RAG。表 2 报告了完整结果。总体而言，G2ConS 在准确率与效率之间取得最佳平衡，在显著降低计算成本的同时，在检索与生成质量上均取得显著提升。

在 MuSiQue 上，G2ConS 展现出最强的整体性能：检索方面，明显优于 LightRAG-Hybrid（+22.3% CR）；生成方面，相比 Fast-GraphRAG 提升显著（+47.8% EM / +54.0% F1），相比 MS-GraphRAG 提升更大（+54.7% EM / +73.2% F1）。效率方面，轻量级变体 G2ConS-Concept 以仅 0.6% 的成本（与 Text-RAG 相当）取得第二佳性能，凸显其可扩展性。

在 HotpotQA 上，G2ConS 表现出数据集依赖性：生成方面达到 SOTA，相比 MS-GraphRAG 提升（+2.0% EM / +4.0% F1），相比 HippoRAG 提升更大（+9.0% EM / +13.0% F1）。尽管与 MS-GraphRAG 的差距较小，G2ConS 的构建成本仅为其 70%（降低 30%），体现出更优的成本-性能权衡。检索方面，G2ConS 略逊于 LightRAG-Hybrid 和 KET-RAG-Keyword，但后者的高 CR 在其他基准上并不一致。

在 2WikiMultihopQA 上，G2ConS 在检索与生成上均创 SOTA：检索方面略优于 HippoRAG（+2% CR），明显优于 KET-RAG（+8% CR）；生成方面相比 HippoRAG 有小幅但一致提升（+3% EM / +1% F1），相比 MS-GraphRAG 提升显著（+67.6% EM / +43.9% F1）。效率方面，尽管性能与 HippoRAG 接近，G2ConS 的计算成本仅为其 58.4%（降低 41.6%），图构建时间仅需其 5%（见图 3(b)），效率优势显著。

### 4.3 与主流 GraphRAG 的兼容性研究

为评估 G2ConS 与主流工业方法的兼容性，我们选取三种代表性 GraphRAG 模型——LightRAG、MS-GraphRAG 和 HippoRAG——作为 G2ConS-CoreKG 索引的构建主干。在完整文本块集上构建 G2ConS 概念图后，我们应用 PageRank 对其重要性排序，并选择前 20% 和 80% 的显著子集用于知识图谱构建。例如，对 LightRAG 构建三个索引：Glight100、Glight80 和 Glight20，分别使用 100%、80% 和 20% 的文本块。在 G2ConS 增强设置中，Glight80 和 Glight20 的局部搜索结果与 G2ConS 概念图检索的上下文合并，合并后的上下文输入 LLM 生成答案（最大输入长度为 \( \lambda \) 个 token）。

如表 3 所示，集成 G2ConS 概念图在显著降低构建开销的同时，对所有三种 GraphRAG 方法均带来一致提升。即使仅使用 20% 的文本，G2ConS 增强模型在 EM 和 F1 上仍优于其完整块基线，且构建成本降低约 80%。使用 80% 文本时，性能增益更大，而成本仍比基线低约 20%。这些结果源于 G2ConS 能识别并保留高连接度的概念，从而为 RAG 方法提供了一个紧凑而语义更丰富的候选段落池用于图构建。双路径检索确保 LLM 在相同 token 预算内利用更丰富的上下文，从而提升准确率并降低计算成本。

> **表3：有无 G2ConS-Concept 的 GraphRAG 框架对比**  
> （注：表格内容略，实际应保留原结构，展示 LightRAG、MS-GraphRAG、HippoRAG 在 100%、80%、20% 块比例下结合 G2ConS-Concept 的 CCR、EM、F1、BERTScore。）

### 4.4 构建时间与成本研究

为考察不同方法在效率与准确率间的权衡，我们在 MuSiQue 基准上评估 G2ConS 与六种代表性 RAG 方法，重点关注构建成本、构建时间与性能的权衡。我们采用数据集范围的成本与性能中位数作为参考阈值，形成二维象限空间，将方法分为四个区域。

如图 3(a) 所示，G2ConS-Concept 定义了低成本-高性能前沿，以最小开销实现领先准确率。Fast-GraphRAG 也位于该象限，但成本更高且性能接近中位数。G2ConS-Core-KG 位于中位数交点，作为平衡基线；G2ConS 位于高成本-高性能区域，在更高构建成本下展现卓越准确率。其余方法聚集在低成本-低性能区域。在“构建时间 vs. 性能”平面上（图 3(b)），G2ConS-Concept 与 Core-KG 再次展现低成本-高性能优势，而 G2ConS 位于中成本-高性能边界。相比之下，HippoRAG 和 LightRAG 需要显著更长的构建时间。总体而言，结果表明 G2ConS 系列拓展了效率-性能前沿：G2ConS-Concept 适用于效率敏感场景，Core-KG 提供稳健平衡，G2ConS 进一步推高性能上限。

### 4.5 消融研究

**共现粒度的影响**。我们比较 G2ConS（词在文本块内共现且超过相似度阈值时构建边）与变体 A（限制共现为句子级别）。如表 4 所示，在 MuSiQue 基准上，变体 A 的 EM 降低 9.1%，F1 降低 7.3%。这表明句子级窗口过于严格，导致图过于稀疏而丢失跨句依赖，而段落级共现能更好覆盖语义相关词对，生成更完整的语义图。

**相似度约束在边构建中的作用**。我们进一步分析相似度阈值的作用，比较 G2ConS（段落级共现 + 相似度过滤）与变体 B（无过滤的段落级共现）。如表 4 所示，G2ConS 在 MuSiQue 上优于变体 B。相似度阈值有效过滤弱相关词对，避免原始段落级共现产生的过度稠密图，突显真正有意义的语义连接。

**向量化粒度的影响**。我们比较 G2ConS（概念嵌入为其所有句子嵌入的平均）与变体 C（概念嵌入为其所有文本块嵌入的平均）。如表 4 所示，变体 C 性能极差，EM 相对 G2ConS 下降 97.0%，F1 下降 92.5%。这是因为段落级平均导致同一块内许多词共享几乎相同的表示，模糊了语义区别，严重降低检索质量。

**重排序粒度的影响**。有三种重排序方法：**局部**（对 top-k 概念直接关联的块重排序）、**全局**（对 N 跳扩展的块重排序）和**朴素**（将局部与全局块混合后不加区分地排序）。我们比较 G2ConS（局部+全局重排序）与变体 D（朴素重排序）。如表 4 所示，局部+全局重排序显著更高（EM 13.2 vs. 6.4）。其优势在于更细的粒度：先按查询对概念排序，再对其关联块排序，优先保留高度相关内容。而朴素重排序直接按查询相似度对所有块排序，稀释了强信号，导致性能下降。

**整体分析**。综合来看，这些消融结果表明 G2ConS 在图构建与重排序中均实现了覆盖度与鲁棒性的理想平衡：段落级共现通过捕捉跨句依赖缓解了句子级图的稀疏性；相似度阈值通过确保边的语义可靠性缓解了原始段落级共现的过度稠密；句子级向量化保留了词级区别，避免了段落级平均导致的语义坍缩。此外，区分局部与全局重排序范围使 G2ConS 能利用概念级线索而不稀释相关性，这与朴素池化形成对比。这些互补的设计选择使 G2ConS 持续优于所有变体。

> **表4：图构建与检索的消融研究**  
> | 指标 | G2ConS-Concept | A（无块共现） | B（无语义相似） | C（无句子嵌入） | D（无局部重排序） |  
> |---|---|---|---|---|---|  
> | EM | 13.2 | 12.0 | 12.4 | 0.4 | 6.4 |  
> | F1 Score | 21.8 | 20.2 | 20.5 | 1.63 | 14.6 |

### 4.6 参数研究

我们进一步考察 G2ConS 中两个关键参数的影响：核心块选择比例 \( \kappa \) 和上下文集成中 core-KG 的权重 \( \lambda \)。

**\( \kappa \) 的影响**。图 4(a) 报告了固定 \( \lambda = 0.4 \) 时变化 \( \kappa \) 的性能。性能随 \( \kappa \) 增加至 0.6 而稳步提升，在 [0.6, 0.8] 区间保持近最优，超过 0.8 后下降。此模式突显了 PageRank 在选择全局重要块中的作用。

**\( \lambda \) 的影响**。图 4(b) 显示固定 \( \kappa = 0.8 \) 时变化 \( \lambda \) 的效果。性能在 \( \lambda = 0.2 \) 前快速提升，此后增速放缓，在 \( \lambda = 0.6 \) 达到峰值后下降。这表明最优融合需平衡 G2ConS-Concept 与 G2ConS-Core-KG 的贡献：\( \lambda \) 过小则未充分利用结构知识，过大则削弱语义信号。基于此，我们将默认值设为 \( \kappa = 0.8 \)、\( \lambda = 0.6 \)，在各基准上均取得稳健性能。

> **图4：不同 \( \kappa \) 和 \( \lambda \) 下的回答质量**。（a）固定 \( \lambda = 0.4 \) 时变化 \( \kappa \)；（b）固定 \( \kappa = 0.8 \) 时变化 \( \lambda \)。

## 5 结论

本文提出了 G2ConS，一种与主流 GraphRAG 兼容的高效 RAG 方案。其核心思想是通过共现关系从知识中挖掘核心概念，并利用这些核心概念过滤文本块，从而在数据层面降低 GraphRAG 的构建成本。实验表明，G2ConS 能同时优化现有 GraphRAG 方法的成本与性能。此外，G2ConS 提出的概念图可独立执行 RAG，在近乎零构建成本下取得与现有方法相当的性能。未来工作将进一步探究核心块选择的底层原理，并对概念图进行冗余分析以改进 G2ConS 的各个方面。此外，我们将探索在多模态场景中应用 G2ConS，以在更通用的设置中构建高效 RAG 系统。

**LLM 使用说明**：我们仅使用 LLM 对论文手稿进行润色。

## 参考文献

（此处为参考文献列表的完整中文翻译，保留原文作者、年份、标题、会议/期刊、arXiv 链接等格式）

Amar Abane, Anis Bekri, Abdella Battou, and Saddek Bensalem. FastRAG：面向半结构化数据的检索增强生成。arXiv 预印本 arXiv:2411.13773，2024。  
CircleMind AI. Fast GraphRAG：用于可解释、高精度、代理驱动检索工作流的简化且可提示框架。https://github.com/circlemind-ai/fast-graphrag，2024。访问日期：2025-04-05。  
Shawn Arnold 和 Clayton Romero。电子发现管理的关键作用，2022。网址 https://legal-tech.blog/the-vital-role-of-managing-e-discovery。  
Cristian S Calude 和 Giuseppe Longo。大数据中的虚假相关洪流。《科学基础》，22(3):595–612，2017。  
……（其余参考文献按相同格式翻译）

## 附录

### A.1 基线方法

**Text-RAG**。Lewis 等人（2020）提出的 Text-RAG 将预训练的序列到序列语言模型与非参数记忆结合，后者实现为外部文档（如维基百科）的稠密向量索引。推理时，给定输入查询，预训练的神经检索器首先对查询编码并从索引中检索相关段落。语言模型随后基于这些检索段落生成输出序列。存在两种形式：一种对所有输出 token 使用相同检索段落，另一种为每个 token 动态选择不同段落。

**MS-GraphRAG**。Edge 等人（2024）提出的 GraphRAG 是一种基于图的方法，用于在大型私有文本语料库上回答全局问题。它首先使用大语言模型（LLM）从源文档构建实体知识图谱。然后，LLM 为图中紧密相关实体的每个社区生成摘要。推理时，给定用户问题，GraphRAG 检索相关社区摘要，从中生成部分响应，并通过汇总这些部分响应生成最终答案。

以下是附录 A.1 中剩余基线方法的完整中文翻译，延续此前风格，严格保持原文结构与技术表述：


**Hybrid-RAG**。Sarmah 等人（2024）提出的 HybridRAG 将基于知识图谱的 RAG（GraphRAG）与基于向量数据库的 RAG（VectorRAG）相结合，以提升对复杂金融文档（如财报电话会议记录）的问答能力。该方法同时从向量索引和领域特定知识图谱中检索相关上下文，然后融合这两种来源的信息以生成最终答案。

**HippoRAG**。Jimenez Gutierrez 等人（2024）提出的 HippoRAG 是一种受人类长期记忆海马体索引理论启发的检索框架。它将大语言模型、知识图谱与个性化 PageRank 算法相结合，以模拟新皮层与海马体的互补作用。在接收到新信息时，HippoRAG 构建一个知识图谱，并应用个性化 PageRank 识别相关节点，这些节点用于引导支持证据的检索以回答问题。该过程实现了高效的单步检索，无需迭代式提示。

**LightRAG**。Guo 等人（2024）提出的 LightRAG 通过将图结构引入文本索引与检索过程来增强检索增强生成。该框架采用双层级检索系统，同时在低层文本单元与高层图结构知识表示上进行操作。在检索阶段，该框架结合向量嵌入与图连通性，高效识别相关实体及其关系。其增量更新算法可动态将新数据集成到图中，确保索引始终保持最新，而无需全量重建。

**FastGraphRAG**。AI（2024）提出的 FastGraphRAG 是一种检索增强生成框架，通过轻量级图结构加速知识密集型推理。它从输入文档构建紧凑的实体关系图，并采用快速近似图遍历方法（如个性化 PageRank 或随机游走）在单次检索步骤中识别相关上下文。通过将基于图的语义连通性与稠密向量表示相结合，FastGraphRAG 能够在最小计算开销下快速获取局部与多跳证据。

**Raptor**。Sarthi 等人（2024）提出的 RAPTOR 是一种检索增强语言模型，通过递归嵌入、聚类和摘要，从文档块构建层次化文本摘要树。该方法从叶子节点的细粒度片段出发，逐步聚合并摘要内容，形成朝向根节点的更高层抽象。在推理阶段，RAPTOR 从该树的多个层级检索相关节点，从而整合不同粒度的信息，支持对长文档的整体理解。

**KET-RAG**。Huang 等人（2025b）提出的 KET-RAG 是一种面向 GraphRAG 的多粒度索引框架，在检索质量与索引效率之间取得平衡。它首先选取少量关键文本块，并利用大语言模型从中提取实体与关系，构建一个紧凑的知识图谱骨架。对于其余文档，则构建轻量级的文本-关键词二分图，而非完整的三元组知识图谱。在检索阶段，KET-RAG 在骨架图上执行局部搜索，同时在二分图上模拟类似的遍历过程以丰富上下文。
