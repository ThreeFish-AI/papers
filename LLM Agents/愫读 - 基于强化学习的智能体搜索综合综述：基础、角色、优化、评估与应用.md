# 愫读摘要：基于强化学习的智能体搜索综合综述

**原文标题**：A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications

**作者**：Minhua Lin; Zongyu Wu; Zhichao Xu; Hui Liu; Xianfeng Tang; Qi He; Charu Aggarwal; Hui Liu; Xiang Zhang; Suhang Wang

---

## 论文概览

本论文是首个专门针对基于强化学习（RL）的智能体搜索的综合综述，从三个互补维度阐明 RL 如何使智能体搜索受益：

1. **RL 的用途**：描述其在指导检索、推理和决策制定中的功能角色
2. **RL 的使用方式**：涵盖奖励设计、策略学习和高级训练方法等优化策略
3. **RL 的应用范围**：从代理级别到步骤和模块级别检查 RL 干预的范围

论文提出了"基于 RL 的智能体搜索"的定义：将 LLM 训练为决策代理，该代理与搜索环境交互，接收外部反馈，并迭代改进策略以最大化奖励。

---

## 摘要

### 核心问题

- 大语言模型（LLMs）存在静态知识截止点限制、事实性幻觉、无法访问实时信息等问题
- 传统 RAG 系统通常是单轮和启发式驱动的，缺乏迭代改进和动态调整能力
- 现有综述较少关注 RL 在智能体搜索中的作用，或仅限于特定子领域

### 主要贡献

- **首个综合性综述**：专门针对基于 RL 的智能体搜索的全面调研
- **三维分析框架**：从用途、使用方式、应用范围三个维度系统分析
- **技术发展趋势**：总结代表性方法并指出新兴趋势

### 论文结构

- **第 2 节**：介绍智能体搜索和 RL 的基础知识
- **第 3-5 节**：从三个维度审视智能体搜索中的 RL
- **第 6 节**：回顾评估指标和代表性应用
- **第 7 节**：讨论开放挑战和未来方向

---

## 1. 引言

### 背景与动机

**LLMs 的优势与局限**：

- 优势：在自然语言理解、推理和生成方面展现前所未有的能力
- 局限：静态知识截止点、事实性幻觉、无法访问实时或特定领域信息

**RAG 范式的演进**：

- **传统 IR**：依赖 TF-IDF、BM25、PageRank 等统计模型
- **RAG**：整合检索与生成，但通常是单轮、启发式驱动
- **智能体搜索**：LLMs 作为自主决策者，动态规划、检索、推理并反思

**RL 带来的变革**：

- **自主性**：代理确定其搜索行动
- **学习性**：策略通过强化而非手动设计获得
- **交互性**：代理与搜索环境进行多轮交换

### 研究定位

如表 1 所示，与现有综述相比，本工作独特地将 RL 基础与智能体搜索行为统一，全面分析 RL 在智能体搜索中的作用。

---

## 2. 背景和预备知识

### 2.1 作为智能体的大语言模型

LLMs 通过外部信息源和决策能力增强，从静态知识消费者转变为能够调用外部工具、与动态环境交互的自主智能体。

### 2.2 从传统 IR 到智能体搜索的演进

#### 传统信息检索（IR）

- **核心方法**：TF-IDF、BM25、PageRank
- **特点**：返回排序文档列表，用户自行解释和综合
- **局限**：无法捕获复杂用户意图或多步推理

#### 检索增强生成（RAG）

- **架构**：检索-然后-阅读
- **优势**：改进事实基础
- **局限**：单轮、缺乏自适应查询改进、容易受噪声影响

#### 智能体搜索

- **定义**：基于 LLM 的系统，整合动态推理、自适应规划、多轮检索、工具使用和证据综合
- **核心转变**：从静态证据注入到动态工具使用

### 2.3 强化学习基础

#### 基本概念

RL 通过代理与环境的交互来学习最优策略，核心组件包括：

- **状态（s_t）**：当前上下文
- **动作（a_t）**：代理的决策
- **奖励（r_t）**：反馈信号
- **策略（π）**：状态到动作的映射

#### 主要算法类别

##### 在线策略优化

- **基于 Critic 的算法**：使用价值函数估计预期返回，如 PPO
- **无 Critic 的算法**：通过相对奖励统计估计优势，如 GRPO、DAPO

**近端策略优化（PPO）**：

$$
  J_PPO(θ) = E[min(π_θ/π_old * A, clip_ε(π_θ/π_old) * A) - β * D_KL(π_θ || π_ref)]
$$

**组相对策略优化（GRPO）**：通过消除价值函数需求，提高训练效率

##### 离线策略优化

**直接偏好优化（DPO）**：直接从偏好标注数据学习，避免显式奖励建模：

$$
  J_DPO(θ) = E[log σ(β(π_θ(y_w|x)/π_ref(y_w|x) - π_θ(y_l|x)/π_ref(y_l|x)))]
$$

### 2.4 基于 RL 的智能体搜索形式化

#### MDP 建模

- **代理**：LLM 策略 π_θ
- **环境**：搜索引擎 API、检索器、知识图等外部资源
- **状态**：查询、推理轨迹、检索证据、动作历史
- **动作**：发出查询、改进查询、选择文档、调用工具、终止
- **奖励**：任务成功、过程质量、资源成本

#### RL 干预点

1. **搜索控制**：是否/何时检索
2. **查询优化**：如何检索
3. **推理集成**：如何使用检索信息

#### 与预 RL 方法的比较

- **基于提示的方法**：依赖人工设计的启发式和工作流
- **基于 SFT 的方法**：从高质量轨迹数据学习行为
- **RL 的优势**：通过试错适应，直接优化任务级奖励

---

## 3. RL 的用途：智能体搜索中的功能角色

RL 在智能体搜索中发挥五大功能角色：

### 3.1 检索控制

#### 自适应搜索决策

- **核心问题**：决定是否需要外部检索
- **代表性方法**：Search-R1、ReSearch、R1-Searcher、DeepRAG
- **机制**：仅在内部知识不足时调用搜索引擎

#### 搜索强度和持久性

- **目标**：优化搜索深度和持久性
- **方法**：
  - Pangu DeepDiver：搜索强度缩放
  - ReZero：失败后重试奖励
  - StepSearch：基于信息增益的逐步奖励

#### 搜索效率

- **关注点**：最小化成本和延迟
- **策略**：
  - R1-Searcher++：组奖励测量检索节俭性
  - IKEA：知识边界感知奖励
  - ParallelSearch：并行子查询降低响应时间

### 3.2 查询优化

#### 对话式改写

- **挑战**：用户查询模糊或依赖上下文
- **解决方案**：
  - ConvSearch-R1：基于检索排名的奖励优化
  - MaskSearch：结合改写器代理改进查询
  - RAG-R1：生成多个并行查询

#### 检索器感知优化

- **目标**：使查询适应特定检索器特性
- **方法**：DeepRetrieval、ZeroSearch、s3 等

### 3.3 推理-检索集成

#### 推理-搜索交错

- **策略**：将推理和检索动作交替进行
- **代表方法**：SWiRL、R-Search、AutoRefine、ReasonRAG

#### 上下文和内存管理

- **挑战**：处理长上下文和记忆问题
- **方案**：
  - **内部管理**：提示压缩、词元级过滤
  - **外部管理**：辅助摘要模块压缩历史上下文

### 3.4 多代理协作

#### 规划器-执行器架构

- **结构**：高级规划器协调专门执行器
- **代表工作**：
  - MAO-ARAG：多代理半马尔可夫决策过程
  - OPERA：分层 RL 框架，使用 MAPGRPO 算法

#### 协作多代理系统

- **特点**：多个代理共享全局奖励，协同优化
- **例子**：SIRAG、MMOA-RAG、AgentGym-RL

### 3.5 工具和知识集成

#### 多工具和多模态推理

- **范围**：协调搜索引擎、代码解释器、视觉模型等
- **代表方法**：Tool-Star、VerlTool、MMSearch-R1、WebWatcher

#### 结构化知识导航

- **目标**：在知识图、数据库等结构化资源中导航
- **方法**：GRAIL（知识图遍历）、DynaSearcher（多奖励优化）

### 要点总结

- **检索控制**：改进何时检索、搜索持久性和效率
- **查询优化**：实现对话式改写和检索器感知适应
- **推理-检索集成**：联合优化推理和证据使用，支持内存管理
- **多代理协作**：对齐规划器-执行器和协作代理的目标
- **工具和知识集成**：协调异构工具和结构化知识源

---

## 4. RL 如何使用：优化策略

### 4.1 训练制度

#### 标准智能体搜索管道

以 Search-R1 为例的典型训练流程：

1. **冷启动初始化**：通过 SFT 提供基线能力
2. **RL 微调**：在模拟或真实环境中优化策略

**Search-R1 提示模板示例**：

```
回答给定问题。每次获得新信息时，你必须首先在 <thinking> 和 </thinking> 内进行推理。推理后，如果你发现缺乏某些知识，可以通过 <search> 查询 </search> 调用搜索引擎，它将在 <information> 和 </information> 之间返回前搜索结果。你可以根据需要搜索多次。
```

#### 冷启动策略

- **SFT 初始化**：提供接口合规性和稳定性（Webagent-R1、WebSailor）
- **纯 RL 训练**：ZeroSearch 使用潜在空间模拟，AgentGym-RL 采用课程学习
- **自改进循环**：EvolveSearch 结合 SFT 和 RL 迭代改进

#### 基于模拟的训练

- **优势**：减少 API 调用成本，避免真实搜索引擎依赖
- **方法**：
  - 检索模拟器：ZeroSearch、IKAR
  - 课程学习：从简单到复杂任务渐进训练

#### RL 算法选择

- **在线策略**：PPO、GRPO、DAPO 等，适用于实时交互
- **离线策略**：DPO、ReMix 等，利用已有数据高效学习
- **混合策略**：结合在线和离线优化的优势

#### 课程学习和水平缩放

- **渐进难度**：从简单任务开始，逐步增加复杂度
- **技能层次**：基础技能到高级推理能力的分层训练
- **适应性调整**：基于代理表现动态调整训练难度

#### 迭代和自进化框架

- **演示生成**：RL 精炼策略生成新演示用于 SFT 重训练
- **持续改进**：通过迭代循环不断提升代理能力
- **自适应优化**：根据性能反馈调整训练策略

### 4.2 奖励设计

#### 奖励函数分类

##### 结果导向奖励

- **基于正确性**：答案准确性、F1 分数、EM 分数
- **基于事实性**：事实验证、引用一致性
- **基于有用性**：用户满意度、任务完成度

##### 过程导向奖励

- **效率指标**：检索次数、响应时间、计算成本
- **推理质量**：逻辑连贯性、步骤合理性
- **工具使用**：工具选择恰当性、调用效率

##### 复合奖励设计

**MAO-ARAG 奖励函数**：

$$
  r_t = r_F1 - α * r_CP - r_FP
$$

其中 r_F1 为 F1 分数，r_CP 为成本惩罚，r_FP 为格式惩罚

#### 奖励机制类型

##### 结果级奖励（ORM）

- **定义**：基于最终输出质量评估的奖励
- **评估方式**：
  - **基于规则**：使用预定义规则计算（如 EM、F1）
  - **基于 LLM**：使用大语言模型作为评判者
- **优势**：直接与任务目标对齐
- **局限**：奖励稀疏，难以指导中间步骤

##### 过程级奖励（PRM）

- **定义**：基于推理过程质量评估的奖励
- **评估维度**：
  - 步骤逻辑性
  - 检索相关性
  - 工具使用恰当性
- **优势**：提供密集反馈信号
- **局限**：评估复杂度较高

#### 奖励塑形技术

- **稀疏奖励缓解**：中间步骤奖励、课程奖励
- **多目标平衡**：正确性、效率、成本的权衡
- **信用分配**：长期贡献的准确归因
- **奖励标准化**：处理不同尺度奖励的统一方法

#### 代表性奖励函数比较

| 方法      | 奖励类型 | 评估方式 | 优化目标             |
| --------- | -------- | -------- | -------------------- |
| Search-R1 | ORM      | 基于规则 | 答案 EM              |
| IKEA      | ORM      | 基于规则 | 答案 EM + 知识边界   |
| ReZero    | ORM+PRM  | LLM 评判 | 格式 + 答案 + 重试   |
| MAO-ARAG  | ORM      | 基于规则 | F1 - 成本 - 格式惩罚 |

### 要点总结

- **训练制度**：从 SFT 冷启动到纯 RL 训练，多种初始化策略
- **环境设计**：真实环境与模拟环境的权衡
- **奖励机制**：从简单正确性到复合多目标奖励

---

## 5. RL 应用范围：优化范围

### 5.1 代理级范围

#### 单代理优化

- **定义**：将整个智能体搜索系统作为单一 RL 代理优化
- **特点**：端到端优化，全局策略学习
- **代表方法**：Search-R1、WebSailor、DeepResearcher
- **优势**：
  - 全局最优策略
  - 统一的奖励函数
  - 简化的训练流程
- **挑战**：
  - 训练复杂度高
  - 可解释性较差
  - 调试困难

#### 多代理协调

- **场景**：多个智能体协同完成复杂搜索任务
- **协调机制**：
  - **规划器-执行器模式**：中央规划器分配任务
  - **平等协作模式**：代理间直接通信协商
  - **层级管理模式**：多级代理架构
- **代表方法**：MAO-ARAG、OPERA、SIRAG

### 5.2 模块级和步级范围

#### 模块级优化

- **模块类型**：
  - **检索模块**：查询生成、文档选择
  - **推理模块**：逻辑推理、答案生成
  - **控制模块**：搜索决策、流程管理
- **优化策略**：
  - 独立训练各模块
  - 模块间协调优化
  - 增量式模块改进

#### 步级优化

- **精细粒度控制**：对每个决策步骤进行精细优化
- **应用场景**：
  - 查询改写步骤
  - 文档检索步骤
  - 推理整合步骤
- **优势**：
  - 更好的可解释性
  - 模块化设计
  - 易于调试和改进

### 5.3 系统级范围

#### 搜索的统一基于 RL 的框架

- **统一架构**：整合不同类型的智能体搜索系统
- **资源共享**：
  - 工具池共享
  - 知识库共享
  - 计算资源分配
- **标准化接口**：统一的 API 和协议设计

#### 跨域优化

- **领域适应**：不同领域的搜索策略迁移
- **知识共享**：跨域知识和经验复用
- **性能平衡**：多领域性能的权衡优化

### 要点总结

不同优化范围各有优势：

- **代理级**：
  - 优势：端到端最优，统一策略
  - 劣势：复杂度高，可解释性差
- **模块级**：
  - 优势：可解释性强，易于调试
  - 劣势：可能局部最优，协调困难
- **系统级**：
  - 优势：资源优化，标准化
  - 劣势：协调复杂，设计难度大

### 优化范围选择指南

- **简单任务**：推荐代理级优化
- **复杂任务**：推荐模块级或系统级优化
- **多应用场景**：推荐系统级统一框架
- **研究原型**：推荐模块级便于分析和改进

---

## 6. 评估和应用

### 6.1 数据集分类

#### 知识密集型 QA

- **单跳 QA**：Natural Questions、TriviaQA
- **多跳 QA**：HotpotQA、2WikiMultiHopQA
- **事实检查**：FEVER、ClaimVerification

#### 网络搜索任务

- **开放域 QA**：WebQuestions、CuratedTrec
- **深度研究**：AssistantBench、SearchBench

#### 多模态任务

- **视觉问答**：OK-VQA、A-OKVQA
- **图文检索**：MSCOCO、Flickr30k

### 6.2 评估指标

#### 效果指标

- **准确性**：EM、F1、精确率、召回率
- **事实性**：引用一致性、事实验证分数
- **有用性**：人类评分、任务完成率

#### 效率指标

- **成本**：API 调用次数、token 消耗
- **延迟**：响应时间、推理速度
- **资源利用**：计算资源、内存使用

#### 鲁棒性指标

- **泛化能力**：跨领域性能
- **噪声容忍**：有噪声检索下的表现
- **长期一致性**：长对话中的连贯性

### 6.3 应用领域

#### 学术研究

- **文献检索**：学术论文查找和综述
- **事实检查**：科学声明验证
- **研究辅助**：实验设计和数据分析

#### 专业服务

- **医疗诊断**：症状分析和文献查询
- **法律研究**：案例检索和法律分析
- **金融分析**：市场信息和投资决策

#### 通用助手

- **问答系统**：开放域问题回答
- **旅行规划**：信息收集和行程安排
- **购物助手**：产品比较和推荐

---

## 7. 挑战和未来方向

### 7.1 多模态智能体搜索

- **挑战**：文本、图像、音频、视频的统一处理
- **方向**：跨模态检索、多模态推理、统一表示学习

### 7.2 内存增强和长视野搜索

- **挑战**：长对话记忆、上下文压缩、持续学习
- **方向**：动态内存管理、分层摘要、渐进式学习

### 7.3 可信赖智能体搜索

- **挑战**：事实性、偏见、安全性、可解释性
- **方向**：事实验证、偏见缓解、安全约束、可解释 RL

### 7.4 跨领域泛化

- **挑战**：领域适应、零样本泛化、快速适应
- **方向**：元学习、领域自适应训练、少样本学习

### 7.5 人机协同搜索

- **挑战**：用户意图理解、交互设计、协同优化
- **方向**：交互式 RL、用户建模、个性化搜索

---

## 8. 结论

### 主要贡献

1. **首创性综述**：首个专门针对基于 RL 的智能体搜索的全面调研
2. **三维分析框架**：系统分析了 RL 的用途、使用方式和应用范围
3. **技术趋势总结**：梳理了代表性方法和新兴发展方向
4. **挑战识别**：指出了当前研究的局限性和未来机会

### 核心洞察

- **统一机制**：RL 是基础、扩展和组织智能体搜索行为的统一机制
- **自适应优势**：RL 使智能体能够通过试错动态适应复杂环境
- **多维度应用**：从检索控制到多代理协作，RL 发挥广泛作用

### 未来展望

基于 RL 的智能体搜索代表了信息检索和 AI 推理融合的重要方向，随着技术发展，有望在更多实际应用场景中发挥重要作用，推动 AI 系统向更自主、更智能、更可信的方向发展。

---

## 关键表格总结

### 表 2：功能角色分类概览

论文详细分类了基于 RL 的搜索代理在五大功能角色上的代表性工作，为研究者和实践者提供了系统的技术地图。

### 表 4：奖励函数比较

总结了不同奖励设计策略的优缺点，指导实际应用中的奖励机制选择。

### 表 7：优化策略概览

从训练制度、奖励设计、算法选择等维度全面梳理了 RL 在智能体搜索中的优化策略。

---

_本摘要基于论文原文内容整理，保留了核心概念、方法分类和技术洞察，旨在为读者提供快速而全面的理解。_
