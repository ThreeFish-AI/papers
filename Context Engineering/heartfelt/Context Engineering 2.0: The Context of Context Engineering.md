以下是论文《[Context Engineering 2.0: The Context of Context Engineering](https://arxiv.org/abs/2510.26493)》的关键内容：

---

> “一个人是其所有上下文的总和。” — 作者

---

## 摘要

卡尔·马克思曾写道：“人的本质是一切社会关系的总和”（Marx, 1845），意味着个体并非孤立存在，而是由其与他者的互动所塑造——其中**上下文**扮演着构成性和本质性的角色。随着计算机与人工智能的出现，这些上下文不再仅限于人与人之间的互动，也包括人机互动。由此产生一个核心问题：**机器如何更好地理解我们的处境与目的？**

为应对这一挑战，研究者近期提出了“上下文工程”（context engineering）这一概念。尽管常被视为智能体时代的新创，但我们认为，相关实践可追溯至 20 多年前。自 1990 年代起，上下文工程已随机器智能水平的演进而经历了若干历史阶段：

- **1.0 时代**：围绕原始计算机构建的早期人机交互（HCI）框架
- **2.0 时代**：由智能体驱动的现代人机智能体交互（HAI）范式
- **3.0 与 4.0 时代**（未来）：迈向人类级乃至超人类智能

本文探讨“上下文工程的上下文”，提供系统性定义，梳理其历史与概念演进，并分析实践中的关键设计考量。我们旨在为上下文工程奠定概念基础，并勾勒其前景，作为推动 AI 系统中系统化上下文工程的社区努力的第一步。

![image.png](%E6%84%AB%E8%AF%BB%E3%80%8AContext%20Engineering%202%200%20The%20Context%20of%20Context%20/image.png)

> 图 1：上下文工程 1.0 至 4.0 概览
>
> 智能越高 → 上下文处理能力越强 → 人机交互成本越低
>
> **我们当前处于 2.0 时代，正向 3.0 时代过渡**

| 时代 | 智能水平   | 人机角色 | 上下文角色       |
| ---- | ---------- | -------- | ---------------- |
| 1.0  | 被动执行者 | 人主导   | 上下文作为“翻译” |
| 2.0  | 主动智能体 | 协作伙伴 | 上下文作为“指令” |
| 3.0  | 可靠协作者 | 平等伙伴 | 上下文作为“场景” |
| 4.0  | 体贴的主人 | 机器主导 | 上下文作为“世界” |

## 1. 引言

近年来，大语言模型（LLM）与智能体的崛起使人们愈发关注**上下文如何影响模型行为**。研究表明，上下文窗口中的内容会显著影响模型性能（Liu et al., 2021）。同时，系统对多步推理与长期任务的需求日益增长（Yao et al., 2023）。

**核心问题**：如何通过有效的上下文机制，使机器在长期任务中更好地理解并执行人类意图？

**上下文工程**被定义为：**设计、组织与管理上下文信息，使机器行为与人类意图对齐**（Mei et al., 2025）。当前实践包括：

- 提示工程（Prompt Engineering）
- 检索增强生成（RAG）
- 工具调用（Tool Calling）
- 长期记忆机制

然而，该领域常被误解为近年新产物，且“上下文”常被狭义理解为对话历史或系统提示。**事实上，上下文工程已有 20 多年历史**，其根基可追溯至普适计算（Ubiquitous Computing）、上下文感知系统（Context-Aware Systems）与人机交互（HCI）研究。

> 上下文工程的本质是“熵减”过程：
>
> 人类能通过共享知识、情感线索等“填补沟壑”，而机器不能。我们必须将高熵（模糊、不完整）的人类意图**预处理为低熵（结构化、明确）的机器可理解形式**。

![image.png](%E6%84%AB%E8%AF%BB%E3%80%8AContext%20Engineering%202%200%20The%20Context%20of%20Context%20/image%201.png)

图 2 展示了碳基（人类）与硅基（机器）认知能力随时间演进的轨迹，**上下文工程正是弥合二者认知鸿沟的桥梁**。

## 2. 理论框架

### 2.1 形式化定义

- **实体与刻画**（Entity & Characterization）：对任一实体 $e \in E$，其情境刻画函数为 $\text{Char}(e)$，返回描述该实体的信息集合。
  当用户在 **Gemini CLI** 中输入命令 `Search related documentation for me` 时，相关实体及其刻画包括：
  - **用户**（User）：输入的提示（prompt）内容
  - **Gemini CLI 应用**（Application）：系统指令、配置文件（如 `GEMINI.md`）
  - **终端环境**（Environment）：当前工作目录（current working directory）
  - **外部工具**（External tools）：可用的插件、搜索工具（如文档检索 API）
  - **记忆模块**（Memory modules）：会话历史（短期）、存储的知识（长期）
  - **后端模型服务**（Model service）：支持的能力（如代码生成、推理格式）
- **交互**（Interaction）：用户与应用之间的可观测行为（显式如点击，隐式如注意力模式）。
  在 Gemini CLI 中：
  - **显式交互**：用户输入的命令 `"Search related documentation for me"`
  - **隐式交互**：
    - 终端当前状态（如 shell 变量、路径）
    - 之前检索到的上下文是否被引用
    - 是否调用了记忆模块（如加载了某段历史对话）
    - 工具调用状态（如是否触发了网络搜索）
- **上下文**（Context）： $C = \bigcup_{e \in E_{\text{rel}}} \text{Char}(e)$，其中 $E_{\text{rel}}$ 为与当前交互相关的实体集合。
  在 Gemini CLI 此次交互中，完整的上下文 C 包含：
  - 用户的输入文本
  - 当前项目根目录下的 `GEMINI.md` 文件内容（定义角色、工具、规范）
  - 终端的当前路径（如 `/home/user/my_project`）
  - 已加载的插件列表（如文档搜索、代码解释器）
  - 上一轮对话的摘要（由系统自动生成）
  - 后端模型支持的响应格式（如 JSON、Markdown）
  > 这些共同构成了机器理解用户意图所需的“情境”。
- **上下文工程**（Context Engineering）： $\text{CE}: (C, T) \rightarrow f_{\text{context}}$，将原始上下文 C 与目标任务 T 转化为优化后的上下文处理函数 $f_{\text{context}}$，以提升任务性能。
  - **任务 T** ：为用户搜索与其当前项目相关的文档
  - **原始上下文 C** ：如上所述（含用户输入、GEMINI.md、路径、工具等）
  - **上下文工程操作 $f_{\text{context}}$** 包括：
    1. **收集**：从文件系统读取 `GEMINI.md`，从终端获取 cwd
    2. **存储**：将会话历史写入本地 SQLite（或内存缓存）
    3. **管理**：
       - 用 AI 自动生成对话摘要（压缩长历史）
       - 按功能打标签（如 `[goal]: search docs`, `[tool]: web_search`）
    4. **使用**：
       - 将摘要 + 当前目标 + 工具说明拼接为系统提示（prompt）
       - 调用检索增强生成（RAG）获取相关文档
       - 将结果与用户历史对齐，避免重复回答
  > 这一整套流程即为一次 上下文工程实践，其目标是让 LLM 在“文档搜索”任务中更准确理解用户意图。

### 2.2 阶段刻画

| 时代    | 时间       | 智能水平   | 特征                                                        |
| ------- | ---------- | ---------- | ----------------------------------------------------------- |
| **1.0** | 1990s–2020 | 原始计算   | 仅处理结构化输入，依赖预定义规则（如菜单选择）              |
| **2.0** | 2020–今    | 智能体中心 | 理解自然语言，处理模糊与不完整信息（如 GPT-3）              |
| **3.0** | 未来       | 人类级智能 | 感知情感、社会线索，实现无缝协作                            |
| **4.0** | 推测       | 超人类智能 | 主动构建上下文，揭示人类未言明的需求（如 AlphaGo 启发棋手） |

## 3. 历史演进（1.0 vs 2.0）

| 维度       | 1.0 时代                      | 2.0 时代                               |
| ---------- | ----------------------------- | -------------------------------------- |
| 技术背景   | 普适计算、上下文感知系统、HCI | LLM、智能体、提示工程                  |
| 典型系统   | Context Toolkit, Cooltown     | ChatGPT, LangChain, AutoGPT            |
| 上下文模态 | 位置、身份、时间、设备状态    | 词元序列、检索文档、工具 API、用户历史 |
| 核心机制   | 传感器融合、规则触发          | 提示、RAG、思维链（CoT）、记忆体代理   |
| 人类相似度 | 低                            | 较高                                   |

## 4–6. 核心设计维度

### 4. 上下文收集与存储

- **1.0**：单设备、本地日志、结构化存储
- **2.0**：多端分布（手机、可穿戴设备、IoT）、分层存储（内存缓存 → 本地 DB → 云）
- **原则**：
  - **最小充分性**：只收集任务必需信息
  - **语义连续性**：保持意义连贯，而非数据连续

### 5. 上下文管理

- **文本处理**：打时间戳、功能标签、QA 压缩、层级笔记
- **多模态融合**：
  ![image.png](%E6%84%AB%E8%AF%BB%E3%80%8AContext%20Engineering%202%200%20The%20Context%20of%20Context%20/image%202.png)
  - 映射至统一向量空间
  - 自注意力跨模态对齐（如 CLIP）
  - 交叉注意力（如 Qwen2-VL）
- **上下文组织**：
  - **分层记忆架构**（短时 vs 长时）
  - **子智能体隔离**（如 Claude Code 子代理）
- **上下文抽象**（Self-Baking）：
  ![image.png](%E6%84%AB%E8%AF%BB%E3%80%8AContext%20Engineering%202%200%20The%20Context%20of%20Context%20/image%203.png)
  - 生成自然语言摘要
  - 按固定模式提取关键事实（如实体图、事件模板）
  - 压缩为语义向量（如 H-MEM）

### 6. 上下文使用

- **系统内共享**：提示嵌入、结构化消息、共享记忆（黑板/图结构）
  ![image.png](%E6%84%AB%E8%AF%BB%E3%80%8AContext%20Engineering%202%200%20The%20Context%20of%20Context%20/image%204.png)
- **跨系统共享**：适配器转换、共享表示（JSON/API/语义向量）
- **上下文选择**：语义相关性、逻辑依赖、近期/高频、去重、用户偏好
- **主动需求推理**：
  - 从提问序列推断隐藏目标
  - 检测用户卡点并主动提供帮助
  - 提前给出可视化或 Checklist
- **终身上下文维护**：需构建“语义操作系统”，支持动态增删改、可解释、抗漂移

## 7. 应用示例

- **CLI 工具**（如 Gemini CLI）：通过 `GEMINI.md` 文件管理项目上下文
- **深度研究智能体**（如 Tongyi DeepResearch）：周期性压缩推理状态，突破上下文窗口限制
- **脑机接口（BCI）**：直接捕获神经信号（注意力、情绪），实现更自然的上下文采集

## 8. 挑战与未来方向

1. **上下文收集低效**：依赖显式输入 → 需多模态、脑机接口等自然采集方式
2. **大规模上下文存储与管理困难**
3. **模型理解能力有限**：仍远逊人类，需更强语义推理与多模态对齐
4. **长上下文性能瓶颈**：Transformer 复杂度高 → 需新架构（如 Mamba、LongMamba）
5. **上下文选择不精准**：需更自适应的过滤与重排机制
6. **数字存在**（Digital Presence）：

   > 人类本质是社会关系的总和 → 在 AI 时代，**人类正被其数字上下文所定义**。这些上下文可持久存在、演化，甚至在人离世后继续通过 AI 与世界互动。

## 9. 结论

上下文工程并非 LLM 时代的突发奇想，而是伴随机器智能演进的**长期学科**。其核心始终是**弥合人类意图与机器理解之间的熵差**。未来，随着机器智能逼近甚至超越人类，AI 或将不仅理解我们，更能**照亮并拓展我们对自身的认知**。
