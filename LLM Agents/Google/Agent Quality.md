# 智能体质量

Google 智能体质量白皮书

2025年11月

---

## 目录

### 致谢

**内容贡献者**
- Hussain Chinoy
- Ale Fin
- Peter Grabowski
- Michelle Liu
- Anant Nawalgaria
- Kanchana Patlolla
- Steven Pecht
- Julia Wiesinger

**策展人和编辑**
- Anant Nawalgaria
- Kanchana Patlolla

**设计师**
- Michael Lanning

---

## 引言 6

### 本指南基于三个核心信息：

**轨迹即是真相**：我们必须超越仅仅评估最终输出的方式。智能体质量和安全的真正衡量标准在于其整个决策过程。

**可观察性是基础**：你无法判断一个你看不到的过程。我们详细介绍了可观察性的"三大支柱"——日志记录、追踪和指标——作为捕获智能体"思维过程"的必要技术基础。

**评估是持续循环**：我们将这些概念综合成"智能体质量飞轮"，一个将数据转化为可操作洞察的运营手册。这个系统使用可扩展的AI驱动评估者和不可或缺的人机协同（HITL）判断的混合方式来推动持续改进。

本白皮书面向构建这个未来的架构师、工程师和产品领导者。它提供了从构建有能力的智能体转向构建可靠和值得信赖的智能体的框架。

### 如何阅读本白皮书

本指南的结构从"为什么"构建到"是什么"最后到"怎么做"。使用本节导航到与您角色最相关的章节。

- **对所有读者**：从第1章"非确定性世界中的智能体质量"开始。本章确立了核心问题，解释了为什么传统QA对AI智能体无效，并介绍了定义我们目标的智能体质量四大支柱（有效性、效率、鲁棒性和安全性）。

- **对产品经理、数据科学家和QA领导者**：如果您负责测量什么以及如何判断质量，请专注于第2章"智能体评估的艺术"。本章是您的战略指南。它详细介绍了"由外向内"评估层次结构，解释了可扩展的"LLM即评判者"范式，并阐明了人机协同（HITL）评估的关键作用。

- **对工程师、架构师和SRE**：如果您构建系统，您的技术蓝图是第3章"可观察性"。本章从理论转向实现。它提供了"厨房类比"（流水线厨师 vs. 美食家主厨）来解释监控与可观察性，并详细介绍了可观察性的三大支柱：日志、追踪和指标——您需要构建"可评估"智能体的工具。

- **对团队负责人和战略家**：要了解这些部分如何创建自我改进系统，请阅读第4章"结论"。本章将概念统一为运营手册。它介绍了"智能体质量飞轮"作为持续改进的模型，并总结了构建可信AI的三个核心原则。

### 智能体质量在非确定性世界中的挑战

人工智能世界正在全速转型。我们正在从构建执行指令的可预测工具转向设计解释意图、制定计划并执行复杂多步骤动作的自主智能体。对于在前沿构建、竞争和部署的数据科学家和工程师来说，这种转型带来了深远挑战。使AI智能体强大的机制也使它们变得不可预测。

要理解这种转变，将传统软件比作配送卡车，将AI智能体比作Formula 1赛车。卡车只需要基本检查（"引擎启动了吗？它遵循固定路线了吗？"）。赛车就像AI智能体，是一个复杂的自主系统，其成功取决于动态判断。它的评估不能是一个简单的检查清单；它需要持续遥测来判断每个决策的质量——从燃油消耗到制动策略。

这种演进从根本上改变了我们必须处理软件质量的方式。传统的质量保证（QA）实践虽然对确定性系统很强大，但对于现代AI的细微和涌现行为是不够的。一个智能体可以通过100个单元测试，但在生产中仍然灾难性地失败，因为它的失败不是代码中的错误；而是其判断中的缺陷。

传统软件验证问："我们正确地构建了产品吗？"它验证逻辑是否符合固定规范。现代AI评估必须问一个更复杂的问题："我们构建了正确的产品吗？"这是一个验证过程，评估动态和不确定世界中的质量、鲁棒性和可信度。

本章审视这个新范式。我们将探讨为什么智能体质量需要新方法，分析使我们旧方法过时的技术转变，并建立评估"思考"系统的战略"由外向内"框架。

### 为什么智能体质量需要新方法

对工程师来说，风险是需要识别和缓解的。在传统软件中，失败是明确的：系统崩溃、抛出NullPointerException或返回明确错误的计算。这些失败是明显的、确定性的，可以追溯到逻辑中的特定错误。

AI智能体的失败方式不同。它们的失败通常不是系统崩溃，而是质量的微妙退化，源于模型权重、训练数据和环境交互的复杂相互作用。这些失败是隐蔽的：系统继续运行，API调用返回200 OK，输出看起来合理。但它是 profoundly 错误的、操作危险的，并悄悄地侵蚀信任。

无法理解这种转变的组织将面临重大失败、操作效率低下和声誉损害。虽然算法偏见和概念漂移等失败模式存在于被动模型中，但智能体的自主性和复杂性放大了这些风险，使其更难追踪和缓解。考虑表1中突出显示的这些真实世界失败模式：

**表1：智能体失败模式**

| 失败模式 | 描述 | 示例 |
|---------|------|------|
| **算法偏见** | 智能体操作化并可能放大其训练数据中存在的系统性偏见，导致不公平或歧视性结果。 | • 负责风险总结的金融智能体基于偏见训练数据中的邮编过度惩罚贷款申请。 |
| **事实幻觉** | 智能体生成听起来合理但事实错误或捏造的信息，通常在找不到有效来源时具有高度信心。 | • 研究工具在学术报告中生成高度具体但完全错误的历史日期或地理位置，破坏学术完整性。 |
| **性能和概念漂移** | 智能体的性能随着其交互的现实世界数据（"概念"）的变化而随时间退化，使其原始训练过时。 | • 欺诈检测智能体未能发现新的攻击模式。 |
| **涌现的意外行为** | 智能体开发出新颖或意想不到的策略来实现其目标，这些策略可能是低效的、无帮助的或利用性的。 | • 发现并利用系统规则中的漏洞。<br>• 与其他机器人进行"代理战"（例如，重复覆盖编辑）。 |

这些失败使传统的调试和测试范式无效。您不能使用断点来调试幻觉。您不能编写单元测试来防止涌现偏见。根本原因分析需要深度数据分析、模型重新训练和系统评估——这是一个全新的学科。

### 范式转变：从可预测代码到不可预测智能体

核心技术挑战源于从以模型为中心的AI到以系统为中心的AI的演进。评估AI智能体与评估算法根本不同，因为智能体是一个系统。这种演进发生在复合阶段，每个阶段都增加了新的评估复杂性层。

**图1：从传统机器学习到多智能体系统**

1. **传统机器学习**：评估回归或分类模型，虽然不简单，是一个明确定义的问题。我们依赖于针对保留测试集的统计指标，如精确度、召回率、F1分数和RMSE。问题很复杂，但"正确"的定义是明确的。

2. **被动LLM**：随着生成模型的兴起，我们失去了简单的指标。我们如何测量生成段落的"准确性"？输出是概率性的。即使输入相同，输出也可能变化。评估变得更加复杂，依赖于人类评分者和模型vs模型基准测试。尽管如此，这些系统主要是被动的、文本输入文本输出工具。

3. **LLM+RAG（检索增强生成）**：下一个飞跃引入了多组件流水线，由Lewis等人（2020）¹在其工作"用于知识密集型NLP任务的检索增强生成"开创。现在，失败可能发生在LLM或检索系统中。智能体给出坏答案是因为LLM推理差，还是因为向量数据库检索了不相关的片段？我们的评估表面从仅模型扩展到包括分块策略、嵌入和检索器的性能。

4. **主动AI智能体**：今天，我们面临深远的架构转变。LLM不再只是文本生成器；它是复杂系统中的推理"大脑"，集成到能够自主行动的循环中。这种智能体系统引入了三个破坏我们评估模型的核心技术能力：

   - **规划和多步推理**：智能体将复杂目标（"规划我的旅行"）分解为多个子任务。这创建了一个轨迹（思考→行动→观察→思考...）。LLM的非确定性现在在每一步都复合。第1步中一个小的、随机的词选择可能会在第4步将智能体发送到完全不同且不可恢复的推理路径。

   - **工具使用和函数调用**：智能体通过API和外部工具（代码解释器、搜索引擎、预订API）与现实世界交互。这引入了动态环境交互。智能体的下一个行动完全取决于外部、不可控世界的状态。

   - **记忆**：智能体维持状态。短期"草稿板"记忆跟踪当前任务，而长期记忆允许智能体从过去的交互中学习。这意味着智能体的行为会演变，昨天有效的输入今天可能会基于智能体"学到"的东西产生不同的结果。

5. **多智能体系统**：当多个主动智能体集成到共享环境中时，出现终极架构复杂性。这不再是单个轨迹的评估，而是系统级涌现现象的评估，引入了新的基本挑战：

   - **涌现系统失败**：系统的成功取决于智能体之间无脚本交互，如资源竞争、通信瓶颈和系统性死锁，这些不能归因于单个智能体的失败。

   - **合作vs竞争评估**：目标函数本身可能变得模糊。在合作MAS（例如，供应链优化）中，成功是全局指标，而在竞争MAS（例如，博弈论场景或拍卖系统）中，评估通常需要跟踪个体智能体性能和整体市场/环境的稳定性。

这种能力组合意味着评估的主要单位不再是模型，而是整个系统轨迹。智能体的涌现行为源于其规划模块、工具、记忆和动态环境之间的复杂相互作用。

### 智能体质量的支柱：评估框架

如果我们不能再依赖简单的准确性指标，必须评估整个系统，我们从哪里开始？答案是称为"由外向内"方法的战略转变。

这种方法将AI评估锚定在以用户为中心的指标和总体业务目标上，超越对内部、组件级技术分数的单一依赖。我们必须停止只问"模型的F1分数是多少？"，开始问"这个智能体提供可衡量的价值并与我们的用户意图一致吗？"

这种策略需要一个将高级业务目标与技术性能连接的全面框架。我们在四个相互连接的支柱上定义智能体质量：

**图2：智能体质量的四大支柱**

**有效性（目标达成）**：这是最终的"黑盒子"问题：智能体是否成功准确地实现了用户的实际意图？这个支柱直接连接到以用户为中心的指标和业务KPI。对于零售智能体，这不仅仅是"它找到产品了吗？"而是"它推动了转化吗？"对于数据分析智能体，不是"它写代码了吗？"而是"代码产生了正确的洞察吗？"有效性是任务成功的最终衡量标准。

**效率（操作成本）**：智能体是否很好地解决了问题？一个需要25步、5次失败的工具调用和3个自我纠正循环来预订简单航班的智能体可能被认为是低质量智能体——即使它最终成功了。效率以消耗的资源测量：总token数（成本）、挂钟时间（延迟）和轨迹复杂性（总步数）。

**鲁棒性（可靠性）**：智能体如何处理逆境和现实世界的混乱？当API超时、网站布局改变、数据缺失或用户提供模糊提示时，智能体是否优雅地失败？鲁棒的智能体重试失败的调用，在需要时要求用户澄清，并报告它不能做什么以及为什么，而不是崩溃或产生幻觉。

**安全性和对齐（可信度）**：这是不可协商的门槛。智能体是否在其定义的道德边界和约束内操作？这个支柱涵盖了从公平性和偏见的负责任AI指标到抵御提示注入和数据泄露的安全性。它确保智能体保持任务，拒绝有害指令，并作为您组织的可信代理运行。

这个框架明确了一点：如果你只能看到最终答案，你就无法测量这些支柱中的任何一个。如果你不计算步数，就无法测量效率。如果你不知道哪个API调用失败，就无法诊断鲁棒性失败。如果你无法检查智能体的内部推理，就无法验证安全性。

智能体质量的全面框架需要智能体可见性的全面架构。

### 总结与下一步

智能体固有的非确定性本质已经打破了传统质量保证。风险现在包括细微问题，如偏见、幻觉和漂移，源于从被动模型到主动、以系统为中心的智能体的转变，这些智能体会规划和使用工具。我们必须将重点从验证（检查规格）转向确认（判断价值）。

---

## 智能体评估的艺术：判断过程 16

AI智能体的兴起，它们能够规划、使用工具并与复杂环境交互，显著复杂化了评估环境。我们必须超越"测试"输出，学习"评估"过程的艺术。本章提供了战略框架来做到这一点：判断智能体的整个决策轨迹，从初始意图到最终结果。

### 战略框架："由外向内"评估层次结构 17

为了避免在组件级指标的海洋中迷失，评估必须是一个自上而下的、战略性的过程。我们称之为"由外向内"层次结构。这种方法优先考虑唯一最终重要的指标——现实世界的成功——然后深入探讨为什么成功或不成功的技术细节。这个模型是一个两阶段过程：从黑盒子开始，然后打开它。

#### "由外向内"视角：端到端评估（黑盒子） 18

**图3：全面智能体评估框架**

第一个也是最重要的问题是："智能体是否有效地实现了用户的目标？"

这是"由外向内"视图。在分析单个内部思考或工具调用之前，我们必须评估智能体对其定义目标的最终表现。

这一阶段的指标专注于整体任务完成情况。我们测量：

- **任务成功率**：关于最终输出是否正确、完整并解决用户实际问题的二元（或分级）分数，例如编码智能体的PR接受率、金融智能体的成功数据库交易率，或客户服务机器人的会话完成率。

- **用户满意度**：对于交互式智能体，这可以是直接的用户反馈分数（例如，点赞/点踩）或客户满意度分数（CSAT）。

- **整体质量**：如果智能体的目标是定量的（例如，"总结这10篇文章"），指标可能是准确性或完整性（例如，"它是否总结了所有10篇？"）。

如果智能体在这一阶段得分100%，我们的工作可能就完成了。但在复杂系统中，很少如此。当智能体产生有缺陷的最终输出、放弃任务或无法收敛到解决方案时，"由外向内"视图告诉我们哪里出了问题。现在我们必须打开盒子看为什么。

#### "由内向外"视角：轨迹评估（透明盒子） 19

一旦识别到失败，我们转移到"由内向外"视图。我们通过系统性地评估智能体执行轨迹的每个组件来分析智能体的方法：

**实用提示：**
要使用智能体开发套件（ADK）构建输出回归测试，启动ADK Web UI（adk web）并与您的智能体交互。当您收到想要设置为基准的理想响应时，导航到Eval选项卡并点击"添加当前会话"。这将整个交互保存为Eval Case（在.test.json文件中），并将智能体的当前文本锁定为ground truth final_response。然后您可以通过CLI（adk eval）或pytest运行这个Eval Set，自动检查未来智能体版本与保存答案的对比，捕获输出质量中的任何回归。

1. **LLM规划（"思考"）**：我们首先检查核心推理。LLM本身是问题吗？这里的失败包括幻觉、无意义或离题的响应、上下文污染或重复的输出循环。

2. **工具使用（选择和参数化）**：智能体与其工具一样好。我们必须分析智能体是否调用了错误的工具、未能调用必要的工具、幻觉工具名称或参数名称/类型，或不必要地调用一个。即使它选择了正确的工具，也可能通过提供缺失参数、错误数据类型或为API调用提供格式错误的JSON而失败。

3. **工具响应解释（"观察"）**：在工具正确执行后，智能体必须理解结果。智能体经常在这里失败，通过错误解释数值数据、未能从响应中提取关键实体，或者，关键地，没有识别工具返回的错误状态（例如，API的404错误）并继续进行，就像调用成功一样。

4. **RAG性能**：如果智能体使用检索增强生成（RAG），轨迹取决于其检索信息的质量。失败包括不相关的文档检索、获取过时或错误的信息，或者LLM完全忽略检索的上下文并且仍然产生幻觉答案。

5. **轨迹效率和鲁棒性**：除了正确性，我们必须评估过程本身：暴露低效的资源分配，如过多的API调用、高延迟或冗余工作。它还揭示了鲁棒性失败，如未处理的异常。

6. **多智能体动态**：在高级系统中，轨迹涉及多个智能体。评估还必须包括智能体间通信日志，以检查误解或通信循环，并确保智能体遵循其定义的角色而不与他人冲突。

通过分析轨迹，我们可以从"最终答案是错误的"（黑盒子）转移到"最终答案是错误的，因为..."（透明盒子）。这种诊断能力水平是智能体评估的整个目标。

### 评估者：智能体判断的人员和内容 21

知道评估什么（轨迹）是战斗的一半。另一半是如何判断它。对于质量、安全性和可解释性等细微方面，这种判断需要复杂的、混合的方法。自动化系统提供规模，但人类判断仍然是质量的至关重要的仲裁者。

**实用提示：**
当您在ADK中保存Eval Case（如前一个技巧所述）时，它还保存整个工具调用序列作为ground truth轨迹。您的自动化pytest或adk eval运行将然后检查这个轨迹的完美匹配（默认情况下）。
要手动实现过程评估（即调试失败），使用ADK Web UI中的Trace选项卡。这提供了智能体执行的交互图，允许您可视化检查智能体的计划，查看它调用的每个工具及其确切参数，并将其实际路径与预期路径进行比较，以精确定位其逻辑失败的确切步骤。

#### 自动化指标 22

自动化指标提供速度和可重复性。它们对回归测试和基准测试输出很有用。示例包括：

- **基于字符串的相似性**（ROUGE、BLEU），比较生成文本与参考文本。
- **基于嵌入的相似性**（BERTScore、余弦相似性），测量语义接近度。
- **任务特定基准测试**，例如TruthfulQA²

指标是高效但浅层的：它们捕获表面相似性，而不是更深的推理或用户价值。

**实用提示：**
将自动化指标实施为CI/CD管道中的第一个质量门。关键是将其视为趋势指标，而不是质量的绝对衡量标准。例如，0.8的特定BERTScore并不明确意味着答案是"好的"。
它们的真正价值在于跟踪变化：如果您的分支在您的"golden set"上持续平均0.8 BERTScore，而新代码提交将平均值降至0.6，您就自动检测到了显著的回归。这使得指标成为完美的、低成本的"第一过滤器"，以在扩展到更昂贵的LLM即评判者或人类评估之前大规模捕获明显失败。

#### LLM即评判者范式 23

我们如何自动化"这个摘要好吗？"或"这个计划合乎逻辑吗？"等定性输出的评估？答案是使用我们试图评估的相同技术。LLM即评判者³范式涉及使用强大的、最先进的模型（如Google的Gemini Advanced）来评估另一个智能体的输出。

我们为"评判者"LLM提供智能体的输出、原始提示、"golden"答案或参考（如果存在），以及详细的评估标准（例如，"在1-5的量表上评估这个响应的帮助性、正确性和安全性，解释你的推理。"）。

这种方法提供可扩展、快速且令人惊讶的微妙反馈，特别是对于智能体的"思考"质量或其工具响应的解释等中间步骤。虽然它不能替代人类判断，但它允许数据科学团队快速评估数千个场景的性能，使迭代评估过程成为可能。

**实用提示：**
要实现这一点，优先考虑成对比较而不是单一评分，以缓解前面提到的确切偏见。首先，对两个不同的智能体版本（例如，您的旧生产智能体 vs. 新的实验智能体）运行您的评估提示集，为每个提示生成"答案A"和"答案B"。
然后，通过给强大的LLM（如Gemini Pro）一个明确的标准和强制选择的提示来创建LLM评判者："给定这个用户查询，哪个响应更有帮助：A还是B？解释你的推理。"通过自动化这个过程，您可以可扩展地计算新智能体的胜/负/平率。高的"胜率"是比绝对（且通常是嘈杂的）1-5分数的小变化更可靠的改进信号。

用于LLM即评判者的提示，特别是对于强大的成对比较，可能看起来像这样：
```
你是客户支持聊天机器人的专家评估者。你的目标是评估两个响应哪个更有帮助、礼貌和正确。

[用户查询]
"你好，我的订单#12345还没到。"

[答案A]
"我可以看到订单#12345目前正在配送中，应该会在今天下午5点前到达。"

[答案B]
"订单#12345在卡车上。它会在5点前到那里。"

请评估哪个答案更好。在正确性、帮助性和语调方面比较它们。提供你的推理，然后在JSON对象中输出你的最终决定，包含"winner"键（"A"、"B"或"tie"）和"rationale"键。
```

#### 智能体即评判者 25

虽然LLM可以评分最终响应，但智能体需要对其推理和行动进行更深入评估。新兴的智能体即评判者⁴范式使用一个智能体来评估另一个的完整执行轨迹。它不是只评分输出，而是评估过程本身。关键评估维度包括：

- **计划质量**：计划是否逻辑结构化且可行？
- **工具使用**：是否选择了正确的工具并正确应用？
- **上下文处理**：智能体是否有效地使用了先前的信息？

这种方法对于过程评估特别有价值，其中失败通常源于有缺陷的中间步骤，而不是最终输出。

**实用提示：**
要实现智能体即评判者，考虑将执行轨迹对象的相关部分提供给您的评判者。首先，配置您的智能体框架以记录和导出轨迹，包括内部计划、选择的工具列表和传递的确切参数。
然后，创建一个专门的"批评智能体"，带有提示（标准），要求它直接评估这个轨迹对象。您的提示应该提出具体的过程问题："1. 基于轨迹，初始计划是否合乎逻辑？2. {tool_A}工具是正确的第一个选择，还是应该使用另一个工具？3. 参数是否正确且格式正确？"这允许您自动检测过程失败（如低效的计划），即使智能体产生了看起来正确的最终答案。

#### 人机协同评估 26

虽然自动化提供规模，但它难以处理深度主观性和复杂领域知识。人机协同（HITL）评估是捕获自动化系统错过的关键定性信号和微妙判断的必要过程。

然而，我们必须摆脱人类评分提供完美"客观ground truth"的想法。对于高度主观的任务（如评估创意质量或微妙语调），完美的注释间协议是罕见的。相反，HITL是建立人类校准基准的不可或缺的方法，确保智能体的行为与复杂的人类价值观、上下文需求和领域特定准确性保持一致。

**HITL过程涉及几个关键功能：**

- **领域专业知识**：对于专门的智能体（例如，医疗、法律或金融），您必须利用领域专家来评估事实正确性和遵守特定行业标准的情况。

- **解释微妙性**：人类对于判断定义高质量交互的微妙品质至关重要，如语调、创意性、用户意图和复杂的伦理对齐。

- **创建"黄金集"**：在自动化有效之前，人类必须建立"黄金标准"基准。这包括策划全面的评估集，定义成功目标，并制作涵盖典型、边缘和对抗性场景的稳健测试用例套件。

#### 用户反馈和评审者界面 27

评估还必须捕获真实世界的用户反馈。每次交互都是有用性、清晰度和信任的信号。这种反馈包括定性信号（如点赞/点踩）和定量产品内成功指标，如编码智能体的拉取请求（PR）接受率，或旅行智能体的成功预订完成率。**最佳实践包括：**

- **低摩擦反馈**：点赞/点踩、快速滑块或简短评论。
- **上下文丰富的审查**：反馈应与完整对话和智能体的推理轨迹配对。
- **评审者用户界面（UI）**：双面板界面：左侧对话，右侧推理步骤，并为"坏计划"或"工具误用"等问题提供内联标签。
- **治理仪表板**：聚合反馈以突出重复问题和风险。

没有可用的界面，评估框架在实践中会失败。强大的UI使用户和评审者反馈可见、快速和可操作。

**实用提示：**
为了运行时安全，实施中断工作流程。在像ADK这样的框架中，您可以配置智能体在承诺高风险工具调用（如execute_payment或delete_database_entry）之前暂停其执行。智能体的状态和计划行动随后在评审者UI中显示，其中人类操作员必须手动批准或拒绝该步骤，然后才允许智能体恢复。

### 超越性能：负责任AI（RAI）和安全评估 28

评估的最终维度不是作为一个组件，而是作为任何生产智能体的强制性、不可协商门槛运作：负责任AI和安全。一个100%有效但造成伤害的智能体是彻底的失败。

安全评估是一个专门的学科，必须编织到整个开发生命周期中。这包括：

- **系统性红队演练**：使用对抗性场景积极尝试破坏智能体。这包括尝试生成仇恨言论、揭示私人信息、传播有害刻板印象，或诱导智能体参与恶意行动。

- **自动化过滤器和人工审查**：实施技术过滤器来捕获策略违规，并将其与人工审查配对，因为单独的自动化可能无法捕捉偏见或毒性的微妙形式。

- **指导原则遵守**：明确评估智能体输出与预定义道德指导原则的一致性，以确保对齐并防止意外后果。

最终，性能指标告诉我们智能体是否能做工作，但安全评估告诉我们它是否应该做。

**实用提示：**
将您的护栏实现为结构化的插件，而不是孤立的功能。在这种模式中，回调是机制（ADK提供的钩子），而插件是您构建的可重用模块。
例如，您可以构建单个SafetyPlugin类。这个插件然后会将其内部方法注册到框架的可用回调中：
1. 您插件的check_input_safety()方法会注册到before_model_callback。这个方法的工作是运行您的提示注入分类器。
2. 您插件的check_output_pii()方法会注册到after_model_callback。这个方法的工作是运行您的PII扫描器。

这种插件架构使您的护栏可重用、可独立测试，并干净地分层在基础模型内置安全设置（如Gemini中的设置）之上。

### 总结与下一步 30

有效的智能体评估需要超越简单测试，转向战略性的、层次化的框架。这种"由外向内"方法首先验证端到端任务完成（黑盒子），然后分析"透明盒子"内的完整轨迹——评估推理质量、工具使用、鲁棒性和效率。

判断这个过程需要混合方法：可扩展的自动化，如LLM即评判者，配有人机协同（HITL）评估者不可或缺的、微妙的判断。

这个框架通过负责任AI和安全评估的不可协商层来保护，以构建可信系统。

我们理解需要判断整个轨迹，但这个框架在没有数据的情况下纯粹是理论性的。要实现这种"透明盒子"评估，系统首先必须是可观察的。第3章将提供架构蓝图，从评估理论转向可观察性的实践，通过掌握三大支柱：日志记录、追踪和指标。

---

## 可观察性：洞察智能体的思维 31

### 从监控到真正的可观察性 31

**AI智能体就像美食家主厨在"神秘盒子"挑战中**：主厨被赋予目标（"创造令人惊叹的甜点"）和一篮食材（用户的提示、数据和可用工具）。没有单一的正确食谱。他们可能制作巧克力熔岩蛋糕、解构提拉米苏或藏红花 infused panna cotta。所有都可能是有效，甚至是出色的解决方案。

**可观察性**就是美食评论家如何评判主厨。评论家不只品尝最终的菜肴。他们想要理解过程和推理。为什么主厨选择将覆盆子与罗勒配对？他们使用什么技术来结晶姜？当他们意识到没有糖时如何适应？我们需要看到他们的"思维过程"来真正评估他们工作的质量。

这代表了AI智能体的根本转变，从简单监控转向真正的可观察性。重点不再仅仅是验证智能体是否活跃，而是理解其认知过程的质量。不再问"智能体是否在运行？"，关键问题变成"智能体是否在有效思考？"

### 可观察性的三大支柱 32

那么，我们如何访问智能体的"思维过程"？我们不能直接读取其思维，但我们可以分析它留下的证据。这是通过在三大基础上构建我们的可观察性实践来实现的：日志记录、追踪和指标。它们是允许我们从品尝最终菜肴到评判整个烹饪表演的工具。

**图4：智能体可观察性的三大基础支柱**

#### 支柱1：日志记录——智能体的日记 33

**什么是日志？**日志是可观察性的原子单位。将它们视为智能体日记中的时间戳条目。每个条目是关于离散事件的原始、不可变的事实："在10:01:32，我被问了一个问题。在10:01:33，我决定使用get_weather工具。"它们告诉我们发生了什么。

**超越print()：什么使日志有效？**
像Google Cloud Logging这样的完全托管服务允许您大规模存储、搜索和分析日志数据。它可以自动从Google Cloud服务收集日志，其Log Analytics功能允许您运行SQL查询来发现智能体行为中的趋势。

一个一流框架使这变得容易。例如，智能体开发套件（ADK）构建在Python的标准日志模块上。这允许开发人员配置所需的详细程度——从生产中的高级INFO消息到开发期间的粒度DEBUG消息——而无需更改智能体的代码。

**关键日志条目的剖析**
要重建智能体的"思维过程"，日志必须富含上下文。结构化JSON格式是黄金标准。

- **核心信息**：一个好的日志捕获完整上下文：提示/响应对、中间推理步骤（智能体的"思维链"，由Wei等人（2022）探讨的概念）、结构化工具调用（输入、输出、错误）以及智能体内部状态的任何变化。

- **权衡：详细程度 vs. 性能**：高度详细的DEBUG日志是开发人员调试的最佳朋友，但在生产环境中可能太"嘈杂"并产生性能开销。这就是为什么结构化日志如此强大；它允许您收集详细数据但高效地过滤它。

这是一个实际示例，显示了结构化日志的强大能力，改编自ADK DEBUG输出：

```
// 捕获单个LLM请求的结构化日志条目...
2025-07-10 15:26:13,778 - DEBUG - google_adk.google.adk.models.google_llm - 发送请求，模型：gemini-2.0-flash，后端：GoogleLLMVariant.GEMINI_API，流：False
2025-07-10 15:26:13,778 - DEBUG - google_adk.google.adk.models.google_llm - LLM请求：
-----------------------------------------------------------
系统指令：
您掷骰子并回答骰子结果的问题.....关于您的描述是"可以掷8面骰子并检查素数的hello world智能体"。
-----------------------------------------------------------
内容：
{"parts":[{"text":"掷一个6面骰子"}],"role":"user"}
{"parts":[{"function_call":{"args":{"sides":6},"name":"roll_die"}}],"role":"model"}
{"parts":[{"function_response":{"name":"roll_die","response":{"result":2}}],"role":"user"}
-----------------------------------------------------------
函数：
roll_die: {'sides': {'type': <Type.INTEGER: 'INTEGER'>}}
check_prime: {'nums': {'items': {'type': <Type.INTEGER: 'INTEGER'>}, 'type': <Type.ARRAY: 'ARRAY'>}}
-----------------------------------------------------------
2025-07-10 15:26:13,779 - INFO - google_genai.models - AFC已启用，最大远程调用：10。
2025-07-10 15:26:14,309 - INFO - google_adk.google.adk.models.google_llm - LLM响应：
-----------------------------------------------------------
文本：
我掷了一个6面骰子，结果是2。
```

**实用提示：**
一个强大的日志记录模式是在行动前记录智能体的意图，在行动后记录结果。这立即阐明了失败尝试与故意决定不行动之间的区别。

#### 支柱2：追踪——跟随智能体的脚步 36

**什么是追踪？**如果日志是日记条目，那么追踪是将它们连接成连贯故事的叙述线程。追踪跟随单个任务——从初始用户查询到最终答案——将单个日志（称为span）缝合成完整的端到端视图。追踪通过显示事件之间的因果关系来揭示关键的"为什么"。

想象一下侦探的证据板。日志是单个线索——一张照片、一张票根。追踪是连接它们的红线，揭示事件的完整序列。

**为什么追踪是不可或缺的**
考虑一个复杂的智能体失败，其中用户问问题并得到无意义的答案。

- **孤立的日志可能显示**：错误：RAG搜索失败，错误：LLM响应验证失败。您看到了错误，但根本原因不清楚。
- **追踪揭示了完整的因果链**：用户查询 → RAG搜索（失败）→ 有缺陷的工具调用（接收到空输入）→ LLM错误（被错误工具输出混淆）→ 错误的最终答案

追踪使根本原因立即变得明显，使其对于调试复杂的多步智能体行为不可或缺。

**智能体追踪的关键要素**
现代追踪建立在OpenTelemetry等开放标准之上。核心组件包括：

- **Spans**：追踪中的单个、命名操作（例如，llm_call span、tool_execution span）。
- **Attributes**：附加到每个span的丰富元数据——prompt_id、latency_ms、token_count、user_id等。
- **上下文传播**：通过唯一trace_id链接spans的"魔力"，允许像Google Cloud Trace这样的后端组装完整画面。Cloud Trace是一个分布式追踪系统，帮助您了解应用程序处理请求需要多长时间。当智能体部署在像Vertex AI Agent Engine这样的托管运行时，这种集成是简化的。Agent Engine处理生产中扩展智能体的基础设施，并自动与Cloud Trace集成，提供端到端可观察性，将智能体调用与所有后续模型和工具调用链接。

**图5：OpenTelemetry视图让您能够检查属性、日志、事件和其他详细信息**

#### 支柱3：指标——智能体的健康报告 38

**什么是指标？**如果日志是主厨的准备笔记，追踪是评论家一步步观看食谱展开，那么指标是评论家发布的最终记分卡。它们是定量的、聚合的健康分数，让您立即、一目了然地了解智能体的整体性能。

关键是，美食评论家不会仅仅基于对最终菜肴的单一品尝来发明这些分数。他们的判断受到他们观察到的一切的启发。指标是相同的：它们不是新数据源。它们通过随时间聚合您的日志和追踪数据来得出。它们回答了"平均而言，表现如何？"的问题。

对于AI智能体，将指标分为两个不同的类别很有用：直接可测量的系统指标和更复杂的、评估性的质量指标。

**系统指标：生命体征**
系统指标是操作健康的基础、定量测量。它们通过聚合函数（如平均值、总和或百分位）从日志和追踪的属性直接计算。将这些视为智能体的生命体征：其脉搏、温度和血压。

**要跟踪的关键系统指标包括：**

**性能：**
- **延迟（P50/P99）**：通过聚合追踪中的duration_ms属性来找到中位数和第99百分位响应时间来计算。这告诉您关于典型和最坏情况下的用户体验。

**成本：**
- **每任务Token数**：所有追踪中token_count属性的平均值，这对于管理LLM成本至关重要。
- **每次运行的API成本**：通过将token计数与模型定价结合，您可以跟踪每次任务的平均财务成本。

**有效性：**
- **任务完成率**：成功达到指定"成功"span的追踪百分比。
- **工具使用频率**：每个工具（例如，get_weather）作为span名称出现的计数，揭示哪些工具最有价值。

这些指标对于运营、设置警报以及管理智能体机队的成本和性能至关重要。

**质量指标：判断决策制定**
质量指标是通过在原始可观察性数据之上应用第2章中详述的判断框架得出的二阶指标。它们超越效率来评估智能体的推理和最终输出质量本身。

**关键质量指标示例包括：**
- **正确性和准确性**：智能体是否提供了事实正确的答案？如果它总结了文档，总结是否忠于原文？
- **轨迹依从性**：智能体是否遵循了给定任务的预期路径或"理想配方"？它是否以正确的顺序调用了正确的工具？
- **安全性和责任感**：智能体的响应是否避免了有害、偏见或不适当的内容？
- **帮助性和相关性**：智能体的最终响应实际上是否对用户有帮助并与他们的查询相关？

生成这些指标需要的不仅仅是简单的数据库查询。它通常涉及将智能体的输出与"golden"数据集进行比较，或使用复杂的LLM即评判者根据标准对响应进行评分。

来自我们日志和追踪的可观察性数据是计算这些分数所需的基本证据，但判断过程本身是一个独立的、关键的学科。

### 融会贯通：从原始数据到可操作的洞察 41

拥有日志、追踪和指标就像拥有一个才华横溢的主厨、一个库存丰富的食品储藏室和一个评判标准。但这些只是组件。要经营一家成功的餐厅，您需要将它们组装成一个繁忙晚餐服务的运行系统。本节是关于这种实用的组装——在实时运营期间将您的可观察性数据转化为实时行动和洞察。

**这涉及三个关键运营实践：**

1. **仪表板和警报：将系统健康与模型质量分离**
   单个仪表板是不够的。要有效管理AI智能体，您需要系统指标和质量指标的独立视图，因为它们服务于不同的目的和不同的团队。

   **运营仪表板（用于系统指标）**：此仪表板类别专注于实时运营健康。它跟踪智能体的核心生命体征，主要面向负责系统正常运行和性能的站点可靠性工程师（SRE）、DevOps和运营团队。
   - **跟踪内容**：P99延迟、错误率、API成本、Token消耗。
   - **目的**：立即发现系统故障、性能下降或预算超支。
   - **示例警报**：警报：P99延迟 > 3秒持续5分钟。这表示系统瓶颈，需要立即工程关注。

   **质量仪表板（用于质量指标）**：此类别跟踪智能体有效性和正确性的更微妙、变化较慢的指标。对于负责智能体决策和输出质量的产品所有者、数据科学家和AgentOps团队至关重要。
   - **跟踪内容**：事实正确性分数、轨迹依从性、帮助性评分、幻觉率。
   - **目的**：检测智能体质量的微妙漂移，特别是在部署新模型或提示后。
   - **示例警报**：警报：'帮助性评分'在过去24小时内下降了10%。这表明虽然系统可能运行正常（系统指标正常），但智能体输出的质量正在下降，需要调查其逻辑或数据。

2. **安全和PII：保护您的数据**
   这是生产运营的不可协商方面。日志和追踪中捕获的用户输入通常包含个人身份信息（PII）。稳健的PII清理机制必须成为日志管道的集成部分，在数据长期存储之前，以确保符合隐私法规并保护您的用户。

3. **核心权衡：粒度 vs. 开销**
   在生产中为每个单一请求捕获高度详细的日志和追踪可能过于昂贵并为您的系统增加延迟。关键是找到战略平衡。

   - **最佳实践 - 动态采样**：在开发环境中使用高粒度日志记录（DEBUG级别）。在生产中，设置较低的默认日志级别（INFO）但实现动态采样。例如，您可能决定只追踪10%的成功请求但100%的所有错误。这为您的指标提供了广泛的性能数据，而不会让您的系统不堪重负，同时仍然捕获调试每个失败所需的丰富诊断细节。

### 总结与下一步 43

要信任自主智能体，您必须首先能够理解其过程。您不会在没有对其食谱、技术和决策过程有一些洞察的情况下评判美食家主厨的最终菜肴。本章已经确立可观察性是框架，为我们提供对智能体的这种关键洞察。它提供了厨房内的"眼睛和耳朵"。

我们已经了解到，强大的可观察性实践建立在三个基础支柱上，它们协同工作将原始数据转化为完整的画面：
- **日志**：结构化的日记，提供每一步发生的事情的粒度、事实记录。
- **追踪**：连接单个日志的叙述故事，显示因果路径以揭示为什么发生。
- **指标**：聚合报告卡，大规模总结性能以告诉我们事情进展得如何。我们进一步将这些分为重要的系统指标（如延迟和成本）和关键质量指标（如正确性和帮助性）。

通过将这些支柱组装成连贯的运营系统，我们从盲目飞行转向对智能体行为、效率和有效性的清晰、数据驱动视图。

我们现在拥有了所有部分：为什么（第1章中非确定性的问题）、是什么（第2章中的评估框架）、以及如何（第3章中的可观察性架构）。

在第4章中，我们将这一切整合到一个单一的运营手册中，展示这些组件如何形成"智能体质量飞轮"——一个构建不仅有能力，而且真正可信的智能体的持续改进循环。

---

## 结论：在自主世界中建立信任 44

### 引言：从自主能力到企业信任 44

我们正处于智能体时代的黎明。创造能够推理、规划和行动的AI能力将是我们这个时代最具变革性的技术转变之一。但强大的能力伴随着构建值得我们信任的系统的深刻责任。

掌握本白皮书中的概念——可以称之为"评估工程"——是下一波AI的关键竞争优势。那些继续将智能体质量视为事后考虑的组织将陷入有前途的演示和失败部署的循环中。相比之下，那些投资于这种严格的、架构集成评估方法的组织将超越炒作，部署真正变革性的、企业级的AI系统。

最终目标不仅仅是构建能够工作的智能体，而是构建被信任的智能体。正如我们所展示的，这种信任不是希望或偶然的问题。它是在持续、全面和架构合理的评估的熔炉中锻造出来的。

### 智能体质量飞轮：框架的综合体 45

一个优秀的智能体不仅表现良好；它还会改进。这种持续评估的学科是将巧妙的演示与企业级系统分开的关键。这种实践创造了一个强大的、自我强化的系统，我们称之为"智能体质量飞轮"。

把它想象成启动一个巨大的、沉重的飞轮。第一次推动是最难的。但结构化的评估实践提供了后续的、一致的推动。每次推动都增加动量，直到轮子以不可阻挡的力量旋转，创造质量和信任的良性循环。这个飞轮是我们讨论的整个框架的操作体现。

**图 6：智能体质量飞轮**

以下是各章的组件如何协同工作以构建这种动量：

- **步骤1：定义质量（目标）**：飞轮需要方向。正如我们在第1章中定义的，一切都从质量四大支柱开始：有效性、成本效率、安全性和用户信任。这些支柱不是抽象的理想；它们是具体的指标，为我们的评估工作提供意义，并将飞轮与真正的商业价值对齐。

- **步骤2：为实现可观察性而配置工具（基础）**：您无法管理您看不到的东西。正如我们在可观察性一章中详述的，我们必须指示我们的智能体生成结构化日志（智能体的日记）和端到端追踪（叙述线程）。这种可观察性是基础实践，生成测量四大支柱所需的丰富证据，为飞轮提供必要的燃料。

- **步骤3：评估过程（引擎）**：建立了可观察性后，我们现在可以判断性能。正如我们在评估章节中探讨的，这涉及战略性的"由外向内"评估，判断最终输出和整个推理过程。这是推动轮子旋转的强大推动力——一个混合引擎，使用可扩展的LLM即评判者系统来获得速度，使用人机协同（HITL）"黄金标准"来获得基准真相。

- **步骤4：构建反馈循环（动量）**：这是第1章中"可评估设计"架构焕发生机的地方。通过构建关键的反馈循环，我们确保每个生产失败，在捕获和注释后，都以编程方式转换为我们的"黄金"评估集中的永久回归测试。每个失败都使系统更智能，让飞轮旋转更快，驱动无情、持续改进。

#### 构建可信智能体的三个核心原则 46

如果您从本白皮书中学不到其他东西，请记住这三个原则。它们代表任何旨在在这个新的、智能体技术水平上构建真正可靠的自主系统的领导者的基础心态。

- **原则1：将评估视为架构支柱，而非最终步骤**：还记得第1章中的赛车类比吗？您不是构建Formula 1赛车然后再螺栓固定传感器。您从一开始就设计遥测端口。智能体工作负载需要相同的DevOps范式。可靠的智能体是"可评估设计"的，从第一行代码开始就配备发射判断所需日志和追踪的工具。质量是架构选择，而不是最终QA阶段。

- **原则2：轨迹即是真相**：对于智能体来说，最终答案只是一个长故事的最后一句话。正如我们在评估章节中确立的，智能体逻辑、安全性和效率的真正衡量标准在于其端到端的"思维过程"——轨迹。这是过程评估。要真正理解智能体成功或失败的原因，您必须分析这条路径。这只有通过我们在第3章中详述的深度可观察性实践才可能实现。

- **原则3：人类是仲裁者**：自动化是我们规模化的工具；人性是我们真理的来源。从LLM即评判者系统到安全分类器的自动化是必不可少的。然而，正如我们在人机协同（HITL）评估的深度探讨中确立的，"好"的基本定义、细致输出的验证，以及关于安全性和公平性的最终判断必须基于人类价值观。AI可以帮助评分考试，但人类编写评分标准并决定"A+"的真正含义。

### 未来是智能体的——也是可靠的 47

我们正处于智能体时代的黎明。创造能够推理、规划和行动的AI将是我们这个时代最具变革性的技术转变之一。但强大的能力伴随着构建值得我们信任的系统的深刻责任。

掌握本白皮书中的概念——可以称之为"评估工程"——是下一波AI的关键竞争优势。那些继续将智能体质量视为事后考虑的组织将陷入有前途的演示和失败部署的循环中。相比之下，那些投资于这种严格的、架构集成评估方法的组织将超越炒作，部署真正变革性的、企业级的AI系统。

最终目标不仅仅是构建能够工作的智能体，而是构建被信任的智能体。正如我们所展示的，这种信任不是希望或偶然的问题。它是在持续、全面和架构合理的评估的熔炉中锻造出来的。

### 参考文献 49

#### 学术论文、书籍和正式报告

1. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... & Rocktäschel, T. (2020). 用于知识密集型NLP任务的检索增强生成。《神经信息处理系统进展》，33，9459-9474。

2. Lin, S., Hilton, J., & Evans, O. (2022). TruthfulQA：衡量模型如何模仿人类错误。载于《计算语言学协会第60届年会论文集（第1卷：长篇论文）》（第3214-3252页）。

3. Li, D., Jiang, B., Huang, L., Beigi, A., Zhao, C., Tan, Z.,... & Liu, H. (2024). 从生成到判断：LLM即评判者的机会和挑战。arXiv预印本 arXiv:2411.16594。

4. Zhuge, M., Wang, M., Shen, X., Zhang, Y., Wang, Y., Zhang, C., ... & Liu, N. (2024). 智能体即评判者：用智能体评估智能体。arXiv预印本 arXiv:2410.10934。

5. Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). AI安全中的具体问题。arXiv预印本 arXiv:1606.06565。

6. Baysan, M. S., Uysal, S., İşlek, İ., Çığ Karaman, Ç., & Güngör, T. (2025). LLM即评判者：使用大语言模型自动评估搜索查询解析。《大数据前沿》，8。可访问于：https://doi.org/10.3389/fdata.2025.1611389。

7. Felderer, M., & Ramler, R. (2021). 基于AI系统的质量保证：概述和挑战。载于《软件质量：云计算中软件工程和软件质量的复杂性和挑战》（第38-51页）。Springer国际出版。

8. Hendrycks, D., Carlini, N., Schulman, J., & Steinhardt, J. (2023). ML安全中未解决的问题。arXiv预印本 arXiv:2306.04944。

9. Ji, Z., Lee, N., Fries, R., Yu, T., Su, D., Xu, Y.,... & Fung, P. (2023). AI生成的文本：任务、评估标准和方法的综述。arXiv预印本 arXiv:2303.07233。

10. Lin, C. Y. (2004). ROUGE：自动评估摘要的软件包。载于ACL-04文本摘要分支工作坊论文集（第74-81页）。

11. National Institute of Standards and Technology. (2023). AI风险管理框架（AI RMF 1.0）。美国商务部。

12. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU：机器翻译自动评估的方法。载于《计算语言学协会第40届年会论文集》（第311-318页）。

13. Retzlaff, C., Das, S., Wayllace, C., Mousavi, P., Afshari, M., Yang, T., ... & Holzinger, A. (2024). 人机协同强化学习：要求、挑战和机会的综述和立场。《人工智能研究杂志》，79，359-415。

14. Slattery, F., Costello, E., & Holland, J. (2024). 语言模型带来的风险分类。arXiv预印本 arXiv:2401.12903。

15. Taylor, M. E. (2023). 强化学习需要人机协同框架和方法。在自适应和学习智能体（ALA）研讨会2023上发表的论文。

16. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E.,... & Zhou, D. (2022). 思维链提示引发大语言模型中的推理。《神经信息处理系统进展》，35，24824-24837。

17. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). BERTScore：用BERT评估文本生成。载于国际学习表示会议。

#### 网络文章、博客文章和一般网页

18. Bunnyshell. (n.d.). LLM即评判者：AI如何更快更智能地评估AI。2025年9月16日检索自 https://www.bunnyshell.com/blog/when-ai-becomes-the-judge-understanding-llm-as-a-j/。

19. Coralogix. (n.d.). AI的OpenTelemetry：追踪提示、工具和推断。2025年9月16日检索自 https://coralogix.com/ai-blog/opentelemetry-for-ai-tracing-prompts-tools-and-inferences/。

20. Drapkin, A. (2025年9月2日). AI出错：AI的错误、失误和幻觉（2023-2025）。Tech.co。2025年9月16日检索自 https://tech.co/news/list-ai-failures-mistakes-errors。

21. Dynatrace. (n.d.). 什么是OpenTelemetry？日志、指标和追踪的开源标准。2025年9月16日检索自 https://www.dynatrace.com/news/blog/what-is-opentelemetry/。

22. Galileo. (n.d.). LLM即评判者评估综合指南。2025年9月16日检索自 https://galileo.ai/blog/llm-as-a-judge-guide-evaluation。

23. Gofast.ai. (n.d.). 现实世界中的智能体幻觉：当AI工具出错时。2025年9月16日检索自 https://www.gofast.ai/blog/ai-bias-fairness-agent-hallucinations-validation-drift-2025。

24. IBM. (2025年2月25日). 什么是LLM可观察性？2025年9月16日检索自 https://www.ibm.com/think/topics/llm-observability。

25. MIT Sloan Teaching & Learning Technologies. (n.d.). 当AI出错时：解决AI幻觉和偏见。2025年9月16日检索自 https://mitsloanedtech.mit.edu/ai/basics/addressing-ai-hallucinations-and-bias/。

26. ResearchGate. (n.d.). (PDF) LLM即评判者综述。2025年9月16日检索自 https://www.researchgate.net/publication/386112851_A_Survey_on_LLM-as-a-Judge。

27. TrustArc. (n.d.). 美国国家标准与技术研究院（NIST）人工智能风险管理。2025年9月16日检索自 https://trustarc.com/regulations/nist-ai-rmf/。

---